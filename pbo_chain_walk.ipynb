{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PBO learnt on several iterations and one weigth one the chain walk environment\n",
                "\n",
                "## Define parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import jax\n",
                "\n",
                "# keys\n",
                "seed = 1\n",
                "key = jax.random.PRNGKey(seed)\n",
                "env_key, key = jax.random.split(key)\n",
                "shuffle_key, q_network_key, random_weights_key, pbo_network_key = jax.random.split(key, 4)\n",
                "\n",
                "# Box over states and actions\n",
                "n_states = 10\n",
                "n_actions = 2\n",
                "sucess_probability = 0.9\n",
                "gamma = 0.8\n",
                "\n",
                "# Weights collection\n",
                "n_weights = 1\n",
                "filtering_weights = True\n",
                "\n",
                "# PBO trainings\n",
                "max_bellman_iterations = 5\n",
                "add_infinity = False\n",
                "# importance_iteration = [0.01, 0.05, 0.1, 0.5, 1, 5]\n",
                "importance_iteration = [1] * (max_bellman_iterations + 1)  \n",
                "\n",
                "## Linear PBO\n",
                "tolerance = 0.00001\n",
                "max_steps = 1000\n",
                "batch_size_samples = 20\n",
                "batch_size_weights = n_weights\n",
                "learning_rate = {\"first\": 0.001, \"last\": 0.00005, \"duration\": max_steps}\n",
                "\n",
                "## Optimal linear PBO\n",
                "tolerance_optimal = 1e-5\n",
                "max_steps_optimal = 5000\n",
                "batch_size_weights_optimal = n_weights\n",
                "learning_rate_optimal = {\"first\": 0.001, \"last\": 0.00005, \"duration\": max_steps_optimal}\n",
                "\n",
                "# Visualisation\n",
                "plot_freq = 2\n",
                "sleeping_time = 0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "from pbo.environment.chain_walk import ChainWalkEnv\n",
                "\n",
                "states = np.arange(n_states)\n",
                "actions = np.arange(n_actions)\n",
                "states_boxes = (np.arange(n_states + 1 + 1) - 0.5)[:-1]\n",
                "actions_boxes = (np.arange(n_actions + 1 + 1) - 0.5)[:-1]\n",
                "\n",
                "env = ChainWalkEnv(env_key, n_states, sucess_probability, gamma)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collect samples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Samples on the mesh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "\n",
                "from pbo.sample_collection.replay_buffer import ReplayBuffer\n",
                "\n",
                "\n",
                "n_samples = n_states * 2\n",
                "replay_buffer = ReplayBuffer()\n",
                "\n",
                "for state in states:\n",
                "    for action in actions:\n",
                "        env.reset(jnp.array([state]))\n",
                "        next_state, reward, _, _ = env.step(jnp.array([action]))\n",
                "\n",
                "        replay_buffer.add(jnp.array([state]), jnp.array([action]), reward, next_state)\n",
                "\n",
                "replay_buffer.cast_to_jax_array()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 2 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from pbo.sample_collection.count_samples import count_samples\n",
                "from pbo.utils.state_action_mesh import StateActionMesh\n",
                "\n",
                "\n",
                "samples_count, n_outside_boxes = count_samples(replay_buffer, states_boxes, actions_boxes)\n",
                "samples_visu_mesh = StateActionMesh(states, actions, sleeping_time=0)\n",
                "\n",
                "samples_visu_mesh.set_values(samples_count, zeros_to_nan=True)\n",
                "samples_visu_mesh.show(\n",
                "    f\"Samples repartition, \\n{int(100 * n_outside_boxes / n_samples)}% are outside the box.\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimal Q function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 2 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "optimal_q = env.optimal_Q_mesh()\n",
                "\n",
                "q_visu_mesh = StateActionMesh(states, actions, sleeping_time)\n",
                "\n",
                "q_visu_mesh.set_values(optimal_q)\n",
                "q_visu_mesh.show(\"Optimal q function\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build q network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.networks.q import TableQ\n",
                "\n",
                "\n",
                "q = TableQ(\n",
                "    network_key=q_network_key,\n",
                "    random_weights_range=None,\n",
                "    random_weights_key=random_weights_key,\n",
                "    n_states=n_states,\n",
                "    n_actions=n_actions,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collect weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Random init weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.weights_collection.weights_buffer import WeightsBuffer\n",
                "\n",
                "\n",
                "weights_buffer = WeightsBuffer()\n",
                "\n",
                "while len(weights_buffer) < n_weights:\n",
                "    weights = q.random_init_weights()\n",
                "    if not filtering_weights or weights.max() < 1 / (1 - gamma):\n",
                "        weights_buffer.add(weights)\n",
                "\n",
                "weights_buffer.cast_to_jax_array()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build the PBOs network and the dataloaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.networks.learnable_pbo import LinearPBO, LinearPBOOnWeights\n",
                "from pbo.networks.optimal_pbo import OptimalTablePBO\n",
                "from pbo.sample_collection.dataloader import SampleDataLoader\n",
                "from pbo.weights_collection.dataloader import WeightsDataLoader\n",
                "\n",
                "\n",
                "data_loader_samples = SampleDataLoader(replay_buffer, batch_size_samples, shuffle_key)\n",
                "\n",
                "data_loader_weights = WeightsDataLoader(weights_buffer, batch_size_weights, shuffle_key)\n",
                "data_loader_weights_optimal = WeightsDataLoader(weights_buffer, batch_size_weights_optimal, shuffle_key)\n",
                "\n",
                "pbo = LinearPBO(q, max_bellman_iterations, add_infinity, importance_iteration, pbo_network_key, learning_rate)\n",
                "pbo_optimal = OptimalTablePBO(q, max_bellman_iterations, add_infinity, importance_iteration, env.apply_bellman_operator, optimal_q)\n",
                "pbo_optimal_linear = LinearPBOOnWeights(q, max_bellman_iterations, add_infinity, importance_iteration, pbo_network_key, learning_rate_optimal, pbo_optimal)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train the optimal linear PBO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "linear l2 loss: 8.186836 Linear l2 grad loss: 6.08531928062439\n",
                        "linear l2 loss: 8.059493 Linear l2 grad loss: 5.813024640083313\n",
                        "linear l2 loss: 7.9395905 Linear l2 grad loss: 5.580524206161499\n",
                        "linear l2 loss: 7.8259635 Linear l2 grad loss: 5.376661419868469\n",
                        "linear l2 loss: 7.7181067 Linear l2 grad loss: 5.196031212806702\n",
                        "linear l2 loss: 7.6157517 Linear l2 grad loss: 5.038584351539612\n",
                        "linear l2 loss: 7.51802 Linear l2 grad loss: 4.915475249290466\n",
                        "linear l2 loss: 7.423839 Linear l2 grad loss: 4.828724265098572\n",
                        "linear l2 loss: 7.3323717 Linear l2 grad loss: 4.761351585388184\n",
                        "linear l2 loss: 7.2431273 Linear l2 grad loss: 4.713492512702942\n",
                        "linear l2 loss: 7.155423 Linear l2 grad loss: 4.6831440925598145\n",
                        "linear l2 loss: 7.069023 Linear l2 grad loss: 4.657242298126221\n",
                        "linear l2 loss: 6.983786 Linear l2 grad loss: 4.638187766075134\n",
                        "linear l2 loss: 6.8992243 Linear l2 grad loss: 4.63872754573822\n",
                        "linear l2 loss: 6.8148775 Linear l2 grad loss: 4.6444807052612305\n",
                        "linear l2 loss: 6.7304645 Linear l2 grad loss: 4.6515244245529175\n",
                        "linear l2 loss: 6.6456513 Linear l2 grad loss: 4.667706489562988\n",
                        "linear l2 loss: 6.5603156 Linear l2 grad loss: 4.687070250511169\n",
                        "linear l2 loss: 6.474376 Linear l2 grad loss: 4.709607362747192\n",
                        "linear l2 loss: 6.3877473 Linear l2 grad loss: 4.730238437652588\n",
                        "linear l2 loss: 6.30042 Linear l2 grad loss: 4.750451445579529\n",
                        "linear l2 loss: 6.212409 Linear l2 grad loss: 4.774583458900452\n",
                        "linear l2 loss: 6.12381 Linear l2 grad loss: 4.7897419929504395\n",
                        "linear l2 loss: 6.0347605 Linear l2 grad loss: 4.80345344543457\n",
                        "linear l2 loss: 5.9453998 Linear l2 grad loss: 4.8090232610702515\n",
                        "linear l2 loss: 5.8558283 Linear l2 grad loss: 4.819293856620789\n",
                        "linear l2 loss: 5.7660894 Linear l2 grad loss: 4.823713481426239\n",
                        "linear l2 loss: 5.676265 Linear l2 grad loss: 4.830120861530304\n",
                        "linear l2 loss: 5.5864143 Linear l2 grad loss: 4.835730075836182\n",
                        "linear l2 loss: 5.4966545 Linear l2 grad loss: 4.843701183795929\n",
                        "linear l2 loss: 5.407144 Linear l2 grad loss: 4.850755929946899\n",
                        "linear l2 loss: 5.3180194 Linear l2 grad loss: 4.860684096813202\n",
                        "linear l2 loss: 5.229379 Linear l2 grad loss: 4.872091233730316\n",
                        "linear l2 loss: 5.141214 Linear l2 grad loss: 4.887652695178986\n",
                        "linear l2 loss: 5.053243 Linear l2 grad loss: 4.913268089294434\n",
                        "linear l2 loss: 4.9652877 Linear l2 grad loss: 4.943468987941742\n",
                        "linear l2 loss: 4.877206 Linear l2 grad loss: 4.979389905929565\n",
                        "linear l2 loss: 4.7890205 Linear l2 grad loss: 5.012459635734558\n",
                        "linear l2 loss: 4.7007184 Linear l2 grad loss: 5.048869490623474\n",
                        "linear l2 loss: 4.6123023 Linear l2 grad loss: 5.087166965007782\n",
                        "linear l2 loss: 4.523771 Linear l2 grad loss: 5.128740549087524\n",
                        "linear l2 loss: 4.435277 Linear l2 grad loss: 5.16763174533844\n",
                        "linear l2 loss: 4.346555 Linear l2 grad loss: 5.2030922174453735\n",
                        "linear l2 loss: 4.2577667 Linear l2 grad loss: 5.246408641338348\n",
                        "linear l2 loss: 4.1689887 Linear l2 grad loss: 5.289709091186523\n",
                        "linear l2 loss: 4.0802145 Linear l2 grad loss: 5.328945636749268\n",
                        "linear l2 loss: 3.992054 Linear l2 grad loss: 5.362430810928345\n",
                        "linear l2 loss: 3.9057453 Linear l2 grad loss: 5.397290587425232\n",
                        "linear l2 loss: 3.8190072 Linear l2 grad loss: 5.399576485157013\n",
                        "linear l2 loss: 3.7321153 Linear l2 grad loss: 5.407082617282867\n",
                        "linear l2 loss: 3.6454587 Linear l2 grad loss: 5.42625492811203\n",
                        "linear l2 loss: 3.5595245 Linear l2 grad loss: 5.452925145626068\n",
                        "linear l2 loss: 3.476732 Linear l2 grad loss: 5.481854557991028\n",
                        "linear l2 loss: 3.3964837 Linear l2 grad loss: 5.458869814872742\n",
                        "linear l2 loss: 3.3169281 Linear l2 grad loss: 5.421394050121307\n",
                        "linear l2 loss: 3.238271 Linear l2 grad loss: 5.386025369167328\n",
                        "linear l2 loss: 3.1592505 Linear l2 grad loss: 5.349936604499817\n",
                        "linear l2 loss: 3.0801506 Linear l2 grad loss: 5.321487784385681\n",
                        "linear l2 loss: 3.004557 Linear l2 grad loss: 5.361287176609039\n",
                        "linear l2 loss: 2.9308221 Linear l2 grad loss: 5.229075849056244\n",
                        "linear l2 loss: 2.8557186 Linear l2 grad loss: 5.263463795185089\n",
                        "linear l2 loss: 2.7848213 Linear l2 grad loss: 5.149891376495361\n",
                        "linear l2 loss: 2.7132192 Linear l2 grad loss: 5.156736433506012\n",
                        "linear l2 loss: 2.6440141 Linear l2 grad loss: 5.0750569105148315\n",
                        "linear l2 loss: 2.5746408 Linear l2 grad loss: 5.086548566818237\n",
                        "linear l2 loss: 2.5159547 Linear l2 grad loss: 5.275999367237091\n",
                        "linear l2 loss: 2.4563234 Linear l2 grad loss: 5.239633798599243\n",
                        "linear l2 loss: 2.3987155 Linear l2 grad loss: 5.300941467285156\n",
                        "linear l2 loss: 2.3379915 Linear l2 grad loss: 5.210997641086578\n",
                        "linear l2 loss: 2.283946 Linear l2 grad loss: 5.359678149223328\n",
                        "linear l2 loss: 2.2266777 Linear l2 grad loss: 5.316868364810944\n",
                        "linear l2 loss: 2.1732461 Linear l2 grad loss: 5.422449588775635\n",
                        "linear l2 loss: 2.1162221 Linear l2 grad loss: 5.307392954826355\n",
                        "linear l2 loss: 2.064779 Linear l2 grad loss: 5.476135790348053\n",
                        "linear l2 loss: 2.0130708 Linear l2 grad loss: 5.521185040473938\n",
                        "linear l2 loss: 1.9603386 Linear l2 grad loss: 5.524796903133392\n",
                        "linear l2 loss: 1.9118146 Linear l2 grad loss: 5.648779928684235\n",
                        "linear l2 loss: 1.8613735 Linear l2 grad loss: 5.641122937202454\n",
                        "linear l2 loss: 1.8162359 Linear l2 grad loss: 5.9147829413414\n",
                        "linear l2 loss: 1.7667922 Linear l2 grad loss: 5.684016168117523\n",
                        "linear l2 loss: 1.7211884 Linear l2 grad loss: 6.136240065097809\n",
                        "linear l2 loss: 1.6789572 Linear l2 grad loss: 6.040966212749481\n",
                        "linear l2 loss: 1.6382564 Linear l2 grad loss: 6.360871851444244\n",
                        "linear l2 loss: 1.6058915 Linear l2 grad loss: 6.636119484901428\n",
                        "linear l2 loss: 1.5747573 Linear l2 grad loss: 6.8736079931259155\n",
                        "linear l2 loss: 1.5436822 Linear l2 grad loss: 7.025768160820007\n",
                        "linear l2 loss: 1.5142281 Linear l2 grad loss: 7.2563546895980835\n",
                        "linear l2 loss: 1.4857843 Linear l2 grad loss: 7.373088240623474\n",
                        "linear l2 loss: 1.4583344 Linear l2 grad loss: 7.572946786880493\n",
                        "linear l2 loss: 1.4319395 Linear l2 grad loss: 7.669991493225098\n",
                        "linear l2 loss: 1.4070472 Linear l2 grad loss: 7.799633860588074\n",
                        "linear l2 loss: 1.3798008 Linear l2 grad loss: 8.024597406387329\n",
                        "linear l2 loss: 1.3594544 Linear l2 grad loss: 8.29301381111145\n",
                        "linear l2 loss: 1.3376726 Linear l2 grad loss: 8.401634097099304\n",
                        "linear l2 loss: 1.3188565 Linear l2 grad loss: 8.629854083061218\n",
                        "linear l2 loss: 1.2998164 Linear l2 grad loss: 8.767367839813232\n",
                        "linear l2 loss: 1.284774 Linear l2 grad loss: 8.990980386734009\n",
                        "linear l2 loss: 1.2724434 Linear l2 grad loss: 9.012622237205505\n",
                        "linear l2 loss: 1.2606605 Linear l2 grad loss: 9.041927456855774\n",
                        "linear l2 loss: 1.2498631 Linear l2 grad loss: 9.039664626121521\n",
                        "linear l2 loss: 1.2406085 Linear l2 grad loss: 9.10567855834961\n",
                        "linear l2 loss: 1.2306426 Linear l2 grad loss: 9.124891638755798\n",
                        "linear l2 loss: 1.2204838 Linear l2 grad loss: 9.209994077682495\n",
                        "linear l2 loss: 1.2118113 Linear l2 grad loss: 9.254264116287231\n",
                        "linear l2 loss: 1.2032124 Linear l2 grad loss: 9.235391616821289\n",
                        "linear l2 loss: 1.1943749 Linear l2 grad loss: 9.226428270339966\n",
                        "linear l2 loss: 1.1870297 Linear l2 grad loss: 9.292104125022888\n",
                        "linear l2 loss: 1.1786636 Linear l2 grad loss: 9.273441314697266\n",
                        "linear l2 loss: 1.1722907 Linear l2 grad loss: 9.352354526519775\n",
                        "linear l2 loss: 1.1655172 Linear l2 grad loss: 9.38648009300232\n",
                        "linear l2 loss: 1.1592274 Linear l2 grad loss: 9.45616865158081\n",
                        "linear l2 loss: 1.153648 Linear l2 grad loss: 9.51152491569519\n",
                        "linear l2 loss: 1.1481292 Linear l2 grad loss: 9.586621642112732\n",
                        "linear l2 loss: 1.1427453 Linear l2 grad loss: 9.658682584762573\n",
                        "linear l2 loss: 1.1363811 Linear l2 grad loss: 9.659255743026733\n",
                        "linear l2 loss: 1.1323036 Linear l2 grad loss: 9.760862350463867\n",
                        "linear l2 loss: 1.1272804 Linear l2 grad loss: 9.778331518173218\n",
                        "linear l2 loss: 1.122393 Linear l2 grad loss: 9.814514994621277\n",
                        "linear l2 loss: 1.1178192 Linear l2 grad loss: 9.818144917488098\n",
                        "linear l2 loss: 1.1150576 Linear l2 grad loss: 9.922234296798706\n",
                        "linear l2 loss: 1.1098799 Linear l2 grad loss: 9.847964406013489\n",
                        "linear l2 loss: 1.1070197 Linear l2 grad loss: 9.86173415184021\n",
                        "linear l2 loss: 1.104293 Linear l2 grad loss: 9.877081155776978\n",
                        "linear l2 loss: 1.097596 Linear l2 grad loss: 9.670144081115723\n",
                        "linear l2 loss: 1.092636 Linear l2 grad loss: 9.624634861946106\n",
                        "linear l2 loss: 1.0891491 Linear l2 grad loss: 9.626715183258057\n",
                        "linear l2 loss: 1.086828 Linear l2 grad loss: 9.666599988937378\n",
                        "linear l2 loss: 1.0847422 Linear l2 grad loss: 9.722094774246216\n",
                        "linear l2 loss: 1.0817788 Linear l2 grad loss: 9.634155631065369\n",
                        "linear l2 loss: 1.0795557 Linear l2 grad loss: 9.718711733818054\n",
                        "linear l2 loss: 1.0775626 Linear l2 grad loss: 9.682632446289062\n",
                        "linear l2 loss: 1.0758456 Linear l2 grad loss: 9.726739645004272\n",
                        "linear l2 loss: 1.0744201 Linear l2 grad loss: 9.756568431854248\n",
                        "linear l2 loss: 1.0720716 Linear l2 grad loss: 9.743547558784485\n",
                        "linear l2 loss: 1.0701774 Linear l2 grad loss: 9.854524374008179\n",
                        "linear l2 loss: 1.0685719 Linear l2 grad loss: 9.80540418624878\n",
                        "linear l2 loss: 1.0671427 Linear l2 grad loss: 9.823870062828064\n",
                        "linear l2 loss: 1.0658743 Linear l2 grad loss: 9.815755009651184\n",
                        "linear l2 loss: 1.0641559 Linear l2 grad loss: 9.821838974952698\n",
                        "linear l2 loss: 1.0627664 Linear l2 grad loss: 9.79942262172699\n",
                        "linear l2 loss: 1.0616268 Linear l2 grad loss: 9.849499940872192\n",
                        "linear l2 loss: 1.0602833 Linear l2 grad loss: 9.877775073051453\n",
                        "linear l2 loss: 1.0593157 Linear l2 grad loss: 9.908448100090027\n",
                        "linear l2 loss: 1.0579076 Linear l2 grad loss: 9.91420829296112\n",
                        "linear l2 loss: 1.056518 Linear l2 grad loss: 9.9441397190094\n",
                        "linear l2 loss: 1.0552769 Linear l2 grad loss: 9.941530466079712\n",
                        "linear l2 loss: 1.0543915 Linear l2 grad loss: 9.948681950569153\n",
                        "linear l2 loss: 1.0535247 Linear l2 grad loss: 9.976139307022095\n",
                        "linear l2 loss: 1.0526587 Linear l2 grad loss: 9.972628831863403\n",
                        "linear l2 loss: 1.0510077 Linear l2 grad loss: 9.958539485931396\n",
                        "linear l2 loss: 1.0501775 Linear l2 grad loss: 9.967366456985474\n",
                        "linear l2 loss: 1.0490435 Linear l2 grad loss: 9.964918851852417\n",
                        "linear l2 loss: 1.048238 Linear l2 grad loss: 9.973862290382385\n",
                        "linear l2 loss: 1.0471201 Linear l2 grad loss: 9.965521097183228\n",
                        "linear l2 loss: 1.0457062 Linear l2 grad loss: 9.972997188568115\n",
                        "linear l2 loss: 1.0449957 Linear l2 grad loss: 9.984314203262329\n",
                        "linear l2 loss: 1.0443499 Linear l2 grad loss: 10.012719869613647\n",
                        "linear l2 loss: 1.0430717 Linear l2 grad loss: 9.946127533912659\n",
                        "linear l2 loss: 1.0423188 Linear l2 grad loss: 10.006973266601562\n",
                        "linear l2 loss: 1.0417436 Linear l2 grad loss: 10.00269091129303\n",
                        "linear l2 loss: 1.0413522 Linear l2 grad loss: 10.041306614875793\n",
                        "linear l2 loss: 1.040817 Linear l2 grad loss: 10.032616376876831\n",
                        "linear l2 loss: 1.0402951 Linear l2 grad loss: 10.043540716171265\n",
                        "linear l2 loss: 1.0399144 Linear l2 grad loss: 10.074408769607544\n",
                        "linear l2 loss: 1.0395018 Linear l2 grad loss: 10.093733191490173\n",
                        "linear l2 loss: 1.0389781 Linear l2 grad loss: 10.069408655166626\n",
                        "linear l2 loss: 1.037858 Linear l2 grad loss: 10.039873838424683\n",
                        "linear l2 loss: 1.0373048 Linear l2 grad loss: 10.08026897907257\n",
                        "linear l2 loss: 1.0368088 Linear l2 grad loss: 10.093977451324463\n",
                        "linear l2 loss: 1.0363216 Linear l2 grad loss: 10.090672373771667\n",
                        "linear l2 loss: 1.0359119 Linear l2 grad loss: 10.12200403213501\n",
                        "linear l2 loss: 1.0355692 Linear l2 grad loss: 10.13896906375885\n",
                        "linear l2 loss: 1.0351648 Linear l2 grad loss: 10.141216039657593\n",
                        "linear l2 loss: 1.0348401 Linear l2 grad loss: 10.163429379463196\n",
                        "linear l2 loss: 1.0338148 Linear l2 grad loss: 10.111822485923767\n",
                        "linear l2 loss: 1.0328407 Linear l2 grad loss: 10.1737619638443\n",
                        "linear l2 loss: 1.0325149 Linear l2 grad loss: 10.21743392944336\n",
                        "linear l2 loss: 1.0321614 Linear l2 grad loss: 10.213287472724915\n",
                        "linear l2 loss: 1.0318534 Linear l2 grad loss: 10.247082233428955\n",
                        "linear l2 loss: 1.0314956 Linear l2 grad loss: 10.232763051986694\n",
                        "linear l2 loss: 1.0311102 Linear l2 grad loss: 10.248236060142517\n",
                        "linear l2 loss: 1.030845 Linear l2 grad loss: 10.274359226226807\n",
                        "linear l2 loss: 1.030594 Linear l2 grad loss: 10.300812482833862\n",
                        "linear l2 loss: 1.0302705 Linear l2 grad loss: 10.319001078605652\n",
                        "linear l2 loss: 1.0299948 Linear l2 grad loss: 10.311075091362\n",
                        "linear l2 loss: 1.0296805 Linear l2 grad loss: 10.331084132194519\n",
                        "linear l2 loss: 1.029424 Linear l2 grad loss: 10.337673902511597\n",
                        "linear l2 loss: 1.0290575 Linear l2 grad loss: 10.360015630722046\n",
                        "linear l2 loss: 1.0287315 Linear l2 grad loss: 10.35323941707611\n",
                        "linear l2 loss: 1.0284216 Linear l2 grad loss: 10.359477639198303\n",
                        "linear l2 loss: 1.0280527 Linear l2 grad loss: 10.370700120925903\n",
                        "linear l2 loss: 1.0277405 Linear l2 grad loss: 10.363317012786865\n",
                        "linear l2 loss: 1.0270672 Linear l2 grad loss: 10.361984133720398\n",
                        "linear l2 loss: 1.0261388 Linear l2 grad loss: 10.390053749084473\n",
                        "linear l2 loss: 1.0258881 Linear l2 grad loss: 10.389330506324768\n",
                        "linear l2 loss: 1.0256847 Linear l2 grad loss: 10.40243113040924\n",
                        "linear l2 loss: 1.0254797 Linear l2 grad loss: 10.408307433128357\n",
                        "linear l2 loss: 1.0252823 Linear l2 grad loss: 10.42129135131836\n",
                        "linear l2 loss: 1.0248573 Linear l2 grad loss: 10.43002963066101\n",
                        "linear l2 loss: 1.0243062 Linear l2 grad loss: 10.42144250869751\n",
                        "linear l2 loss: 1.0235102 Linear l2 grad loss: 10.46496057510376\n",
                        "linear l2 loss: 1.0233287 Linear l2 grad loss: 10.460687398910522\n",
                        "linear l2 loss: 1.023061 Linear l2 grad loss: 10.509157419204712\n",
                        "linear l2 loss: 1.0228667 Linear l2 grad loss: 10.51563584804535\n",
                        "linear l2 loss: 1.022607 Linear l2 grad loss: 10.528428316116333\n",
                        "linear l2 loss: 1.022424 Linear l2 grad loss: 10.537433505058289\n",
                        "linear l2 loss: 1.0221094 Linear l2 grad loss: 10.548694014549255\n",
                        "linear l2 loss: 1.0215777 Linear l2 grad loss: 10.536866664886475\n",
                        "linear l2 loss: 1.0211644 Linear l2 grad loss: 10.547258734703064\n",
                        "linear l2 loss: 1.0207987 Linear l2 grad loss: 10.54738974571228\n",
                        "linear l2 loss: 1.0206201 Linear l2 grad loss: 10.542518973350525\n",
                        "linear l2 loss: 1.0204554 Linear l2 grad loss: 10.529507279396057\n",
                        "linear l2 loss: 1.0202807 Linear l2 grad loss: 10.552889704704285\n",
                        "linear l2 loss: 1.0201042 Linear l2 grad loss: 10.564361333847046\n",
                        "linear l2 loss: 1.019928 Linear l2 grad loss: 10.577670335769653\n",
                        "linear l2 loss: 1.0195955 Linear l2 grad loss: 10.588768005371094\n",
                        "linear l2 loss: 1.0190719 Linear l2 grad loss: 10.574511647224426\n",
                        "linear l2 loss: 1.0189074 Linear l2 grad loss: 10.594611167907715\n",
                        "linear l2 loss: 1.0187365 Linear l2 grad loss: 10.603208303451538\n",
                        "linear l2 loss: 1.0182612 Linear l2 grad loss: 10.59940481185913\n",
                        "linear l2 loss: 1.018091 Linear l2 grad loss: 10.602215766906738\n",
                        "linear l2 loss: 1.0179228 Linear l2 grad loss: 10.605018019676208\n",
                        "linear l2 loss: 1.0176862 Linear l2 grad loss: 10.63141405582428\n",
                        "linear l2 loss: 1.0171639 Linear l2 grad loss: 10.612288355827332\n",
                        "linear l2 loss: 1.0169979 Linear l2 grad loss: 10.63064169883728\n",
                        "linear l2 loss: 1.0166423 Linear l2 grad loss: 10.628754496574402\n",
                        "linear l2 loss: 1.016495 Linear l2 grad loss: 10.617227673530579\n",
                        "linear l2 loss: 1.016325 Linear l2 grad loss: 10.62432873249054\n",
                        "linear l2 loss: 1.0161576 Linear l2 grad loss: 10.63143503665924\n",
                        "linear l2 loss: 1.0158639 Linear l2 grad loss: 10.628857016563416\n",
                        "linear l2 loss: 1.0153524 Linear l2 grad loss: 10.617783427238464\n",
                        "linear l2 loss: 1.0149068 Linear l2 grad loss: 10.628660202026367\n",
                        "linear l2 loss: 1.0147463 Linear l2 grad loss: 10.637739300727844\n",
                        "linear l2 loss: 1.014538 Linear l2 grad loss: 10.648337364196777\n",
                        "linear l2 loss: 1.0143793 Linear l2 grad loss: 10.643456935882568\n",
                        "linear l2 loss: 1.014219 Linear l2 grad loss: 10.638592839241028\n",
                        "linear l2 loss: 1.0140599 Linear l2 grad loss: 10.64349114894867\n",
                        "linear l2 loss: 1.0137337 Linear l2 grad loss: 10.630111455917358\n",
                        "linear l2 loss: 1.0131816 Linear l2 grad loss: 10.634122014045715\n",
                        "linear l2 loss: 1.0130236 Linear l2 grad loss: 10.628064632415771\n",
                        "linear l2 loss: 1.0128678 Linear l2 grad loss: 10.641706347465515\n",
                        "linear l2 loss: 1.0127131 Linear l2 grad loss: 10.640110850334167\n",
                        "linear l2 loss: 1.0125562 Linear l2 grad loss: 10.635274291038513\n",
                        "linear l2 loss: 1.0123781 Linear l2 grad loss: 10.656887769699097\n",
                        "linear l2 loss: 1.0122222 Linear l2 grad loss: 10.65202009677887\n",
                        "linear l2 loss: 1.0117247 Linear l2 grad loss: 10.64443826675415\n",
                        "linear l2 loss: 1.0115697 Linear l2 grad loss: 10.639528036117554\n",
                        "linear l2 loss: 1.0107344 Linear l2 grad loss: 10.665257215499878\n",
                        "linear l2 loss: 1.0105991 Linear l2 grad loss: 10.653806686401367\n",
                        "linear l2 loss: 1.0104401 Linear l2 grad loss: 10.671403169631958\n",
                        "linear l2 loss: 1.0102841 Linear l2 grad loss: 10.678345203399658\n",
                        "linear l2 loss: 1.0101277 Linear l2 grad loss: 10.687321782112122\n",
                        "linear l2 loss: 1.0099726 Linear l2 grad loss: 10.685648918151855\n",
                        "linear l2 loss: 1.0097669 Linear l2 grad loss: 10.707083940505981\n",
                        "linear l2 loss: 1.009111 Linear l2 grad loss: 10.674885988235474\n",
                        "linear l2 loss: 1.0089577 Linear l2 grad loss: 10.700069189071655\n",
                        "linear l2 loss: 1.0088078 Linear l2 grad loss: 10.705838918685913\n",
                        "linear l2 loss: 1.0086573 Linear l2 grad loss: 10.710175037384033\n",
                        "linear l2 loss: 1.0085062 Linear l2 grad loss: 10.703963041305542\n",
                        "linear l2 loss: 1.0083586 Linear l2 grad loss: 10.71198308467865\n",
                        "linear l2 loss: 1.0078728 Linear l2 grad loss: 10.712015509605408\n",
                        "linear l2 loss: 1.0077266 Linear l2 grad loss: 10.703468084335327\n",
                        "linear l2 loss: 1.0074242 Linear l2 grad loss: 10.72764277458191\n",
                        "linear l2 loss: 1.0071505 Linear l2 grad loss: 10.720895886421204\n",
                        "linear l2 loss: 1.0070027 Linear l2 grad loss: 10.715962529182434\n",
                        "linear l2 loss: 1.0068576 Linear l2 grad loss: 10.727853417396545\n",
                        "linear l2 loss: 1.0067126 Linear l2 grad loss: 10.728351473808289\n",
                        "linear l2 loss: 1.0065299 Linear l2 grad loss: 10.74665915966034\n",
                        "linear l2 loss: 1.0060499 Linear l2 grad loss: 10.721519589424133\n",
                        "linear l2 loss: 1.0057551 Linear l2 grad loss: 10.711363911628723\n",
                        "linear l2 loss: 1.0056115 Linear l2 grad loss: 10.73710012435913\n",
                        "linear l2 loss: 1.0054682 Linear l2 grad loss: 10.733129739761353\n",
                        "linear l2 loss: 1.0053252 Linear l2 grad loss: 10.747055292129517\n",
                        "linear l2 loss: 1.0047432 Linear l2 grad loss: 10.787543058395386\n",
                        "linear l2 loss: 1.004599 Linear l2 grad loss: 10.77820098400116\n",
                        "linear l2 loss: 1.00442 Linear l2 grad loss: 10.780596733093262\n",
                        "linear l2 loss: 1.0039456 Linear l2 grad loss: 10.76801872253418\n",
                        "linear l2 loss: 1.0038046 Linear l2 grad loss: 10.774001240730286\n",
                        "linear l2 loss: 1.0036615 Linear l2 grad loss: 10.777962327003479\n",
                        "linear l2 loss: 1.0035203 Linear l2 grad loss: 10.771660923957825\n",
                        "linear l2 loss: 1.0033807 Linear l2 grad loss: 10.776768684387207\n",
                        "linear l2 loss: 1.0032691 Linear l2 grad loss: 10.795109272003174\n",
                        "linear l2 loss: 1.0031326 Linear l2 grad loss: 10.787660598754883\n",
                        "linear l2 loss: 1.0029937 Linear l2 grad loss: 10.787030696868896\n",
                        "linear l2 loss: 1.0028547 Linear l2 grad loss: 10.781859993934631\n",
                        "linear l2 loss: 1.0026809 Linear l2 grad loss: 10.801274538040161\n",
                        "linear l2 loss: 1.0024102 Linear l2 grad loss: 10.780800342559814\n",
                        "linear l2 loss: 1.0023248 Linear l2 grad loss: 10.830071687698364\n",
                        "linear l2 loss: 1.0021847 Linear l2 grad loss: 10.84234869480133\n",
                        "linear l2 loss: 1.0020725 Linear l2 grad loss: 10.840156197547913\n",
                        "linear l2 loss: 1.0019608 Linear l2 grad loss: 10.837956070899963\n",
                        "linear l2 loss: 1.0018493 Linear l2 grad loss: 10.835725784301758\n",
                        "linear l2 loss: 1.0017381 Linear l2 grad loss: 10.833471179008484\n",
                        "linear l2 loss: 1.0016278 Linear l2 grad loss: 10.831263542175293\n",
                        "linear l2 loss: 1.0010507 Linear l2 grad loss: 10.812344431877136\n",
                        "linear l2 loss: 1.0008842 Linear l2 grad loss: 10.82042145729065\n",
                        "linear l2 loss: 1.00075 Linear l2 grad loss: 10.840564608573914\n",
                        "linear l2 loss: 1.000617 Linear l2 grad loss: 10.838735461235046\n",
                        "linear l2 loss: 1.0004843 Linear l2 grad loss: 10.836823225021362\n",
                        "linear l2 loss: 1.0003747 Linear l2 grad loss: 10.853791952133179\n",
                        "linear l2 loss: 1.0002654 Linear l2 grad loss: 10.851642489433289\n",
                        "linear l2 loss: 1.0001568 Linear l2 grad loss: 10.856826066970825\n",
                        "linear l2 loss: 1.0000476 Linear l2 grad loss: 10.854589700698853\n",
                        "linear l2 loss: 0.9998006 Linear l2 grad loss: 10.853998303413391\n",
                        "linear l2 loss: 0.9996681 Linear l2 grad loss: 10.848860144615173\n",
                        "linear l2 loss: 0.99922127 Linear l2 grad loss: 10.856590509414673\n",
                        "linear l2 loss: 0.99881107 Linear l2 grad loss: 10.910787343978882\n",
                        "linear l2 loss: 0.9986475 Linear l2 grad loss: 10.924644231796265\n",
                        "linear l2 loss: 0.9985458 Linear l2 grad loss: 10.908281326293945\n",
                        "linear l2 loss: 0.99843735 Linear l2 grad loss: 10.905953764915466\n",
                        "linear l2 loss: 0.99819374 Linear l2 grad loss: 10.905690908432007\n",
                        "linear l2 loss: 0.9980853 Linear l2 grad loss: 10.904532790184021\n",
                        "linear l2 loss: 0.9979793 Linear l2 grad loss: 10.923454523086548\n",
                        "linear l2 loss: 0.99784756 Linear l2 grad loss: 10.916994214057922\n",
                        "linear l2 loss: 0.99771804 Linear l2 grad loss: 10.919896721839905\n",
                        "linear l2 loss: 0.9975894 Linear l2 grad loss: 10.917832970619202\n",
                        "linear l2 loss: 0.997482 Linear l2 grad loss: 10.92098844051361\n",
                        "linear l2 loss: 0.9973754 Linear l2 grad loss: 10.932470440864563\n",
                        "linear l2 loss: 0.9972683 Linear l2 grad loss: 10.930108547210693\n",
                        "linear l2 loss: 0.99703246 Linear l2 grad loss: 10.916560649871826\n",
                        "linear l2 loss: 0.9969269 Linear l2 grad loss: 10.935526371002197\n",
                        "linear l2 loss: 0.9964882 Linear l2 grad loss: 10.912684202194214\n",
                        "linear l2 loss: 0.996332 Linear l2 grad loss: 10.936981558799744\n",
                        "linear l2 loss: 0.9962066 Linear l2 grad loss: 10.925722360610962\n",
                        "linear l2 loss: 0.9960804 Linear l2 grad loss: 10.919236421585083\n",
                        "linear l2 loss: 0.9959556 Linear l2 grad loss: 10.918195843696594\n",
                        "linear l2 loss: 0.9957078 Linear l2 grad loss: 10.919189929962158\n",
                        "linear l2 loss: 0.99556255 Linear l2 grad loss: 10.940704226493835\n",
                        "linear l2 loss: 0.9951321 Linear l2 grad loss: 10.9212007522583\n",
                        "linear l2 loss: 0.9950054 Linear l2 grad loss: 10.928317546844482\n",
                        "linear l2 loss: 0.9948826 Linear l2 grad loss: 10.927323698997498\n",
                        "linear l2 loss: 0.99476033 Linear l2 grad loss: 10.932843327522278\n",
                        "linear l2 loss: 0.99463934 Linear l2 grad loss: 10.930754542350769\n",
                        "linear l2 loss: 0.9945351 Linear l2 grad loss: 10.933725476264954\n",
                        "linear l2 loss: 0.99443215 Linear l2 grad loss: 10.94558835029602\n",
                        "linear l2 loss: 0.99432814 Linear l2 grad loss: 10.94322955608368\n",
                        "linear l2 loss: 0.994225 Linear l2 grad loss: 10.940917015075684\n",
                        "linear l2 loss: 0.994122 Linear l2 grad loss: 10.938537120819092\n",
                        "linear l2 loss: 0.9940198 Linear l2 grad loss: 10.950392842292786\n",
                        "linear l2 loss: 0.9937974 Linear l2 grad loss: 10.937692284584045\n",
                        "linear l2 loss: 0.9936976 Linear l2 grad loss: 10.972766995429993\n",
                        "linear l2 loss: 0.9935956 Linear l2 grad loss: 10.975769519805908\n",
                        "linear l2 loss: 0.9934932 Linear l2 grad loss: 10.973428606987\n",
                        "linear l2 loss: 0.9930903 Linear l2 grad loss: 10.95577096939087\n",
                        "linear l2 loss: 0.99258727 Linear l2 grad loss: 10.96805191040039\n",
                        "linear l2 loss: 0.99249077 Linear l2 grad loss: 11.00883662700653\n",
                        "linear l2 loss: 0.9923727 Linear l2 grad loss: 11.016050100326538\n",
                        "linear l2 loss: 0.99227285 Linear l2 grad loss: 11.010009407997131\n",
                        "linear l2 loss: 0.99217176 Linear l2 grad loss: 11.007591724395752\n",
                        "linear l2 loss: 0.99206066 Linear l2 grad loss: 11.011813640594482\n",
                        "linear l2 loss: 0.99195594 Linear l2 grad loss: 11.007343769073486\n",
                        "linear l2 loss: 0.99174213 Linear l2 grad loss: 11.009530544281006\n",
                        "linear l2 loss: 0.9916315 Linear l2 grad loss: 11.014850854873657\n",
                        "linear l2 loss: 0.99152917 Linear l2 grad loss: 11.002862811088562\n",
                        "linear l2 loss: 0.9911129 Linear l2 grad loss: 11.00426995754242\n",
                        "linear l2 loss: 0.99099666 Linear l2 grad loss: 11.003036618232727\n",
                        "linear l2 loss: 0.990899 Linear l2 grad loss: 11.006858110427856\n",
                        "linear l2 loss: 0.99079907 Linear l2 grad loss: 11.013919830322266\n",
                        "linear l2 loss: 0.99069995 Linear l2 grad loss: 11.011435508728027\n",
                        "linear l2 loss: 0.9906009 Linear l2 grad loss: 11.003814697265625\n",
                        "linear l2 loss: 0.9903788 Linear l2 grad loss: 10.99200165271759\n",
                        "linear l2 loss: 0.9902649 Linear l2 grad loss: 11.01350998878479\n",
                        "linear l2 loss: 0.99015135 Linear l2 grad loss: 11.011155247688293\n",
                        "linear l2 loss: 0.99002963 Linear l2 grad loss: 11.015441179275513\n",
                        "linear l2 loss: 0.9899177 Linear l2 grad loss: 11.030667424201965\n",
                        "linear l2 loss: 0.9898054 Linear l2 grad loss: 11.028267979621887\n",
                        "linear l2 loss: 0.9896943 Linear l2 grad loss: 11.025882601737976\n",
                        "linear l2 loss: 0.9892761 Linear l2 grad loss: 11.028783559799194\n",
                        "linear l2 loss: 0.9891655 Linear l2 grad loss: 11.027453660964966\n",
                        "linear l2 loss: 0.98903275 Linear l2 grad loss: 11.02951169013977\n",
                        "linear l2 loss: 0.988923 Linear l2 grad loss: 11.04305899143219\n",
                        "linear l2 loss: 0.98881406 Linear l2 grad loss: 11.04167652130127\n",
                        "linear l2 loss: 0.9887047 Linear l2 grad loss: 11.037107825279236\n",
                        "linear l2 loss: 0.9886088 Linear l2 grad loss: 11.043223261833191\n",
                        "linear l2 loss: 0.9885026 Linear l2 grad loss: 11.036867499351501\n",
                        "linear l2 loss: 0.98839617 Linear l2 grad loss: 11.057078242301941\n",
                        "linear l2 loss: 0.9882907 Linear l2 grad loss: 11.055578112602234\n",
                        "linear l2 loss: 0.988186 Linear l2 grad loss: 11.053019762039185\n",
                        "linear l2 loss: 0.988083 Linear l2 grad loss: 11.073675870895386\n",
                        "linear l2 loss: 0.9879765 Linear l2 grad loss: 11.07297682762146\n",
                        "linear l2 loss: 0.9878831 Linear l2 grad loss: 11.079047679901123\n",
                        "linear l2 loss: 0.9876873 Linear l2 grad loss: 11.068982243537903\n",
                        "linear l2 loss: 0.9872966 Linear l2 grad loss: 11.066518783569336\n",
                        "linear l2 loss: 0.9869401 Linear l2 grad loss: 11.077777624130249\n",
                        "linear l2 loss: 0.9868491 Linear l2 grad loss: 11.103582620620728\n",
                        "linear l2 loss: 0.98674786 Linear l2 grad loss: 11.098746418952942\n",
                        "linear l2 loss: 0.98665535 Linear l2 grad loss: 11.10476815700531\n",
                        "linear l2 loss: 0.9865632 Linear l2 grad loss: 11.106271862983704\n",
                        "linear l2 loss: 0.98647165 Linear l2 grad loss: 11.10355818271637\n",
                        "linear l2 loss: 0.986381 Linear l2 grad loss: 11.10943853855133\n",
                        "linear l2 loss: 0.9862899 Linear l2 grad loss: 11.106713056564331\n",
                        "linear l2 loss: 0.9861992 Linear l2 grad loss: 11.103947877883911\n",
                        "linear l2 loss: 0.98610896 Linear l2 grad loss: 11.101160883903503\n",
                        "linear l2 loss: 0.9859166 Linear l2 grad loss: 11.10360586643219\n",
                        "linear l2 loss: 0.98582613 Linear l2 grad loss: 11.109512329101562\n",
                        "linear l2 loss: 0.98573637 Linear l2 grad loss: 11.107857584953308\n",
                        "linear l2 loss: 0.98564714 Linear l2 grad loss: 11.105109095573425\n",
                        "linear l2 loss: 0.98555845 Linear l2 grad loss: 11.103404641151428\n",
                        "linear l2 loss: 0.9854676 Linear l2 grad loss: 11.114096522331238\n",
                        "linear l2 loss: 0.9850883 Linear l2 grad loss: 11.099601864814758\n",
                        "linear l2 loss: 0.98498887 Linear l2 grad loss: 11.097803831100464\n",
                        "linear l2 loss: 0.9848906 Linear l2 grad loss: 11.094933986663818\n",
                        "linear l2 loss: 0.9848029 Linear l2 grad loss: 11.096355199813843\n",
                        "linear l2 loss: 0.9846247 Linear l2 grad loss: 11.099876642227173\n",
                        "linear l2 loss: 0.9845397 Linear l2 grad loss: 11.120228290557861\n",
                        "linear l2 loss: 0.98445207 Linear l2 grad loss: 11.11854088306427\n",
                        "linear l2 loss: 0.98436576 Linear l2 grad loss: 11.132055282592773\n",
                        "linear l2 loss: 0.98427594 Linear l2 grad loss: 11.118155837059021\n",
                        "linear l2 loss: 0.9838993 Linear l2 grad loss: 11.11085832118988\n",
                        "linear l2 loss: 0.9838199 Linear l2 grad loss: 11.149074792861938\n",
                        "linear l2 loss: 0.9837341 Linear l2 grad loss: 11.14390516281128\n",
                        "linear l2 loss: 0.9836475 Linear l2 grad loss: 11.142264723777771\n",
                        "linear l2 loss: 0.98356193 Linear l2 grad loss: 11.140606164932251\n",
                        "linear l2 loss: 0.9834768 Linear l2 grad loss: 11.13785469532013\n",
                        "linear l2 loss: 0.9833924 Linear l2 grad loss: 11.136160492897034\n",
                        "linear l2 loss: 0.9833094 Linear l2 grad loss: 11.156169772148132\n",
                        "linear l2 loss: 0.9832241 Linear l2 grad loss: 11.14882230758667\n",
                        "linear l2 loss: 0.98313105 Linear l2 grad loss: 11.153864026069641\n",
                        "linear l2 loss: 0.9830476 Linear l2 grad loss: 11.155203223228455\n",
                        "linear l2 loss: 0.98296684 Linear l2 grad loss: 11.166781663894653\n",
                        "linear l2 loss: 0.98279804 Linear l2 grad loss: 11.158353209495544\n",
                        "linear l2 loss: 0.9827158 Linear l2 grad loss: 11.173509120941162\n",
                        "linear l2 loss: 0.98263305 Linear l2 grad loss: 11.17076575756073\n",
                        "linear l2 loss: 0.9825506 Linear l2 grad loss: 11.167986154556274\n",
                        "linear l2 loss: 0.9824689 Linear l2 grad loss: 11.166253924369812\n",
                        "linear l2 loss: 0.98238796 Linear l2 grad loss: 11.186665654182434\n",
                        "linear l2 loss: 0.98202467 Linear l2 grad loss: 11.189212322235107\n",
                        "linear l2 loss: 0.98194283 Linear l2 grad loss: 11.190594673156738\n",
                        "linear l2 loss: 0.98186153 Linear l2 grad loss: 11.187795162200928\n",
                        "linear l2 loss: 0.9817811 Linear l2 grad loss: 11.185076713562012\n",
                        "linear l2 loss: 0.98170024 Linear l2 grad loss: 11.182220458984375\n",
                        "linear l2 loss: 0.98153967 Linear l2 grad loss: 11.190288066864014\n",
                        "linear l2 loss: 0.9814596 Linear l2 grad loss: 11.188576459884644\n",
                        "linear l2 loss: 0.9813804 Linear l2 grad loss: 11.193766713142395\n",
                        "linear l2 loss: 0.9813027 Linear l2 grad loss: 11.215508103370667\n",
                        "linear l2 loss: 0.98122245 Linear l2 grad loss: 11.21272075176239\n",
                        "linear l2 loss: 0.981143 Linear l2 grad loss: 11.209916710853577\n",
                        "linear l2 loss: 0.9810643 Linear l2 grad loss: 11.207127928733826\n",
                        "linear l2 loss: 0.9809858 Linear l2 grad loss: 11.204330325126648\n",
                        "linear l2 loss: 0.98090804 Linear l2 grad loss: 11.21849501132965\n",
                        "linear l2 loss: 0.9807547 Linear l2 grad loss: 11.210046768188477\n",
                        "linear l2 loss: 0.9804018 Linear l2 grad loss: 11.224020838737488\n",
                        "linear l2 loss: 0.98036253 Linear l2 grad loss: 11.27619731426239\n",
                        "linear l2 loss: 0.9802851 Linear l2 grad loss: 11.300881385803223\n",
                        "linear l2 loss: 0.9802039 Linear l2 grad loss: 11.29386305809021\n",
                        "linear l2 loss: 0.9801257 Linear l2 grad loss: 11.29112160205841\n",
                        "linear l2 loss: 0.9800483 Linear l2 grad loss: 11.288395166397095\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288395285606384\n",
                        "linear l2 loss: 0.98004836 Linear l2 grad loss: 11.288403630256653\n",
                        "linear l2 loss: 0.9800483 Linear l2 grad loss: 11.288400053977966\n",
                        "linear l2 loss: 0.9800483 Linear l2 grad loss: 11.288408994674683\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.28840434551239\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288399696350098\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288405418395996\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288403034210205\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288399577140808\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288398504257202\n",
                        "linear l2 loss: 0.9800483 Linear l2 grad loss: 11.288414716720581\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288399696350098\n",
                        "linear l2 loss: 0.9800483 Linear l2 grad loss: 11.288419485092163\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288402080535889\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288405299186707\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288400769233704\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.2884019613266\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288402915000916\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288403034210205\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.28842031955719\n",
                        "linear l2 loss: 0.9800483 Linear l2 grad loss: 11.288424015045166\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288406610488892\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.28842282295227\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288403272628784\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288406372070312\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288423895835876\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.28842282295227\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288415789604187\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288416862487793\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.28842282295227\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288419246673584\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288432121276855\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288416862487793\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288413643836975\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.2884202003479\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.288409948348999\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.288428544998169\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288426280021667\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.28841781616211\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.288398265838623\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288423776626587\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288424849510193\n",
                        "linear l2 loss: 0.9800482 Linear l2 grad loss: 11.28843092918396\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288432121276855\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.28842842578888\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288423776626587\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.28843092918396\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288427233695984\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288424730300903\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288415431976318\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288410782814026\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288427114486694\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.28842842578888\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288423657417297\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288423657417297\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.288437843322754\n",
                        "linear l2 loss: 0.9800481 Linear l2 grad loss: 11.28843092918396\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.2884202003479\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288431882858276\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.2884202003479\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288419127464294\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288422703742981\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288432836532593\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.28843379020691\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288423299789429\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288430452346802\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288418889045715\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288415312767029\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.28841519355774\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.28841519355774\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.288412928581238\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.28840684890747\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.288431406021118\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288435101509094\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.288431525230408\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.28842580318451\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.288434982299805\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.288418889045715\n",
                        "linear l2 loss: 0.980048 Linear l2 grad loss: 11.288443326950073\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.288436532020569\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.288435101509094\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.28844428062439\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.288441777229309\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.288445234298706\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.2884441614151\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.288439750671387\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.288447737693787\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.288439750671387\n",
                        "linear l2 loss: 0.9800477 Linear l2 grad loss: 11.288408041000366\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288417339324951\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288416266441345\n",
                        "linear l2 loss: 0.9800477 Linear l2 grad loss: 11.288421750068665\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288416147232056\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288419723510742\n",
                        "linear l2 loss: 0.9800477 Linear l2 grad loss: 11.288427829742432\n",
                        "linear l2 loss: 0.9800477 Linear l2 grad loss: 11.28843104839325\n",
                        "linear l2 loss: 0.9800479 Linear l2 grad loss: 11.288437962532043\n",
                        "linear l2 loss: 0.9800477 Linear l2 grad loss: 11.288413763046265\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288408875465393\n",
                        "linear l2 loss: 0.9800477 Linear l2 grad loss: 11.288424968719482\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.28843069076538\n",
                        "linear l2 loss: 0.9800477 Linear l2 grad loss: 11.288427472114563\n",
                        "linear l2 loss: 0.9800478 Linear l2 grad loss: 11.288421630859375\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288414597511292\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288411021232605\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288406372070312\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288406491279602\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.28841495513916\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288411259651184\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288396716117859\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288403749465942\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288408756256104\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288413643836975\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288411140441895\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288405179977417\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288387894630432\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288400888442993\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288399696350098\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288406491279602\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288408756256104\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288405179977417\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288402795791626\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288415670394897\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288426160812378\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288416862487793\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288419127464294\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288405418395996\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288410902023315\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288415789604187\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288411021232605\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288408756256104\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288416862487793\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288417935371399\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288413405418396\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288414478302002\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288414359092712\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288437843322754\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288434386253357\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288433194160461\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288436532020569\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288437724113464\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288430333137512\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288430333137512\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288431406021118\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288416624069214\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288416624069214\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288435459136963\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288436651229858\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288438558578491\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288442492485046\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288454055786133\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288440108299255\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288452863693237\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288451910018921\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288463711738586\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.28846025466919\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288462400436401\n",
                        "linear l2 loss: 0.98004764 Linear l2 grad loss: 11.288462400436401\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288448452949524\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288439273834229\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288441061973572\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288437604904175\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.28844130039215\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288448214530945\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288437724113464\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288439869880676\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288444757461548\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.28844130039215\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.28843891620636\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.28845739364624\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288455247879028\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288459658622742\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288460969924927\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.28846561908722\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288472652435303\n",
                        "linear l2 loss: 0.9800475 Linear l2 grad loss: 11.288471460342407\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288471221923828\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288446545600891\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.28843343257904\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288435697555542\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288440585136414\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.28844165802002\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288439154624939\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288434624671936\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288437128067017\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288440585136414\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288445234298706\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288438320159912\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288440585136414\n",
                        "linear l2 loss: 0.98004717 Linear l2 grad loss: 11.288440585136414\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288456916809082\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288442850112915\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288448810577393\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288445353507996\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.288453459739685\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288461565971375\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288464903831482\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.28845226764679\n",
                        "linear l2 loss: 0.98004735 Linear l2 grad loss: 11.28846526145935\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.2884441614151\n",
                        "linear l2 loss: 0.9800474 Linear l2 grad loss: 11.288469672203064\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288454532623291\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288443922996521\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288445353507996\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.28845226764679\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288454413414001\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288448572158813\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.288454413414001\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288433194160461\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.28843343257904\n",
                        "linear l2 loss: 0.98004717 Linear l2 grad loss: 11.288433194160461\n",
                        "linear l2 loss: 0.98004717 Linear l2 grad loss: 11.288429617881775\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288413405418396\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288413286209106\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.2884122133255\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288427352905273\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.28842282295227\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.28842842578888\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288429737091064\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288437724113464\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.28843641281128\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288432002067566\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288432955741882\n",
                        "linear l2 loss: 0.98004717 Linear l2 grad loss: 11.288439750671387\n",
                        "linear l2 loss: 0.98004717 Linear l2 grad loss: 11.288444638252258\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.28842806816101\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288429379463196\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288442134857178\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288442134857178\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288442134857178\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288443326950073\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288445472717285\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288445591926575\n",
                        "linear l2 loss: 0.9800472 Linear l2 grad loss: 11.28844666481018\n",
                        "linear l2 loss: 0.98004717 Linear l2 grad loss: 11.288434982299805\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.28843605518341\n",
                        "linear l2 loss: 0.98004717 Linear l2 grad loss: 11.288427948951721\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288430333137512\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288434982299805\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.28842568397522\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288421988487244\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288422107696533\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.28842568397522\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.28841757774353\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288426518440247\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288419365882874\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288434743881226\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288430094718933\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288423895835876\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288426280021667\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288427352905273\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288431167602539\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288434743881226\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288435697555542\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288432478904724\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288435816764832\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288435697555542\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288435697555542\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288434743881226\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288434743881226\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288436889648438\n",
                        "linear l2 loss: 0.98004705 Linear l2 grad loss: 11.288454413414001\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288466095924377\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288449883460999\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288447499275208\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288446307182312\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288448691368103\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288453459739685\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288453817367554\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288457155227661\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288450598716736\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288454413414001\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288451910018921\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288454294204712\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288466095924377\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288465142250061\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288453221321106\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288453221321106\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288454294204712\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.28844952583313\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288463711738586\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288463473320007\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288472890853882\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288477420806885\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288469314575195\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288466930389404\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288466930389404\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.28846800327301\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288475513458252\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288469433784485\n",
                        "linear l2 loss: 0.9800469 Linear l2 grad loss: 11.288485646247864\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288479924201965\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.28847861289978\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288482308387756\n",
                        "linear l2 loss: 0.98004687 Linear l2 grad loss: 11.288483381271362\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288479924201965\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288483381271362\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288485765457153\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288487911224365\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.28848421573639\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288471698760986\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.28847861289978\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288479685783386\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288471698760986\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288472771644592\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288472771644592\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288471460342407\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.28847599029541\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288478136062622\n",
                        "linear l2 loss: 0.9800466 Linear l2 grad loss: 11.288478374481201\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288481712341309\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288480401039124\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288475751876831\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288479208946228\n",
                        "linear l2 loss: 0.9800466 Linear l2 grad loss: 11.28848397731781\n",
                        "linear l2 loss: 0.9800466 Linear l2 grad loss: 11.288470029830933\n",
                        "linear l2 loss: 0.9800466 Linear l2 grad loss: 11.288470029830933\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.28846526145935\n",
                        "linear l2 loss: 0.9800466 Linear l2 grad loss: 11.28847336769104\n",
                        "linear l2 loss: 0.9800467 Linear l2 grad loss: 11.288481593132019\n",
                        "linear l2 loss: 0.98004675 Linear l2 grad loss: 11.288480520248413\n",
                        "linear l2 loss: 0.9800466 Linear l2 grad loss: 11.28847587108612\n",
                        "linear l2 loss: 0.9800466 Linear l2 grad loss: 11.288470029830933\n",
                        "linear l2 loss: 0.9800464 Linear l2 grad loss: 11.288465142250061\n",
                        "linear l2 loss: 0.9800464 Linear l2 grad loss: 11.28846263885498\n",
                        "linear l2 loss: 0.9800464 Linear l2 grad loss: 11.28846263885498\n",
                        "linear l2 loss: 0.9800464 Linear l2 grad loss: 11.288461565971375\n",
                        "linear l2 loss: 0.9800464 Linear l2 grad loss: 11.288454174995422\n",
                        "linear l2 loss: 0.9800464 Linear l2 grad loss: 11.288454294204712\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288445353507996\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288448691368103\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288459300994873\n",
                        "linear l2 loss: 0.9800463 Linear l2 grad loss: 11.288467288017273\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288456916809082\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288459062576294\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288455486297607\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288455486297607\n",
                        "linear l2 loss: 0.9800463 Linear l2 grad loss: 11.288448452949524\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288455367088318\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288461565971375\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288458108901978\n",
                        "linear l2 loss: 0.9800463 Linear l2 grad loss: 11.288465142250061\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288455486297607\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288462400436401\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288462162017822\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.28844940662384\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.28844940662384\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.28845763206482\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288462162017822\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288456559181213\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288446068763733\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288446068763733\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288448333740234\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288450598716736\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288450598716736\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.28845751285553\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288453221321106\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288466930389404\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288471579551697\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288458585739136\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288482189178467\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288482189178467\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288472771644592\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288472771644592\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.28848659992218\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288485527038574\n",
                        "linear l2 loss: 0.9800463 Linear l2 grad loss: 11.288490653038025\n",
                        "linear l2 loss: 0.9800463 Linear l2 grad loss: 11.288495182991028\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288482069969177\n",
                        "linear l2 loss: 0.9800463 Linear l2 grad loss: 11.28848659992218\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288487792015076\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.28848683834076\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288485407829285\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288485288619995\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.28848671913147\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288492560386658\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.28847587108612\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288476943969727\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.28848659992218\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288488984107971\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288470268249512\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.28846788406372\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288470149040222\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288470149040222\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288487792015076\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288498520851135\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288500905036926\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288492798805237\n",
                        "linear l2 loss: 0.9800463 Linear l2 grad loss: 11.288500666618347\n",
                        "linear l2 loss: 0.9800462 Linear l2 grad loss: 11.288491368293762\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288481831550598\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.28848671913147\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288493871688843\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288502931594849\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288505554199219\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288505554199219\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288496017456055\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288510918617249\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288501381874084\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288499355316162\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288487315177917\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288488388061523\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288488388061523\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.28849172592163\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288488864898682\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288492202758789\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288492321968079\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.28849446773529\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288496971130371\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288498044013977\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288491129875183\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288491010665894\n",
                        "linear l2 loss: 0.9800461 Linear l2 grad loss: 11.288502812385559\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288501620292664\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288497924804688\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288500189781189\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288496732711792\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288490056991577\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.28848147392273\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288483619689941\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288487195968628\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288483738899231\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288494110107422\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288482427597046\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288500308990479\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.28850269317627\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288501143455505\n",
                        "linear l2 loss: 0.980046 Linear l2 grad loss: 11.288501143455505\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288483500480652\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288485527038574\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288485646247864\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.288475275039673\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288477659225464\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288479924201965\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.288476347923279\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.288476347923279\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288495182991028\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288496255874634\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288494110107422\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288484692573547\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288533806800842\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288531064987183\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288531064987183\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288510084152222\n",
                        "linear l2 loss: 0.9800459 Linear l2 grad loss: 11.288519263267517\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288512706756592\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288504600524902\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.2885103225708\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288511157035828\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.28850781917572\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.288513660430908\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288512587547302\n",
                        "linear l2 loss: 0.9800458 Linear l2 grad loss: 11.288516163825989\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288517355918884\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.2885240316391\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288519144058228\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288507461547852\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288511157035828\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.288512110710144\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288508653640747\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.288507342338562\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.28851068019867\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288511991500854\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.288511633872986\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288498163223267\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.288500189781189\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.288500308990479\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288498044013977\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288506269454956\n",
                        "linear l2 loss: 0.9800456 Linear l2 grad loss: 11.28851056098938\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.28851068019867\n",
                        "linear l2 loss: 0.98004574 Linear l2 grad loss: 11.28851854801178\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.28850507736206\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288479208946228\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288517713546753\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288511753082275\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.28850245475769\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288497924804688\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288492798805237\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288497686386108\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.28849995136261\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288490653038025\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288492798805237\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288485884666443\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288484573364258\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28848910331726\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288495063781738\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288485646247864\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288503527641296\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288520216941833\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288520216941833\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288503646850586\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288497924804688\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288501262664795\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288496613502502\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288483500480652\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28848946094513\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288493275642395\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288483738899231\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288498997688293\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288493037223816\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.28849995136261\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288506746292114\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288488388061523\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28848934173584\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288489699363708\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288486957550049\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288492560386658\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288491606712341\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288508176803589\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288493037223816\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288496494293213\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288490533828735\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288505792617798\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28849744796753\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.28851306438446\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288502216339111\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.28849184513092\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288493037223816\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288504600524902\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288527965545654\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288525581359863\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288543105125427\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288532614707947\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288536071777344\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.28854787349701\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.28854787349701\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288546323776245\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288539409637451\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288542985916138\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288543105125427\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288544297218323\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288539290428162\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288542985916138\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288545489311218\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288546562194824\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288545370101929\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288542032241821\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288541793823242\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288542985916138\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.28853452205658\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288530111312866\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288532495498657\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288533329963684\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288531064987183\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.28853452205658\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288532257080078\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288563966751099\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288565039634705\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28854250907898\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288537859916687\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288537859916687\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288529753684998\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288561344146729\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288559079170227\n",
                        "linear l2 loss: 0.98004544 Linear l2 grad loss: 11.288560152053833\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288561344146729\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288561224937439\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288559079170227\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28856110572815\n",
                        "linear l2 loss: 0.9800455 Linear l2 grad loss: 11.288575530052185\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288553357124329\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.28854751586914\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288557529449463\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288556218147278\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288554310798645\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288554310798645\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288558840751648\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288561224937439\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288554191589355\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288554191589355\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288553953170776\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288551568984985\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.28855049610138\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28855276107788\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28855836391449\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288553833961487\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288552641868591\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288552522659302\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288553953170776\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288553833961487\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288555026054382\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.288576126098633\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288549065589905\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288551449775696\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288541913032532\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288544178009033\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288554787635803\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288553714752197\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288534045219421\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28856086730957\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288571238517761\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288575053215027\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288569808006287\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.28856873512268\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288576006889343\n",
                        "linear l2 loss: 0.9800453 Linear l2 grad loss: 11.28856635093689\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.28856635093689\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288559556007385\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288561940193176\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.28855848312378\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.2885662317276\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288563847541809\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288563966751099\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.28855848312378\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288570880889893\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288560509681702\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.28856360912323\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288556694984436\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.288554430007935\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288556814193726\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288560509681702\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.28855311870575\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288556814193726\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288567423820496\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.28858482837677\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288588166236877\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.28858232498169\n",
                        "linear l2 loss: 0.98004526 Linear l2 grad loss: 11.28858494758606\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288582682609558\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288586378097534\n",
                        "linear l2 loss: 0.98004514 Linear l2 grad loss: 11.288581013679504\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.28857159614563\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288571834564209\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288573026657104\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288573026657104\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.28856611251831\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288581252098083\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288581371307373\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288591861724854\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288587093353271\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.28858494758606\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288579821586609\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288588166236877\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288586020469666\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288570404052734\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288583159446716\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288586854934692\n",
                        "linear l2 loss: 0.98004484 Linear l2 grad loss: 11.288573980331421\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288596510887146\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288597583770752\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.28857159614563\n",
                        "linear l2 loss: 0.98004484 Linear l2 grad loss: 11.288584351539612\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288581013679504\n",
                        "linear l2 loss: 0.98004466 Linear l2 grad loss: 11.288568139076233\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.28856110572815\n",
                        "linear l2 loss: 0.98004484 Linear l2 grad loss: 11.288580060005188\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288572907447815\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288572907447815\n",
                        "linear l2 loss: 0.98004484 Linear l2 grad loss: 11.288583636283875\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288585662841797\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288596034049988\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.28860318660736\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288593649864197\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288578629493713\n",
                        "linear l2 loss: 0.98004484 Linear l2 grad loss: 11.288589119911194\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288606643676758\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288606762886047\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288610458374023\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.28860342502594\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288599491119385\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288617134094238\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288618445396423\n",
                        "linear l2 loss: 0.980045 Linear l2 grad loss: 11.288618445396423\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288599967956543\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288619637489319\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288617491722107\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288617491722107\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.28862178325653\n",
                        "linear l2 loss: 0.98004466 Linear l2 grad loss: 11.28859257698059\n",
                        "linear l2 loss: 0.98004484 Linear l2 grad loss: 11.28860056400299\n",
                        "linear l2 loss: 0.98004484 Linear l2 grad loss: 11.288599371910095\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288593649864197\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288593649864197\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288595914840698\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288593769073486\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288594722747803\n",
                        "linear l2 loss: 0.98004496 Linear l2 grad loss: 11.288593649864197\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.288570165634155\n",
                        "linear l2 loss: 0.98004466 Linear l2 grad loss: 11.288574695587158\n",
                        "linear l2 loss: 0.9800448 Linear l2 grad loss: 11.288585186004639\n",
                        "linear l2 loss: 0.98004466 Linear l2 grad loss: 11.288580656051636\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.288563251495361\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.288564205169678\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288559556007385\n",
                        "linear l2 loss: 0.98004466 Linear l2 grad loss: 11.288575768470764\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.28856885433197\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.288575887680054\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.288573622703552\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.28856885433197\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288574814796448\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288575768470764\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288575768470764\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.288573384284973\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288573622703552\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288567543029785\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288560628890991\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288565158843994\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288582801818848\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288584113121033\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288584232330322\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288585305213928\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.288578033447266\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288580656051636\n",
                        "linear l2 loss: 0.9800445 Linear l2 grad loss: 11.288580417633057\n",
                        "linear l2 loss: 0.98004454 Linear l2 grad loss: 11.28858745098114\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288579106330872\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288571953773499\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.28858482837677\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288583755493164\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288581132888794\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288582563400269\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288582801818848\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288583636283875\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288583993911743\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288565874099731\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.28856909275055\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288566946983337\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288565874099731\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288562178611755\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288565874099731\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288562297821045\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288555383682251\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288562178611755\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288562178611755\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288558721542358\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288565635681152\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288564443588257\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288562059402466\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.28856098651886\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288585543632507\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288572788238525\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288585662841797\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288583159446716\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288585424423218\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28854489326477\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288551449775696\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28855288028717\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288555026054382\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288577437400818\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288585782051086\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288583517074585\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288591742515564\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288593888282776\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288591623306274\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288593769073486\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288594961166382\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.28860330581665\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288588047027588\n",
                        "linear l2 loss: 0.98004436 Linear l2 grad loss: 11.288594841957092\n",
                        "linear l2 loss: 0.9800443 Linear l2 grad loss: 11.288601875305176\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288594603538513\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28859567642212\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288583755493164\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288587212562561\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288580417633057\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288580536842346\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28859555721283\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288594365119934\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288583993911743\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288583874702454\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288586020469666\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.28858757019043\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288582921028137\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288588762283325\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288586378097534\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288586616516113\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288582921028137\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28858757019043\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28858757019043\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288571000099182\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288571238517761\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288578152656555\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288594722747803\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288591027259827\n",
                        "linear l2 loss: 0.9800442 Linear l2 grad loss: 11.288609743118286\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28859806060791\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288573265075684\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288573265075684\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.28858733177185\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288587093353271\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288596510887146\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28860592842102\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288585901260376\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288580060005188\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.288590550422668\n",
                        "linear l2 loss: 0.98004407 Linear l2 grad loss: 11.28858733177185\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288570523262024\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.28857171535492\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288580179214478\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288570642471313\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.288583755493164\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.28860342502594\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288591384887695\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288584351539612\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288593530654907\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288585305213928\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288591265678406\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.2885901927948\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.28860330581665\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.28860330581665\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288590431213379\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.288596391677856\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288584470748901\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.288604617118835\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288609027862549\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288607954978943\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288623452186584\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288618445396423\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288602232933044\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288599729537964\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.28861141204834\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.288603067398071\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.28860080242157\n",
                        "linear l2 loss: 0.980044 Linear l2 grad loss: 11.288609743118286\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288611054420471\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288607716560364\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288613438606262\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288613438606262\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.288609743118286\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288610935211182\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.288617968559265\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288606405258179\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288609862327576\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288604974746704\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288594484329224\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288609504699707\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288597822189331\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288602471351624\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288608312606812\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288600087165833\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288605690002441\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288602232933044\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288600087165833\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288610577583313\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288610577583313\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288604497909546\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288601160049438\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288601160049438\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288608193397522\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.28862452507019\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.28861653804779\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288612961769104\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288618683815002\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288621187210083\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288613080978394\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288625717163086\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288611769676208\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288617491722107\n",
                        "linear l2 loss: 0.9800439 Linear l2 grad loss: 11.288629174232483\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288618683815002\n",
                        "linear l2 loss: 0.9800438 Linear l2 grad loss: 11.288619875907898\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288621068000793\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288616180419922\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.28861391544342\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288606762886047\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288619875907898\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288622260093689\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288623213768005\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288604617118835\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288613438606262\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288615942001343\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288614749908447\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288614749908447\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288610339164734\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288610219955444\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288614869117737\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288602948188782\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.28861141204834\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288620710372925\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.28861939907074\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288614988327026\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288620591163635\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288617134094238\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288620829582214\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288624167442322\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288618445396423\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.28860890865326\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288610219955444\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288624286651611\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288634777069092\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288644194602966\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288626551628113\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288627624511719\n",
                        "linear l2 loss: 0.9800437 Linear l2 grad loss: 11.288643956184387\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.28864300251007\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288624048233032\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288635849952698\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288635849952698\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288643836975098\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288630962371826\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288631081581116\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288653135299683\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288634538650513\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288657903671265\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.2886483669281\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288643836975098\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288635730743408\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288627624511719\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288615703582764\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.28861677646637\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288630485534668\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288636922836304\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288641452789307\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.2886381149292\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288633584976196\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.2886483669281\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288628220558167\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288624882698059\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288629531860352\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288625955581665\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288629651069641\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288636445999146\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288640141487122\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288631916046143\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288634896278381\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288633584976196\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288631558418274\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288628935813904\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.28863525390625\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288639903068542\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288638591766357\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288633704185486\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288613677024841\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288652658462524\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288655996322632\n",
                        "linear l2 loss: 0.9800436 Linear l2 grad loss: 11.288669228553772\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288666725158691\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288663148880005\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288668990135193\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.28865385055542\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288655996322632\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288655996322632\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288650274276733\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288652420043945\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288660526275635\n",
                        "linear l2 loss: 0.98004353 Linear l2 grad loss: 11.288658261299133\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288642764091492\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288643836975098\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288643836975098\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288645029067993\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288645267486572\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288649916648865\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288649916648865\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288653254508972\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288660287857056\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.28865909576416\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288665056228638\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.28865933418274\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288666486740112\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288658022880554\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288646697998047\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288650035858154\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288650035858154\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.28865110874176\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.28866159915924\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288668632507324\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288653492927551\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288654685020447\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288655757904053\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288654804229736\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.28865110874176\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.28865122795105\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.28865134716034\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.28866958618164\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288670897483826\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288674592971802\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.288672089576721\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288675785064697\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288676857948303\n",
                        "linear l2 loss: 0.9800434 Linear l2 grad loss: 11.288676857948303\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288668155670166\n",
                        "linear l2 loss: 0.98004335 Linear l2 grad loss: 11.28866982460022\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.28866720199585\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288666009902954\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288663506507874\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288663625717163\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288657903671265\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.28866708278656\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.28866708278656\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.28865659236908\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288681149482727\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288681149482727\n",
                        "linear l2 loss: 0.98004323 Linear l2 grad loss: 11.288680911064148\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.28865396976471\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288666725158691\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288668990135193\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288665413856506\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288667798042297\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288660645484924\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288635969161987\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288637161254883\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288633346557617\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.288644075393677\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288673639297485\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288677215576172\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288677215576172\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288673639297485\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.28868305683136\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288677215576172\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288677334785461\n",
                        "linear l2 loss: 0.9800431 Linear l2 grad loss: 11.288679599761963\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.288655161857605\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.2886643409729\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288652658462524\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.28865373134613\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288663268089294\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.2886723279953\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288659453392029\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288658499717712\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.288684368133545\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.288673758506775\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.28869104385376\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.288686513900757\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.288686394691467\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.28867506980896\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.288670063018799\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288666367530823\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288666367530823\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288633346557617\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288640856742859\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288641810417175\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288650035858154\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288647890090942\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.28865921497345\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.28865098953247\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.28865909576416\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288655638694763\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288655638694763\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.288671970367432\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288670897483826\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288670897483826\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288672924041748\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288649916648865\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.28866159915924\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288668394088745\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.28866732120514\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288668394088745\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.288682341575623\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.288676619529724\n",
                        "linear l2 loss: 0.98004293 Linear l2 grad loss: 11.28867769241333\n",
                        "linear l2 loss: 0.98004305 Linear l2 grad loss: 11.288682341575623\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288676857948303\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288678646087646\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.28868019580841\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288676142692566\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288671493530273\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288655161857605\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288666844367981\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288671493530273\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.28865373134613\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288669109344482\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288667917251587\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288668990135193\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288668990135193\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288673758506775\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288673758506775\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288673639297485\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288662075996399\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288657307624817\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288658618927002\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288658618927002\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288661241531372\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288661003112793\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.28867506980896\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288671255111694\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288666486740112\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288668155670166\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288678407669067\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288700699806213\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288672804832458\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.28867495059967\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288684368133545\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288699507713318\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288727641105652\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288713455200195\n",
                        "linear l2 loss: 0.9800429 Linear l2 grad loss: 11.288720488548279\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288688659667969\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.28869903087616\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288692116737366\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288714528083801\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288718938827515\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288692355155945\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288689732551575\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288684964179993\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.28870141506195\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288710474967957\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.28870141506195\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288700580596924\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288702607154846\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288704872131348\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288702726364136\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288704872131348\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288706183433533\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288702368736267\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288710594177246\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288710832595825\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288710713386536\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288710832595825\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288704872131348\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288700103759766\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288696527481079\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288696527481079\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288703560829163\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288699865341187\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.28869640827179\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.28869616985321\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288698673248291\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288716554641724\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288715600967407\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288723707199097\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288722276687622\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288707375526428\n",
                        "linear l2 loss: 0.98004276 Linear l2 grad loss: 11.288722395896912\n",
                        "linear l2 loss: 0.98004264 Linear l2 grad loss: 11.288710832595825\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288723468780518\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288695335388184\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288694262504578\n",
                        "linear l2 loss: 0.9800426 Linear l2 grad loss: 11.288699984550476\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.288691759109497\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.288692951202393\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.288682222366333\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288665890693665\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288665890693665\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.28866708278656\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.288689136505127\n",
                        "linear l2 loss: 0.98004246 Linear l2 grad loss: 11.288686990737915\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288666725158691\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288677453994751\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.288686871528625\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288684248924255\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.2886962890625\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288680791854858\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288696050643921\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288668155670166\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288669109344482\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.28866446018219\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288679838180542\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288671731948853\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288673877716064\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288682103157043\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.288680791854858\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.28868317604065\n",
                        "linear l2 loss: 0.9800424 Linear l2 grad loss: 11.288714528083801\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288711547851562\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288702011108398\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288702130317688\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288684487342834\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288686871528625\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.28869378566742\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.28869616985321\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288700580596924\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.28869366645813\n",
                        "linear l2 loss: 0.9800423 Linear l2 grad loss: 11.288692474365234\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288698077201843\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288698077201843\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288694620132446\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288689851760864\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288688778877258\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288699269294739\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.28869104385376\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288704991340637\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288697838783264\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288697957992554\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288691997528076\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288692235946655\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288696646690369\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288694381713867\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288696885108948\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288717985153198\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288724899291992\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.28872275352478\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.28872275352478\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288713216781616\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.28870952129364\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288716435432434\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288716554641724\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288724780082703\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.28871750831604\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288715481758118\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288715362548828\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288718700408936\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.28870713710785\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288705825805664\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288710474967957\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.28870940208435\n",
                        "linear l2 loss: 0.98004216 Linear l2 grad loss: 11.288727045059204\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288715243339539\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288718819618225\n",
                        "linear l2 loss: 0.9800421 Linear l2 grad loss: 11.288719892501831\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288702249526978\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.28871476650238\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288700819015503\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288703322410583\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288705587387085\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288715124130249\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.28871488571167\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288703203201294\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288701891899109\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288706660270691\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288709044456482\n",
                        "linear l2 loss: 0.9800418 Linear l2 grad loss: 11.28871488571167\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288719415664673\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288719415664673\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288702130317688\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288713574409485\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288713574409485\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288708686828613\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288724064826965\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288720488548279\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288717031478882\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288720726966858\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288721919059753\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288725137710571\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.28872549533844\n",
                        "linear l2 loss: 0.980042 Linear l2 grad loss: 11.288726329803467\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288726329803467\n",
                        "linear l2 loss: 0.9800418 Linear l2 grad loss: 11.288717150688171\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288717031478882\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288719296455383\n",
                        "linear l2 loss: 0.9800419 Linear l2 grad loss: 11.288719296455383\n",
                        "linear l2 loss: 0.9800418 Linear l2 grad loss: 11.288714528083801\n",
                        "linear l2 loss: 0.9800418 Linear l2 grad loss: 11.288713335990906\n",
                        "linear l2 loss: 0.9800418 Linear l2 grad loss: 11.288719296455383\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288715600967407\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288717865943909\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288723945617676\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288719177246094\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288723826408386\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288726091384888\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288716554641724\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288711905479431\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288704991340637\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.28870713710785\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288694620132446\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288697838783264\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288701295852661\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288701295852661\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.2886883020401\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.28869915008545\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.28870964050293\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288706302642822\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288714408874512\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288713455200195\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288703560829163\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.28869891166687\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288703441619873\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.28870713710785\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288699865341187\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288708329200745\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288698673248291\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.28870689868927\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288718581199646\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288710117340088\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288710117340088\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288710117340088\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288717269897461\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288697481155396\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.28870975971222\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288705587387085\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288684606552124\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288723349571228\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288723349571228\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288723349571228\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288716316223145\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288721799850464\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.28872299194336\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288719177246094\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288717150688171\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288710117340088\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288699269294739\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288698315620422\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288698434829712\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288697242736816\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.28873598575592\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288708806037903\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.28872287273407\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288721799850464\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288746237754822\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288745284080505\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288734674453735\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288734674453735\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288745403289795\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288737058639526\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288755893707275\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288726687431335\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288726329803467\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288743019104004\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288740634918213\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288740634918213\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288738369941711\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288753509521484\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288758039474487\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288758039474487\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288753271102905\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288756728172302\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288756608963013\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288753151893616\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288750767707825\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288750767707825\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.28874146938324\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288740396499634\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.2887624502182\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.28875195980072\n",
                        "linear l2 loss: 0.9800417 Linear l2 grad loss: 11.288758993148804\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288748621940613\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288755416870117\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288746118545532\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288755297660828\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288757801055908\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288748383522034\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.2887624502182\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.28876006603241\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288755655288696\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.2887544631958\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288754105567932\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288757681846619\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288756728172302\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.28875994682312\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288757562637329\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288755416870117\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288743734359741\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.28875720500946\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288756251335144\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288756132125854\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288736462593079\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288751363754272\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288751363754272\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288751363754272\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288749098777771\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288751482963562\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288764715194702\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288763523101807\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288764595985413\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.28875994682312\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288783550262451\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288776636123657\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288772940635681\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288772940635681\n",
                        "linear l2 loss: 0.9800416 Linear l2 grad loss: 11.288776397705078\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288769245147705\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288764476776123\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288763284683228\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288765788078308\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288764476776123\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288763284683228\n",
                        "linear l2 loss: 0.98004144 Linear l2 grad loss: 11.288764476776123\n",
                        "linear l2 loss: 0.9800415 Linear l2 grad loss: 11.288764476776123\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288762927055359\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288732528686523\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288740634918213\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288728952407837\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288750171661377\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288758039474487\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288757920265198\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288753390312195\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288751363754272\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.28875470161438\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288757085800171\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288746237754822\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288737058639526\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288746237754822\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.28874146938324\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288740396499634\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288750767707825\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288755536079407\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288755536079407\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288757801055908\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288757801055908\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288756608963013\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288771629333496\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288761138916016\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288764476776123\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288764476776123\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288771510124207\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288765668869019\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288765668869019\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.28875994682312\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288775086402893\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288777112960815\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288752555847168\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288775205612183\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.28878664970398\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.28876793384552\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288784503936768\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.28877866268158\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288785338401794\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288771390914917\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288771390914917\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288770318031311\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288771390914917\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288778066635132\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288783431053162\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288782238960266\n",
                        "linear l2 loss: 0.9800413 Linear l2 grad loss: 11.288798689842224\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.28878915309906\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288786888122559\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288786888122559\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288786768913269\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288799524307251\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288797497749329\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288791537284851\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.28878915309906\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288797378540039\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.28881025314331\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288803219795227\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288806676864624\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288804292678833\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288805603981018\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288793802261353\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288804054260254\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288796067237854\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288799405097961\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.28879451751709\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288810968399048\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.288808703422546\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288789987564087\n",
                        "linear l2 loss: 0.9800412 Linear l2 grad loss: 11.28880751132965\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288798213005066\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.28879463672638\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288800597190857\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288801550865173\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288805365562439\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288790225982666\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.28880763053894\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288818001747131\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288816928863525\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288802981376648\n",
                        "linear l2 loss: 0.98004115 Linear l2 grad loss: 11.288806319236755\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288807272911072\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288799405097961\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288800477981567\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288801431655884\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288788437843323\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288785099983215\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288795471191406\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288801312446594\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.28879690170288\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288795709609985\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288800358772278\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288808465003967\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288808345794678\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288801431655884\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288801431655884\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288815379142761\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288813948631287\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288814306259155\n",
                        "linear l2 loss: 0.980041 Linear l2 grad loss: 11.288814067840576\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288809418678284\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288808345794678\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288809418678284\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288821339607239\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288794159889221\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288804411888123\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288801193237305\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.28880226612091\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288803458213806\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.28878104686737\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.28878104686737\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288774013519287\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288780689239502\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288775086402893\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288773894309998\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288773894309998\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288773775100708\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288777351379395\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288778305053711\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288784503936768\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288796186447144\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288807988166809\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288780689239502\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288774967193604\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288778424263\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288790464401245\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288790345191956\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288796067237854\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288798213005066\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288798213005066\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288792610168457\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288796305656433\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288819193840027\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288827657699585\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.28882884979248\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.28882884979248\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288814902305603\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288810133934021\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288810133934021\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288812398910522\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288830876350403\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288830876350403\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288828730583191\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288833141326904\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288817882537842\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288827061653137\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288838982582092\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288835406303406\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288837909698486\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288840532302856\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288842797279358\n",
                        "linear l2 loss: 0.98004097 Linear l2 grad loss: 11.288854241371155\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288830876350403\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288842916488647\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288841724395752\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288840293884277\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.28883683681488\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288835763931274\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288847208023071\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.28883683681488\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288839101791382\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288838982582092\n",
                        "linear l2 loss: 0.98004085 Linear l2 grad loss: 11.288855195045471\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288850545883179\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288850665092468\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288849472999573\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288849472999573\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.28884482383728\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.28884482383728\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.28884494304657\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288828730583191\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288846373558044\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288848519325256\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288849592208862\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288849592208862\n",
                        "linear l2 loss: 0.9800407 Linear l2 grad loss: 11.288849711418152\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288843631744385\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288849353790283\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288849472999573\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288856148719788\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288853883743286\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288856148719788\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288856148719788\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288855075836182\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288858413696289\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288859605789185\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288863062858582\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.28886330127716\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.2888503074646\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.2888503074646\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.288849234580994\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.28885042667389\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288845658302307\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288846850395203\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288866877555847\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288858652114868\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288861155509949\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288843274116516\n",
                        "linear l2 loss: 0.98004055 Linear l2 grad loss: 11.288861989974976\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288875102996826\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288875102996826\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288872718811035\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.28888213634491\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288877606391907\n",
                        "linear l2 loss: 0.98004067 Linear l2 grad loss: 11.288877606391907\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288860082626343\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.288860082626343\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.288851141929626\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.28885018825531\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.288851499557495\n",
                        "linear l2 loss: 0.9800405 Linear l2 grad loss: 11.28885531425476\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.288844347000122\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.288846850395203\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.288845539093018\n",
                        "linear l2 loss: 0.98004025 Linear l2 grad loss: 11.288837552070618\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.2888503074646\n",
                        "linear l2 loss: 0.9800404 Linear l2 grad loss: 11.288849115371704\n",
                        "linear l2 loss: 0.9800402 Linear l2 grad loss: 11.288843393325806\n",
                        "linear l2 loss: 0.98004025 Linear l2 grad loss: 11.288835167884827\n",
                        "linear l2 loss: 0.9800402 Linear l2 grad loss: 11.28883409500122\n",
                        "linear l2 loss: 0.98004025 Linear l2 grad loss: 11.28884220123291\n",
                        "linear l2 loss: 0.98004025 Linear l2 grad loss: 11.288846731185913\n",
                        "linear l2 loss: 0.98004025 Linear l2 grad loss: 11.288845658302307\n",
                        "linear l2 loss: 0.9800402 Linear l2 grad loss: 11.288832783699036\n",
                        "linear l2 loss: 0.9800402 Linear l2 grad loss: 11.288830518722534\n",
                        "linear l2 loss: 0.9800402 Linear l2 grad loss: 11.288832426071167\n",
                        "linear l2 loss: 0.9800402 Linear l2 grad loss: 11.288833737373352\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.28881847858429\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.28881847858429\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.2888103723526\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288815021514893\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288811326026917\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.28881824016571\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288818359375\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.28881847858429\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288816213607788\n",
                        "linear l2 loss: 0.9800401 Linear l2 grad loss: 11.288816094398499\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288815975189209\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288815975189209\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288819670677185\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288823246955872\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288816213607788\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288814902305603\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288814902305603\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288814902305603\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288824200630188\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288822889328003\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.2888263463974\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288814663887024\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288831949234009\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288830995559692\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288830757141113\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288829803466797\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288830876350403\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.28881585597992\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.28881585597992\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288830876350403\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288840293884277\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288824915885925\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288838863372803\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288838863372803\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288838982582092\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288856387138367\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.28885281085968\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.28885293006897\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288843631744385\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.28884243965149\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288854956626892\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288848042488098\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288845539093018\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288849115371704\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288828015327454\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288830280303955\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288830399513245\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288827776908875\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288830280303955\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288846731185913\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288841843605042\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288861632347107\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288857102394104\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288853406906128\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288854479789734\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288853526115417\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288853526115417\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288845181465149\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288860321044922\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288863778114319\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288867473602295\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288864970207214\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288863897323608\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288851022720337\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.28886365890503\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288862466812134\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288860201835632\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288869619369507\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288869500160217\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288869500160217\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288868188858032\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288878798484802\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288870811462402\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288870930671692\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.2888685464859\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288893222808838\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288898825645447\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288896560668945\n",
                        "linear l2 loss: 0.9800401 Linear l2 grad loss: 11.288909435272217\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288901090621948\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288898944854736\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288900136947632\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288894176483154\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.28890609741211\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.28890609741211\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288904070854187\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288904070854187\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288913488388062\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288912177085876\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.28890860080719\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.28890872001648\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.28890872001648\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288908958435059\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288908958435059\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288913488388062\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288908839225769\n",
                        "linear l2 loss: 0.9800399 Linear l2 grad loss: 11.288906455039978\n",
                        "linear l2 loss: 0.98004 Linear l2 grad loss: 11.288907647132874\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.28889000415802\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288882970809937\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288888812065125\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.28888750076294\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.28889811038971\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.28888988494873\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288896799087524\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288896560668945\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288894414901733\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.28887927532196\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288894057273865\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288888335227966\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.28890347480774\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.28889286518097\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288901448249817\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288894176483154\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288891792297363\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288892984390259\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288894176483154\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.28889775276184\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288910388946533\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288909196853638\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.28891146183014\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288911700248718\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288911700248718\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288915395736694\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.2889164686203\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.2889164686203\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288928270339966\n",
                        "linear l2 loss: 0.9800398 Linear l2 grad loss: 11.288919925689697\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.28890597820282\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288909316062927\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288910388946533\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288911700248718\n",
                        "linear l2 loss: 0.9800397 Linear l2 grad loss: 11.288909196853638\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288908004760742\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288891553878784\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288887977600098\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288889169692993\n",
                        "linear l2 loss: 0.9800396 Linear l2 grad loss: 11.288891434669495\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288887977600098\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288896203041077\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288897156715393\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288891553878784\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288886547088623\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288886547088623\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288885474205017\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288885474205017\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.2889004945755\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288904309272766\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.28890311717987\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.28891372680664\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288915991783142\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288912773132324\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288902044296265\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288905382156372\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288922667503357\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288916826248169\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.28893530368805\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288931727409363\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.288923740386963\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.28892469406128\n",
                        "linear l2 loss: 0.98003954 Linear l2 grad loss: 11.28892469406128\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288926243782043\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.28891897201538\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288920044898987\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288918852806091\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288920998573303\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288928389549255\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288916826248169\n",
                        "linear l2 loss: 0.9800394 Linear l2 grad loss: 11.288913011550903\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288912057876587\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288920044898987\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.28892207145691\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288922190666199\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288922190666199\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.2889164686203\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.28891408443451\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.28891408443451\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288925766944885\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288927912712097\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288925766944885\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.2889244556427\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288928985595703\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288923263549805\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288923263549805\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288925766944885\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288915514945984\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288931846618652\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.28893268108368\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288918852806091\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.28892707824707\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288919687271118\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288918495178223\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288906812667847\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288918495178223\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.28891134262085\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288907885551453\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288910031318665\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288910388946533\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288908004760742\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288911581039429\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288927674293518\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288911700248718\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.28890335559845\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.28890335559845\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288917183876038\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288923144340515\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288923144340515\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288926839828491\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288926601409912\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.28891634941101\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288934707641602\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288911581039429\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288912773132324\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288934707641602\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288925528526306\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288920879364014\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288939237594604\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288931012153625\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288942575454712\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288941621780396\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288941740989685\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288942813873291\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288941740989685\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288925290107727\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288935780525208\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288935899734497\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288934707641602\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288934707641602\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288956880569458\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288930058479309\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288961410522461\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288946032524109\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.28896188735962\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288964033126831\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288956761360168\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288962841033936\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288966178894043\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288962721824646\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.288962721824646\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288962602615356\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288960456848145\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288960337638855\n",
                        "linear l2 loss: 0.9800391 Linear l2 grad loss: 11.288949847221375\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288942694664001\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.28894293308258\n",
                        "linear l2 loss: 0.98003924 Linear l2 grad loss: 11.288975358009338\n",
                        "linear l2 loss: 0.9800393 Linear l2 grad loss: 11.28897774219513\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288961410522461\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288961410522461\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288962483406067\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.28895914554596\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288936734199524\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288938999176025\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288946032524109\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288946866989136\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288949489593506\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288931846618652\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288942217826843\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288949608802795\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288936257362366\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288936495780945\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288962483406067\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288961291313171\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288942575454712\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288938999176025\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288941025733948\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288929224014282\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288941025733948\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288951635360718\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288954257965088\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288955450057983\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288943767547607\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288952946662903\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.28894567489624\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288951516151428\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288957595825195\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288954138755798\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288951635360718\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288959860801697\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288959860801697\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288952827453613\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288949251174927\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288947105407715\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288951754570007\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288954138755798\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.28895616531372\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288952589035034\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288952589035034\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288949251174927\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288954019546509\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288952350616455\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288945317268372\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288960576057434\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288960695266724\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288975477218628\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288975596427917\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288975596427917\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288959264755249\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288955926895142\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288964033126831\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288957118988037\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288956999778748\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288948893547058\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288938403129578\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288967251777649\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288990497589111\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.288989305496216\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288993000984192\n",
                        "linear l2 loss: 0.98003906 Linear l2 grad loss: 11.288993000984192\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.28898024559021\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288981318473816\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288983702659607\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288974404335022\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.2889883518219\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288985013961792\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288984894752502\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.2889883518219\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288990497589111\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.2889883518219\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288970708847046\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.288962364196777\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288970589637756\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.288970589637756\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.288993000984192\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.2889906167984\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288995385169983\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288997530937195\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.288986206054688\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288984537124634\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288984537124634\n",
                        "linear l2 loss: 0.98003894 Linear l2 grad loss: 11.289008498191833\n",
                        "linear l2 loss: 0.9800388 Linear l2 grad loss: 11.289006233215332\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288994431495667\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.289003610610962\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.289008140563965\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.289004802703857\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.289007067680359\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288984775543213\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288991451263428\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288990497589111\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288991451263428\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.288983464241028\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.28898811340332\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.28898811340332\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.288984298706055\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.288989186286926\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.288992762565613\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288991451263428\n",
                        "linear l2 loss: 0.98003864 Linear l2 grad loss: 11.288991451263428\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.288999557495117\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.289000630378723\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.289001703262329\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.288998365402222\n",
                        "linear l2 loss: 0.98003876 Linear l2 grad loss: 11.289010047912598\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.289003252983093\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.28900420665741\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.28899347782135\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.28899347782135\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.288998126983643\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.288998126983643\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.288999319076538\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.28899598121643\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.288988947868347\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.288994669914246\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.288989901542664\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.288990020751953\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.288994550704956\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.288994550704956\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.28899347782135\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.288994431495667\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.288994550704956\n",
                        "linear l2 loss: 0.98003817 Linear l2 grad loss: 11.288975834846497\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.28897500038147\n",
                        "linear l2 loss: 0.98003817 Linear l2 grad loss: 11.288970112800598\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.288997888565063\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.28900158405304\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.28900158405304\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289005160331726\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289007306098938\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289003729820251\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289000153541565\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.288982629776001\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.28898274898529\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.288982629776001\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.288997769355774\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.2890123128891\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.2890123128891\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.289018154144287\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289011001586914\n",
                        "linear l2 loss: 0.9800386 Linear l2 grad loss: 11.289014339447021\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.289007306098938\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.289007306098938\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.289007425308228\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289010763168335\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289009928703308\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.289013147354126\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289007544517517\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289007544517517\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289013266563416\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.28901207447052\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289016604423523\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289013147354126\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289005994796753\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289004921913147\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289015650749207\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289017915725708\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289019107818604\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.28902018070221\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289015650749207\n",
                        "linear l2 loss: 0.9800381 Linear l2 grad loss: 11.289000391960144\n",
                        "linear l2 loss: 0.9800381 Linear l2 grad loss: 11.28900158405304\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289016723632812\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289022326469421\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289015412330627\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289027333259583\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289009690284729\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289028525352478\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289028525352478\n",
                        "linear l2 loss: 0.98003846 Linear l2 grad loss: 11.28903079032898\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289027214050293\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289024591445923\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289024710655212\n",
                        "linear l2 loss: 0.98003817 Linear l2 grad loss: 11.289016366004944\n",
                        "linear l2 loss: 0.98003834 Linear l2 grad loss: 11.289021372795105\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289025068283081\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289023637771606\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289024829864502\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289024710655212\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.28902006149292\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289026618003845\n",
                        "linear l2 loss: 0.9800383 Linear l2 grad loss: 11.289026618003845\n",
                        "linear l2 loss: 0.9800381 Linear l2 grad loss: 11.28900921344757\n",
                        "linear l2 loss: 0.98003817 Linear l2 grad loss: 11.289016008377075\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.288995265960693\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.288995265960693\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.28899621963501\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.288986682891846\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.288990259170532\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.288990139961243\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.288989067077637\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.288989067077637\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.288987874984741\n",
                        "linear l2 loss: 0.9800381 Linear l2 grad loss: 11.28902554512024\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289020657539368\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289019107818604\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289022326469421\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289022326469421\n",
                        "linear l2 loss: 0.9800381 Linear l2 grad loss: 11.289027571678162\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289021849632263\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.28902280330658\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.288998365402222\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289021730422974\n",
                        "linear l2 loss: 0.9800381 Linear l2 grad loss: 11.289029836654663\n",
                        "linear l2 loss: 0.9800381 Linear l2 grad loss: 11.289029836654663\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289026498794556\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289026498794556\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289021611213684\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289024233818054\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.28902518749237\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289026379585266\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289026498794556\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289027690887451\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289029955863953\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289039492607117\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289034485816956\n",
                        "linear l2 loss: 0.980038 Linear l2 grad loss: 11.289029836654663\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289003610610962\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.289004802703857\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.289004802703857\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289016366004944\n",
                        "linear l2 loss: 0.98003787 Linear l2 grad loss: 11.289015412330627\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.28900945186615\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.289010643959045\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.289010763168335\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.289010763168335\n",
                        "linear l2 loss: 0.9800377 Linear l2 grad loss: 11.289013981819153\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.289004802703857\n",
                        "linear l2 loss: 0.9800378 Linear l2 grad loss: 11.28900945186615\n"
                    ]
                }
            ],
            "source": [
                "linear_l2_grad_norm = float(\"inf\")\n",
                "iteration = 0\n",
                "\n",
                "while linear_l2_grad_norm > tolerance_optimal and iteration < max_steps_optimal:\n",
                "    linear_l2_loss = 0\n",
                "\n",
                "    data_loader_weights_optimal.shuffle()\n",
                "    for batch_weights in data_loader_weights_optimal:\n",
                "        pbo_optimal_linear.params, pbo_optimal_linear.optimizer_state, l2_loss, l2_grad_loss = pbo_optimal_linear.learn_on_batch_on_weights(pbo_optimal_linear.params, pbo.optimizer_state, batch_weights)\n",
                "        linear_l2_loss += l2_loss\n",
                "\n",
                "    iteration += 1\n",
                "\n",
                "    # Visualization\n",
                "    if iteration % plot_freq == 0:\n",
                "        linear_l2_grad_norm = 0\n",
                "        for layers in l2_grad_loss.values():\n",
                "                for grad in layers.values():\n",
                "                    linear_l2_grad_norm += np.linalg.norm(grad)\n",
                "\n",
                "        print(\"linear l2 loss:\", linear_l2_loss, \"Linear l2 grad loss:\", linear_l2_grad_norm)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train PBO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 2 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from pbo.utils.shared_axis_plot import shared_axis_plot\n",
                "\n",
                "\n",
                "# For visualization\n",
                "full_batch = {\n",
                "    \"state\": replay_buffer.states,\n",
                "    \"action\": replay_buffer.actions,\n",
                "    \"reward\": replay_buffer.rewards,\n",
                "    \"next_state\": replay_buffer.next_states,\n",
                "}\n",
                "l1_losses = []\n",
                "\n",
                "iteration = 0\n",
                "l1_loss = float(\"inf\")\n",
                "bellman_iterations = jnp.arange(max_bellman_iterations + 1)\n",
                "\n",
                "while l1_loss > tolerance and iteration < max_steps:\n",
                "    cumulative_l2_loss = 0\n",
                "    \n",
                "    data_loader_weights.shuffle()\n",
                "    for batch_weights in data_loader_weights:\n",
                "        data_loader_samples.shuffle()\n",
                "        for batch_samples in data_loader_samples:\n",
                "            pbo.params, pbo.optimizer_state, l2_loss = pbo.learn_on_batch(pbo.params, pbo.optimizer_state, batch_weights, batch_samples)            \n",
                "            cumulative_l2_loss += l2_loss\n",
                "\n",
                "    # Visualization\n",
                "    if iteration % plot_freq == 0:  \n",
                "        l1_loss = pbo.loss(pbo.params, data_loader_weights.weights, full_batch, ord=1)\n",
                "\n",
                "        l1_losses.append(l1_loss)\n",
                "\n",
                "    iteration += 1\n",
                "\n",
                "shared_axis_plot(l1_losses, np.array(l1_losses) * np.nan, f\"every {plot_freq} iteration\", \"l1 loss\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "iterated_weights = weights.copy()\n",
                "\n",
                "for i in range(max_bellman_iterations):\n",
                "    iterated_weights = pbo.network.apply(pbo.params, iterated_weights)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "DeviceArray(2.9802103, dtype=float32)"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "jnp.linalg.norm(optimal_q - q.to_params(iterated_weights)[\"TableQNet\"][\"table\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Performances of the operators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'LinearPBO' object has no attribute 'l1_loss'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "\u001b[1;32m/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000025?line=0'>1</a>\u001b[0m l1_loss_optimal_linear \u001b[39m=\u001b[39m pbo\u001b[39m.\u001b[39;49ml1_loss(pbo_optimal_linear\u001b[39m.\u001b[39mparams, data_loader_weights\u001b[39m.\u001b[39mweights, full_batch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000025?line=1'>2</a>\u001b[0m l1_loss_optimal \u001b[39m=\u001b[39m pbo_optimal\u001b[39m.\u001b[39ml1_loss(pbo, q, data_loader_weights\u001b[39m.\u001b[39mweights, full_batch, max_bellman_iterations, add_infinity)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000025?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39ml1 loss\u001b[39m\u001b[39m\"\u001b[39m, l1_loss)\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'LinearPBO' object has no attribute 'l1_loss'"
                    ]
                }
            ],
            "source": [
                "l1_loss_optimal_linear = pbo.l1_loss(pbo_optimal_linear.params, data_loader_weights.weights, full_batch)\n",
                "l1_loss_optimal = pbo_optimal.l1_loss(pbo, q, data_loader_weights.weights, full_batch, max_bellman_iterations, add_infinity)\n",
                "\n",
                "print(\"l1 loss\", l1_loss)\n",
                "print(\"l1 loss optimal linear\", l1_loss_optimal_linear)\n",
                "print(\"l1 loss optimal\", l1_loss_optimal)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "q_thin = Theoretical3DQ(\n",
                "    network_key=q_network_key,\n",
                "    random_weights_range=None,\n",
                "    random_weights_key=random_weights_key,\n",
                "    action_range_on_max=10 * action_range_on_max,\n",
                "    n_actions_on_max=10 * n_actions_on_max,\n",
                ")\n",
                "\n",
                "pbo_thin = LinearPBO(pbo_network_key, q_thin, learning_rate, max_bellman_iterations, add_infinity)\n",
                "\n",
                "l1_loss_optimal_thin = pbo_optimal.l1_loss(pbo_thin, q_thin, data_loader_weights.weights, full_batch, max_bellman_iterations, add_infinity)\n",
                "\n",
                "print(\"l1 loss optimal thin scale\", l1_loss_optimal_thin)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize iterations on weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def add_points(ax, points, size, label, color):\n",
                "    xdata = points[:, 0]\n",
                "    ydata = points[:, 1]\n",
                "    zdata = points[:, 2]\n",
                "    ax.scatter3D(xdata, ydata, zdata, s=size, label=label, color=color)\n",
                "\n",
                "fig = plt.figure(figsize=(7, 7))\n",
                "ax = fig.add_subplot(111, projection='3d')\n",
                "sizes = [1, 5, 300, 1000]\n",
                "colors = [\"black\", \"b\", \"red\", \"g\"]\n",
                "iterated_weights = weights_buffer.weights\n",
                "\n",
                "for iteration in range(4):\n",
                "    add_points(ax, iterated_weights, sizes[iteration], f\"iteration {iteration}\", colors[iteration])\n",
                "    iterated_weights = pbo_optimal(iterated_weights)\n",
                "\n",
                "ax.set_xlabel('k')\n",
                "ax.set_xticklabels([])\n",
                "ax.set_xticks([])\n",
                "\n",
                "ax.set_ylabel('i')\n",
                "ax.set_yticklabels([])\n",
                "ax.set_yticks([])\n",
                "\n",
                "ax.set_zlabel('m')\n",
                "ax.set_zlim(-2, 5)\n",
                "\n",
                "ax.legend()\n",
                "ax.view_init(0, 0)\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(7, 7))\n",
                "ax = plt.axes(projection='3d')\n",
                "\n",
                "iterated_weights = weights_buffer.weights\n",
                "\n",
                "for iteration in range(4):\n",
                "    add_points(ax, iterated_weights, sizes[iteration], f\"iteration {iteration}\", colors[iteration])\n",
                "    iterated_weights = pbo_optimal_linear.network.apply(pbo_optimal_linear.params, iterated_weights)\n",
                "\n",
                "ax.set_xlabel('k')\n",
                "ax.set_xticklabels([])\n",
                "ax.set_xticks([])\n",
                "\n",
                "ax.set_ylabel('i')\n",
                "ax.set_yticklabels([])\n",
                "ax.set_yticks([])\n",
                "\n",
                "ax.set_zlabel('m')\n",
                "ax.set_zlim(-2, 5)\n",
                "\n",
                "ax.legend()\n",
                "ax.view_init(0, 0)\n",
                "fig.tight_layout()\n",
                "print(\"Contracting facteur\", jnp.linalg.norm(pbo_optimal_linear.params[\"LinearPBONet/linear\"][\"w\"], ord=1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(7, 7))\n",
                "ax = plt.axes(projection='3d')\n",
                "\n",
                "iterated_weights = weights_buffer.weights\n",
                "\n",
                "for iteration in range(4):\n",
                "    add_points(ax, iterated_weights, sizes[iteration], f\"iteration {iteration}\", colors[iteration])\n",
                "    iterated_weights = pbo.network.apply(pbo.params, iterated_weights)\n",
                "\n",
                "ax.set_xlabel('k')\n",
                "ax.set_xticklabels([])\n",
                "ax.set_xticks([])\n",
                "\n",
                "ax.set_ylabel('i')\n",
                "ax.set_yticklabels([])\n",
                "ax.set_yticks([])\n",
                "\n",
                "ax.set_zlabel('m')\n",
                "ax.set_zlim(-2, 5)\n",
                "\n",
                "ax.legend()\n",
                "ax.view_init(0, 0)\n",
                "fig.tight_layout()\n",
                "print(\"Contracting facteur\", jnp.linalg.norm(pbo.params[\"LinearPBONet/linear\"][\"w\"], ord=1))"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "1432d270abb514d077d760a4c8d2edd41cc0752b595b0513fca29951003961c1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('env': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
