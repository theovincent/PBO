{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PBO learnt on several iterations and one weigth\n",
                "\n",
                "## Define parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import jax\n",
                "\n",
                "# keys\n",
                "seed = 1\n",
                "key = jax.random.PRNGKey(seed)\n",
                "env_key, key = jax.random.split(key)\n",
                "shuffle_key, q_network_key, random_weights_key, pbo_network_key = jax.random.split(key, 4)\n",
                "\n",
                "# Box over states and actions\n",
                "n_states = 20\n",
                "n_actions = 2\n",
                "sucess_probability = 0.9\n",
                "gamma = 0.8\n",
                "\n",
                "# Weights collection\n",
                "n_weights = 1\n",
                "filtering_weights = True\n",
                "\n",
                "# PBO training\n",
                "tolerance = 0.1\n",
                "max_steps = 2000\n",
                "batch_size_samples = 20\n",
                "batch_size_weights = n_weights\n",
                "learning_rate = {\"first\": 0.001, \"last\": 0.00005, \"duration\": max_steps}\n",
                "max_bellman_iterations = 5\n",
                "add_infinity = False\n",
                "importance_iteration = [1] * (max_bellman_iterations + 1)  # [0.01, 0.05, 0.1, 0.5, 1, 5]\n",
                "\n",
                "# Optimal linear PBO training\n",
                "tolerance_linear = 1e-5\n",
                "max_steps_linear = 5000\n",
                "batch_size_weights_linear = n_weights\n",
                "learning_rate_optimal_linear = {\"first\": 0.001, \"last\": 0.00005, \"duration\": max_steps_linear}\n",
                "\n",
                "# Visualisation\n",
                "plot_freq = 2\n",
                "sleeping_time = 0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "from pbo.environment.chain_walk import ChainWalkEnv\n",
                "\n",
                "states = np.arange(n_states)\n",
                "actions = np.arange(n_actions)\n",
                "states_boxes = (np.arange(n_states + 1 + 1) - 0.5)[:-1]\n",
                "actions_boxes = (np.arange(n_actions + 1 + 1) - 0.5)[:-1]\n",
                "\n",
                "env = ChainWalkEnv(env_key, n_states, sucess_probability, gamma)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collect samples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Samples on the mesh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "\n",
                "from pbo.sample_collection.replay_buffer import ReplayBuffer\n",
                "\n",
                "\n",
                "n_samples = n_states * 2\n",
                "replay_buffer = ReplayBuffer()\n",
                "\n",
                "for state in states:\n",
                "    for action in actions:\n",
                "        env.reset(jnp.array([state]))\n",
                "        next_state, reward, _, _ = env.step(jnp.array([action]))\n",
                "\n",
                "        replay_buffer.add(jnp.array([state]), jnp.array([action]), reward, next_state)\n",
                "\n",
                "replay_buffer.cast_to_jax_array()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ0AAAEYCAYAAABGLPQJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAl90lEQVR4nO2debxd49XHvz8ZiEhERL1IDDVHVJSiRWseokVbNVVJ5+qgilbV2+qkg+prqH4MVVKihibSV1HkJaFKzAQ1R0hiiIgIQSL3rveP9Zxk35Nzzj7n3nPP2dP389mfu+/ez957nX3Weaa11rNkZhQUtJKV2i1AQf4olK6g5RRKV9ByCqUraDmF0hW0nELpClpOppRO0k8ljW+3HL2NpB9JurjG+c9LuqWVMjVCU5RO0i6S7pL0pqT5kv4t6SPNuHfekbSbpNnRY2b2KzP7Sji/oSST1Ddy/goz26fVstZL3/gitZE0GLgeOBa4BugP7Aos7um9k4ikvma2tFXPasVzWo6Z9WgDtgcW1Di/MXAb8DowD7gCGBI5PxP4PjAdWAT8GVgb+CfwFvB/wBqh7IaAAV8DXgJeBk6K3OunwPjI/zsBdwELgEeA3SLnxgIzwjOeBz5fRf6fAhOA8cBC4CvA6kHOl4E5wC+BPpH7/hs4D3gTeBLYM3K/LwJPhOfOAL4eObcbMBs4GXgF+BvwLtAJvB22daOfE3gxvJPS+Y8GGe6M3PdjwH1BnvuAj0XOTQV+EWR+C7gFGNZTvaipM01QusFBof4C7F9SkMj5TYC9gZWBtYA7gLPLlG5aULT1gLnAg8C2wCq4wp5WpnRXAgOBrYHXgL3KlS7c63VgDN6N2Dv8v1a4diGweSi7DrBVDaV7Hzg43GcAMAm4MNznA8C9JeUJX/hS4HtAP+Cw8GUPDecPwH+IAj4BvAN8OKJ0S4Hfhvc1oKSIFWQaX/ZO+pb9oO4M+0OBN4Av4C3bEeH/NSNK9xywWXjeVOA3val0Pe7TmdlCYJfwwf8EvCbpOklrh/PPmtlkM1tsZq8B/xNedpQ/mNmrZjYH+Bdwj5k9ZGbv4V/wtmXlf2Zmi8zsUeDS8CLLOQq40cxuNLNOM5sM3I8rIXjtMUrSADN72cwer/Ex7zazv5tZJ/4jGwMcH2SYC5wFHB4pPxf/Yb1vZlcDT+HKhpndYGbPmXM7XrPsGrm2E/+RLTazd2vIVC8HAM+Y2eVmttTMrsRr309FylxqZk+H510DjG7Cc6vSlIGEmT1hZmPNbDgwCm8CzgaQtLakqyTNkbQQb6aGld3i1cj+uxX+X62s/KzI/gvheeVsAHxO0oLShv841jGzRXgN9A3gZUk3SNqixkeMPm8DvAZ7OXLfC/Ear8QcC9VIuYyS9pc0LQy4FuAKHH0fr4UfW7NYNzw/ygt4S1Dilcj+O6z4vptK06dMzOxJYByufAC/wmvBrc1sMF4DqYePGRHZXx/v35UzC7jczIZEtoFm9psg581mtjfetD6J19LViCrQLHyQNCxy38FmtlWkzHqSop9xfeAlSSsDE4EzgbXNbAhwI13fR7nbT5wbUNz5l/AfSpT18b5oW+ix0knaQtKJkoaH/0fgzd20UGQQ3sF9U9J6+KChp/xY0qqStsI75ldXKDMe+JSkfSX1kbRKmH4YHmrfgyQNxBXobbxZi8XMXsabxN9LGixpJUkbS4p2GT4AHCepn6TPAVviytUf76u9BiyVtD8QN7XxKrCmpNWrnH8tyP7BKudvBDaTdKSkvpIOA0biMw6xSJopaWw9ZeulGTXdW8COwD2SFuHK9hhwYjj/M+DDeGf6BuDaJjzzduBZ4FbgTDNbYSLUzGYBBwE/wr+YWbjCrxS2E/BaYD7exzy2gecfjSvQf/BO+QS8xixxD7ApPlo/HTjEzF43s7eA4/B+0xvAkcB1tR4UWo4rgRmhOV+37Pw74Rn/Dud3Kjv/OvBJ/Pt4HfgB8Ekzmxf3ISX1B9ZkeQXSFNS165FsJG2IT2/0sxbNlTVKqBW+Yma7tFuWniJpF+BbZlZpoNZtsjn5WNAUzOxO4M5m3zdTtteCdJCq5rUgGxQ1XUHLKZSuh0i6QNKPa5w3SZs06VnjJP2ySfeaKukrzbhXo8QqnaShkiZJWiTpBUlHRs5tI+lxSfMknRA53k/SPWHOLnU08oWY2TfM7Be9IMNYSU3vxCeBekavfwSW4Ab50cANkh4JtspfAyfhHiLTJf3VzF7B58AmhrmypiCpj5l1NOt+BW2kljcA7kWxBNgscuxyghcC7qKzctifBuyAm1zuxefS4u7/N9zu9ybufbJV5Nw44Hx8Rn0RsBduR5yIT/Y+DxxX496rA5eFsi8A/w2sZGVeGlbmqYFPtHYA7+GWivNwM9VZuCF/IfAoMCoi5y8j9/o+7vL0EvClcN9NwrmVcRPYi7il4QJgQAXZtwzP7wgyLIg864/4JPtb+CT0xpHrtgAm4xPeTwGH1ng/U/FK497wmf6X4AkTzh8IPI67hU0FtgzHTw7P7Rv+PzaUWyXu+1527xil2BZ4p+zYScA/IkrzKWB4UJ41gb8Dn6jr4f6lDApfxtnAw2VK9yawM94NWBV4APgJbg34IO6Ptm+Ve18WXuSgoFRPA1+OU7rIF/KVyPl9w7OH4Aq4Je440EXpgP2CMo3Cf7B/LVO6s3ALxNAg1z+AX1eRfywRn7jIs17Hf9x9cd/EqyIVxCzcLNg3fHfzgJE1lG5ORNaJLHeX2gz/oe+NOzf8ALcA9Q/fxR3hHW6KW1a2rVfh6lG6XYFXyo59FZga9jfAa6IHcXvrgXhNuH74wm8HPlenAg4JX9DqkRd8WeT8jsCLZdecgrvllN+rD15Dj4wc+3pE7kaVbg9caXci1JZlilBSukuI+KKFL89wn0KFLzJaM30UeL5Bpbs48v8Y4Mmwfxjwr7LyFxJ8EasoXVTWkeGd9QF+DFwTObdSUNDdIu9rPt7SndKIwplZbJ/ubdx/LMpgvGrHzF4IHxxJqwJ34wbsP+BG+BuAxyTdambzozeR1Advyj6HO1aWDO7D8BoOVnQpWje4A5Xog/vflTMM/4VGXXrK3Xnqxsxuk3Qe3rRtIOla3GN5YVnRdfEaMfrMEmsRauuIA4rCZ2iEam5IGwA7lr2fvnglUI1yF7F++Lvr4g5lZp2SZhHen5nNlDQF/+7/2KD8saPXp4G+kjaNHNsGb8PL+QnwJzN7Fffovd/M3sTdrytNGRyJG+T3wvtfG4bj1dx8ZuG1wpDINsjMxrAi83Bv36hLT9SdZxGuACX+q+z6FWbMzexcM9sOrxE2o7K3zMus6HYVleldvN9akn91M6vmu9borP0s4Pay97OamdVyZCiX9f0gZxd3qOCmNYLw/iQdgNfStwK/a1DO2kpn7ux4LfBzSQMl7YwrSpdfj6SRuFv1+eHQ88AewXt4U7zjXM4g3K3odVwBfhUj673AW5JOljQguCuNqhR1Zj7KvQY4XdIgSRvgI+pSeOLDwMclrR9chk4pu8WrRFyFJH1E0o6S+uEK+x6VXaGuAcZKGhlq/tMiMnXiPntnSfpAuO96kvat8nlfBYYHT496uB53YfpCmLLqF+TessY1R0Vk/TkwIfLuDpC0Z/jMJ+Lf1V2ShgEX47Eix+DuY5V++NWpo681FB8cLMKV58gKZaYAO0b+3wZ3+5kHnFDlvqvh/b638Kr8aLp2uscRGRWGY+vibj6v4B3YaYT4iAr3XwNXspJb00+I9MfwZmEB3kH+Kl37dB/Fa/k3gHOBPfFpobdZHly0WiU5gR8G+SqNXlfBf1wz8BHjE1QZgeOd9hvwvtO8Ks/ajUj8BLB5uOY1/Md8GzC6Rp8uOnr9B5GAHODT4Tt8E++bbxWOXwtcECm3f/ispZiLt4Fda+lUYXstaDmFGayg5WRa6SRdImmupMcix7aRdLekRyX9Qx4sngsqvY+y81uEd7NY0kll5/aT9JSkZyX9MHJ8o2DyfFbS1fX0QTOtdHgfaL+yYxcDPzSzrfHwxmbEbKSFcaz4PqLMx93pz4weDNNbf8T7byOBI8LgETxG9ywz2wTvA385TohMK52Z3YG/yCib4TPq4Cajz7ZUqDZS5X1Ez881s/vwqZMoOwDPmtkMM1sCXAUcFKZS9sBjRMAD7g+OkyOP7uqP49M+f8cnpqt6wkj6Gr6EBerff7t+a3+gWtFlLJk1e56ZrdUUSQP77j7QXp8f7+vwwPTFj+PTOSUuMrOLmiDCenSdSJ6NW4jWxO3CSyPHYyfg86h0XwLODT5w1+Gmn4qEL+wigJXXH2Hrnfi92Js/f/yJ5YHNPWbe/A7uuXl4bLl+6zz3nplt3+znN5vcKZ15SN8+AJI2Iyz3kGyMDqsrLLe3mEPXFmF4OPY6METLV7IqHa9Jpvt0lYhYA1bC3Z0uaK9E8RjQicVuvch9wKZhpNofX7flOvNJ3inAIaHcMfiEf00yXdNJuhKftR8mX1jwNGA1Sd8KRa7FF+BJNIbxfhP8V6u8j34AZnaBpP/CFxkaDHRKOh731Fko6dvAzbiDwiW2fMGhk4Grghv9Q/gSajXJtNJZ9SDhc1oqSBNoRk1W432Uzr+CN5GVzt2Iu7GVH5+Bj27rJtNKlxUM6Ojd5rOlFEqXEnq5z9ZSMj2QqGIGGy1fH+5hSfdLaqhpaAcGvG8Wu6WFTCsdlc0+Z+AreY7G3Z3OaLFMDWMYHXVsaSHTzauZ3SFf6anLYZa74K9O5QUVk4VBR3p0KpZMK10VjgdulnQmXtN/rFrBqBmszxprtES4Svg8XXbIevNaiWOB75nZCHwF9KrzSmZ2kZltb2bb91ltYMsEXEEOxPsWv6WFPCrdMSxfDfRvNDjH1C46UOyWFvKodC+xPKXAHsAzbZSlLnyeLjtKl+k+XRWzz1eBc+QpkN4j9NmSTmeKms84Mq10Ncw+27VUkB7SiVjScEx2csm00mWJoqYraCmlPl1WyLTSSboEz6Ew18xGhWNX40HJ4Iv2LAjWiQQjOiw7Y75MKx1uBjsPXzYMADM7rLQv6fcsX6wnsRjwftGnSwdVzGDAskVhDsWnTRKNWVHTZYVdgVfNrOo8XVLMYOAj2KyQnZ9P4xyBL8ZTleSYwaCDlWK3OOqI8Jekc0O0/nRJHw7Hdw+uYKXtPUkHh3PjJD0fOTc6To5c1nRhYvgzpGS+zm2vTfmqxlHWxy1jf3xpt03xuNbz8dW4phASD0saiq90FU0C+H0zm0Cd5LWm2wtfNnV2uwWplw5T7BZHXIQ/HoR+mTnT8PDCdcrKHAL80zz7YrfItNIFM9jdwOaSZksqrbNxODFNa5IwVG/zOix4Q5e2Rk18lSL5yyP2K72700NzfJY8kXJNMt28VjODmdnYFovSYzrrG73O680I/1DrbY2HIpY4BV8Esj++GsLJ+KqeVcm00mWFTsQSa8k8XbVI/hKHApPMbNkCO+aZvgEWS7oUT/lQk6w3rxVHa5K+I+lJeYqpxMdIAHSyUuzWBK4Djg6j2J2ANyNKBRVG/KU+X5j3PBjPal6TrNd04ygbrUnaHe8wb2Nmi0vLTCQZM5oyORwX4Y8HU4/BR6fv4IlQStduiNeCt5fd9gpJa+Gr4j8MfCNOjkwrXRWLxLF40o7FoczclgvWMGrK5HAdEf4GfKvKuZlUWAbMzBq26GS6ea3CZsCuYcnS21UhJUDSMGCJ9Y3d0kJ6JG0effE0BTsBHwGukfRBq7DMfFLMYIYy5U+Xx5puNnBtmAC9F4/uG1apYFLMYNAcM1hSSI+kzePvwO6wbFHE/nhCksRi+Dxd3JYWMt28VhmtXQJcEqZRlgDHVGpak4TbXgt/ulRQY7R2VEsFaQKFu3pBSzFTqprPOAqlSwlZ8hzOziepQJX16X4qaU7E6bCxtJFtoNSni9vSQqaVjuppic4ys9FhW2Ed3aTho1fFbmkh081rrcCctJGmebg4svNJGuPbwenwEklVTQ2SvlZyiOx4e1Er5etCySKRlZouj0p3PrAx7vP/MvD7agWTYpEwI1N9ukw3r5Uws1dL+5L+BFzfRnHqJk01WRy5UzpJ60QcEz9NHU6H7cab1+w0SplWuipmsN1CbKYBM4Gvt0u+RigsEimhihksNndV0jDE0s6e99kqLShUdl54CqsxuOfwWDN7MJzrAB4NRV80swPD8Y3wpMNrAg8AXwiJiKuSnTo743QG7+FaWx2Mo3Y69Wiw9dfwQVeJdyNzmwdGjhfp1LOIx0gkJth6GaFmbDideqaVrtbaHZJOlGSSKjpwJo0W+dPVCrZeJcxZTiutY0KRTr0i46iwdoekEXh26xfbIFPDGGJpfUo1TNL9kf8vCinhm8EGZjZH0geB2yQ9SjfX9su00tUwg50F/IA6sjAngZLttQ56GuFfNdjazEp/Z0iaCmwLTKRIpx6PpIOAOWb2SB1lE2EGg5Y1rxWDrSWtUVqjJHRHdgb+U6RTrwNJqwI/wpvWWELTdBHAyuuPaJtLu1ndzWtNehBsvSVwoaROvKL6jZn9J5wr0qnHsDGwEfCID7wYDjwoaYeQSjyxNMMM1t1gazO7C184p9I1RTr1WpjZo8CyZSQkzQS2N7MURINlxyKR6T5djfXpUkeWXJsyXdPV0Zxs2CJRekQDUyapINNKlxksW81roXQpoOjTpYgq0WC/CK7qD0u6RdK67ZSxXrLUp8u00lHZq+J3ZvahkA/seuAnrRaqUQzR0blS7JYWMt28VjKDmdnCyL8D8dYr8WQpY06mla4akk4HjsYN1rvXKJeM9ekyNpBIT53cRMzsVDMbAVwBfLtGuUREg7ksit3SQi6VLsIVwGfbLUQ82erTpUfSJiFp08i/BwFPtkuWeimWlUgRVbwqxkjaHF/29QXqWIK+7Zj367JCppUuK9FgUIxeC1pMaZ4uKxRKlxKy1Lxm5+dTgSpmsN+FvGDTJU2SNKSNItZNMWWSHsaxohlsMjDKzD4EPI2njkw0Zs1RulohmeG8VDmd+mhJd8sT+E2XdFjkmobTqWda6SoFF5vZLZE4zWm4y3ri6ehU7FYH4+hehP87wNFmtlW4/uyyFuL7kej/h+OEyHuf7kvA1dVOJsUMBjSl+axjZdJlEf7ANElDwipXT0fu8ZKkucBawILuyJHpmq4Wkk4FluJWiYokxQxmxDetQSl7PZ26pB3wLEPPRQ4X6dTjkDQWX71oz6RnyylRp5CtSKd+OZ5lqDMcbjideu5qOkn74dH9B5rZO+2Wpy4MrFOxWxOoGuEvaTBwA3BqWFzHRTN7OSy4sxi4lDrCETOtdFWiwc4DBgGTw2jrgrYKWSctmjKpFuHfH5iE9/cmRC9QkU69K1kygzWjE9CDCP9DgY8Da4auCfiCiQ9TpFPPJkbTRq/djfAfD4yvck2RTj1KFYvE58IkZ6ekXut0N5XW9elaQqaVjsqToY8BnwHuaLk0PcHq2FJCppvXKoE5TwCEBXRSQrpsq3FkWul6SpIsEmmqyeLIevPaI5Jikchan66o6VJDepQqjkLp0kLRvLYWSWdIGiypn6RbJb0m6ag6rlvBIiHp02Fi9KPADZJu7m35m0Ixem05+5jZDyR9Gs/nVZryqDhhWaLGZOik5orXy4Q+XVZIi9KV5DwA+JuZvZmuKY8mkKKaLI60KN31kp4E3gWODba+99osU2vJ0DxdKvp0ZvZD4GP4otTv48bog+Kuq2IGGyppsqRnwt82T8DVhyx+SwupUDp5/odvstxnf12gHrvpOFY0g/0QuNXMNgVuDf8nGxN01rGlhFQoHe4cuASv7cAdC38Zd1GVrH8H4dn6oM6sfYkgQ6PXtCjdxmZ2BvA+QPD47e5Pe+1IOvVXgLWrFVSC0jQVStd6lkgaQHi1kjYGFvf0psF/rOrXlRgzGBRK1wZ+CtwEjJB0Bd4XO7mb93o14mK9DjC3KRL2JgbqVOwWR3eDrcO5Y8Lg6xlJx0SObyfp0XDNuapjLisVSmdmt+ATwmOBK/FR7JRu3u46PFsf1Jm1LxE0p6YbRzeCrSUNxV3bd8QDb06LjPrPB74aua7W/YGUKJ2kW83sdTO7wcyuN7N5km6t47pKgTm/AfaW9AywV/g/F/Qgnfq+wGQzm29mb+BLc+wXzg02s2mhq3IZdQzMEj05LGkVYFU8kGQNlg8eBlNH2u4aZrA9myNh62jRPFy1YOtax2dXOF6TRCsd8HXgeHxe7gGWK91CPJQwHxj1zsP1Zjr1ppFopTOzc4BzJH3HzP7QbnnaSn01XW+lU5+Dhy5Gj08Nx4dXKF+TVPTpzOwPkkZJOlTS0aWtJ/eU9F1Jj4XIsOObJGqv0SIzWMVga+BmYB95WvU18MzgN4dzCyXtFEatR5OVdOqSTsN/aSPxgOD9gTvxjmt37jcKH3HtgFs6bpJ0vZk92xSBe4M2Blub2XxJvwDuC7f6uZmVBiTfxEfFA4B/hq0mqVA64BBgG+AhM/uipLWJ8aWLYUvgntJaJpJux6dkzuixpL2ADNQZXy6O7gZbh3OXAJdUOH4/MKoROVLRvALvhlWClsoXcplL175HozwG7CppzeBMMKbS/ZJlBlP8lhLSUtPdL1/58U/4KPZtfP6tW5jZE5J+C9wCLMLX4OioUO4ifPkrVl5/RHsNTSkyc8WRCqUzs2+G3Qsk3YRPSE7v4T3/TFhMR9Kv6DrflDjS5C8XRyqa16j1wcxmmtn0eiwSMff8QPi7Pt6f+2vPpOxFQp8ubksLia7pemqRiGGipDVxd6lvmdmCHt6vd8lQTZdopWNFi0SJt+ihRcLMdu3J9S0nQ0qX9Ob1Ltxb+CQz+yDwM3zkeTtJbg57gSJGonVcCCwOFomPA7/GXczfJIwqc0PhxNky+kRmvg/DDdgTzezHwCY9ubGk7wUT2GOSrgz9x2RSRy1X1HTNo4+kUr9zT+C2yLlu90clrQcchzuDjgL6AId3W8pWkKGaLukDiSuB2yXNwwOt/wUgaRO8ie0JfYEBkt7HR8gv9fB+vUuKlCqORCudmZ0e5uPWAW6JJBpZCfhOD+47R9KZwIu4Mt8SXOK7oIQsiijSNQ8XR9KbV4Ir9CQzWxQ59rSZPdjde4Y5v4OAjfDpmIGqsApUYqLBij5dJtgLeN7MXgvLVFzL8kDuZJKhPl1ele5FYCdJqwbnwz2BJ9osU20ypHSJ7tP1FmZ2j6QJwIN4JsSHSPi8X5b6dLlUOgAzOw33nE0+KavJ4shr85o6mjWQkLSfpKdCRP4KK1ZJ2kC+xO50SVMlDQ/Hd9fylOkPS3pP0sHhXEMp1XOpdJI2L3uBCxMfnNOEPp2kPsAf8RiTkcARkkaWFTsTD7j+EJ639dcAZjYlpEsfDeyBx1BEp5nqTqmey+bVzJ4CRsOyL2IOCV+HuEl9uh2AZ81sBoCkq/Cpo/9EyowETgj7U4C/V7jPIcA/u5svN5c1XRl7As+Z2QvtFqQq9dRyXtPFpVOPTZcOPII7tQJ8GhgU/A6jHI5bi6LUnVK9ULrKLxBITmCO6twIwdaRrTsj8pOAT0h6CPgE3gosix8J65dsjcfCljgF2AL4CDCUmBW1cq108ozNBwJ/q3Q+MRYJaNY8XdV06cseY/aSmX3GzLYFTg3HFkSKHApMCpPqpWsaSqmea6XDO9QPmtmr7RYkjibFSNwHbCppo/CDOxyP6l/+HGmYpJJenMKKsa5HUNYyqMGU6nlXuhVeYGJpQk1nZkuBb+NN4xPANWb2uKSfSzowFNsNeErS0/jSuKeXrpenMR2Be25HuULSo8CjwDBi1oPO5egVQNJAYG88DiPZNNGgb2Y34stHRI/9JLI/AZhQ5dqZVAiIsgZTqudW6YLXSvmoLLlkyCKRW6VLG4XttaDlpMlfLo7cDiQkDZE0QdKTkp6Q9NF2y1SV+ieHU0Gea7pzgJvM7JAwfbBquwWqSYqUKo5cKp2k1YGP4ykCMLMl+OKIiaSIkcgGGwGvAZdKekjSxWEKpQtJMYMByCx2Swt5Vbq+wIeB84O5ZxEVsiEmxgyWsT5dXpVuNjDbzO4J/0/AlTCxFNFgKcfMXgFmSdo8HNqTrj5liaNYny4bfAe3GfYHZhBWEk8sKarJ4sit0gWX6p4k+mgdKWs+48it0qWOQukKWonP02VH63KrdJJm4svIdgBLe5hTq9cpmtfssLuZzWu3ELGkbB4ujlxOmaSRZk2ZdDfYOpzriMQKXxc5vpGke8I9rw4zAlXJs9IZcIukByqE6gEJM4M1Qel6EmwdeDcSUH1g5PhvgbPMbBPgDeDLteTIs9LtYmYfxr+Ab4WFtLuQLDOYxW/xLAu2Dk4OpWDrKCNZvszulArnuxCCcfZguYv7X4hJqZ5bpTOzOeHvXDy6v2bYXLtpkhmsp8HWq4Saf1ppHRPc5X9BCPqpds8u5FLpJA2UNKi0jyfNrRk2105Krk11NK9xEf71UCvYeoMwyj8SOFvSxt35PHkdva4NTPKWgb7AX83spvaKVIP6m8+4dOp1BVsTajpJqwGfLQVbR1qHGZKmAtsCE4EhkvqG2i42pXoulS4sILNNu+VohCbN0y0LtsYV43C81lr+HGkYMD/k110WbC1fp/kdM1scyuwMnGFmJmkKvqjOVcAxxKRUz2XzmkraH2y9JZ539xF8gPEbMyt55pwMnCDpWbyP9+dacuSypksdBupoTlXX3WBrM7sLXzin0j1n0MBALNc1naQ+wV39+nbLEkuGPIfzXtN9F29mBrdbkDiyZHvNbU0XzDsHABe3W5a6aM7kcCLIc013NvADYFC1AkpImiYsXe7oceSyppP0SWCumT1Qq1xSzGAiWyGIea3pdgYOlDQGWAUYLGm8ma2QHywxFDVdujGzU8xsuJltiE+Q3pZohaOo6QpajRkU7urZwcymAlPbLEYsWZoyyb3SpYYUNZ9x5FLpJK0C3AGsjL+DCSFBXTLJ2JRJLpUOWAzsYWZvS+oH3Cnpn2Y2rd2CVaXo06UbMzPg7fBvv7Al+ltN0+g0jlxOmcAyY//DwFxgcmQFp2SSITNYbpXOzDpCGsnhwA6SRpWXSUw0mOGTw3FbSsit0pUIrthTgP0qnEuIGcxQZ2fslhZyqXSS1pI0JOwPwDPnPNlWoeIomtfUsw4wRdJ0PG5gspkl15Gzic1rdyP8JY2WdLekx8O5wyLXNJROPa+j1+l4JFNqaMboNRLhvzcen3qfpOsisQ6wPML/L5L2wCP8v4CnTz/azJ6RtC7wgKSbI2k5vx9c3WPJa02XMgw6O+O3eLod4W9mT5vZM2H/JXzUv1Z3Pk2hdGmg/mUlWpJOXdIOQH/gucjhIp16LSSNkDRF0n9CH+W77ZYplvr6dK1Kp3458MUQGwsNplPPZZ8OWAqcaGYPhuUlHpA0uaxvkyiaZJHoUYS/pMHADcCpUZOhmb0cdhdLuhRX3KrksqYLOecfDPtv4RFhNRd9aSsGdHTGb/F0O516KD8JH2RMKLumSKfeCPIU4dsCK5jBEmORoI7+XB01YQ8j/A8l5FOrMDVSpFOvl9B8TASON7OF5edDn+gigJXXH9He2dcmTf72IMJ/PDC+yj2LdOr1EFyaJgJXmNm17ZYnlhRZHOLIpdKFvsefgSfM7H/aLU8sZtDREV8uJeS1T7czPsu+R6R/MqbdQtUkQ7bXXNZ0ZnYnHsOcDozCc7igDaSoJoujULpUYPXaVlNBLvt0ki6RNFdSYhe37oLRLIN/Isil0gHjqOApnGgyNJDIpdKZ2R3A/HbL0RAZUrqiT1eD5KxPZ1gxT5cPkhKYA/iUSdyWEoqaLi2kqPmMo1C6NGDFlEnqkXQlcDewuaTZkmqmikwC1tERu6WFXNZ0ZnZEu2VojHSNTuPIpdKljsL2WtAWrOjTpZ64SPckYWGerhl9uu5G+Idzx0h6JmzHRI5vJ+nRcM9zg79iVXKpdKovl32isE6L3eKo83OXIvw/BPwcj/BH0lDgNGBHPGj7NHk6ToDzga8Cm4atpokxl0pHfZHuycI647d4uh3hD+yLr/ky38zeACYD+4VIsMFmNi0sNnkZHhFWlbz26SpFuu9YXihqBgMWP3/8ifV4pWzec/G68hZv3Px/NmFYHUVXkXR/5P+LygKu6/ncpQj/c+ga4V9tdYD1wn758arkVenqIhoNJun+mFTllMr1ghyt9Ig5CThP0lh8MfAuEf7NIK9KFxvpnlG6HeEvaQ4eExu9dmq4fnjZ8ZrvMq99uthI94zS7Qh/PEB7H0lrhAHEPsDNYUmJhZJ2CqPWo4H/rSmFmeVyA8YAT+MrD51aR/mv1Xnfusol6XPjo9QDw/4hwDOhzMXAypFrvwQ8G7YvRo5vjy8l8RxwHqBaMihcVFDQMvLavBa0kULpClpOoXQx1GsuS12EWRsplK4GDZrLxpG2CLM2UShdbeo2l1kaI8zaRKF0talnYeiCBimUrqDlFEpXm7yay3qVQulqk1dzWa9SKF0NrMrC0JXKpjHCrF0UZrCCllPUdAUtp1C6gpZTKF1ByymUrqDlFEpX0HJyrXSSDpZkkraIKXe8pFUj/98oaUivC5hRcj1lIulqYF3gNjM7rUa5mcD2ZjavVbJlmdzWdCHSaRfgy7ilAUl9JJ0p6bGwrMJ3JB2HK+YUSVNCuZmShoX9E0L5xyQdH45tKOkJSX+SJzG+RdKAcO44eXLj6ZKuav0nTwDtDhRpY4DK54E/h/27gO2AY/EMgH3D8aHh70xgWOTamXiKye3wdJMDgdWAx/E0nhviiYxHh/LXAEeF/ZcIwS7AkHa/h3Zsua3pgCNw/zjC3yOAvYALzc1fmFmcf9wuwCQzW2RmbwPXAruGc8+b2cNh/wFcEQGm4/lRj8IVM3fkMtg6LAazB7C1JAP64KvA3dfExyyO7HcAA8L+AXiy3k8Bp0rauqTkeSGvNd0hwOVmtoGZbWhmI4Dn8XU8vi6pLyxTToC3gEEV7vMv4GBJq0oaiK/98a9qDw1BzCPMbApwMrA63iznirwq3RF4PvooE4F1gBeB6ZIeAY4M5y4CbioNJEqY2YN4bMS9eDr2i83soRrP7QOMD6nHHwLONbMFPfso6SPXUyYF7SGvNV1BGymUrqDlFEpX0HIKpStoOYXSFbScQukKWk6hdAUt5/8BooD1/5Vs4VMAAAAASUVORK5CYII=",
                        "text/plain": [
                            "<Figure size 432x288 with 2 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from pbo.sample_collection.count_samples import count_samples\n",
                "from pbo.utils.state_action_mesh import StateActionMesh\n",
                "\n",
                "\n",
                "samples_count, n_outside_boxes = count_samples(replay_buffer, states_boxes, actions_boxes)\n",
                "samples_visu_mesh = StateActionMesh(states, actions, sleeping_time=0)\n",
                "\n",
                "samples_visu_mesh.set_values(samples_count, zeros_to_nan=True)\n",
                "samples_visu_mesh.show(\n",
                "    f\"Samples repartition, \\n{int(100 * n_outside_boxes / n_samples)}% are outside the box.\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimal Q function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 2 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "optimal_q = env.optimal_Q_mesh()\n",
                "\n",
                "q_visu_mesh = StateActionMesh(states, actions, sleeping_time)\n",
                "\n",
                "q_visu_mesh.set_values(optimal_q)\n",
                "q_visu_mesh.show(\"Optimal q function\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build q network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.networks.q import TableQ\n",
                "\n",
                "\n",
                "q = TableQ(\n",
                "    network_key=q_network_key,\n",
                "    random_weights_range=None,\n",
                "    random_weights_key=random_weights_key,\n",
                "    n_states=n_states,\n",
                "    n_actions=n_actions,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collect weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Random init weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.weights_collection.weights_buffer import WeightsBuffer\n",
                "\n",
                "\n",
                "weights_buffer = WeightsBuffer()\n",
                "\n",
                "while len(weights_buffer) < n_weights:\n",
                "    weights = q.random_init_weights()\n",
                "    if not filtering_weights or weights.max() < 1 / (1 - gamma):\n",
                "        weights_buffer.add(weights)\n",
                "\n",
                "weights_buffer.cast_to_jax_array()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build the PBOs network and the dataloaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.networks.pbo import LinearPBO, OptimalTablePBO, WeightsOptimalLinearTablePBO\n",
                "from pbo.sample_collection.dataloader import SampleDataLoader\n",
                "from pbo.weights_collection.dataloader import WeightsDataLoader\n",
                "\n",
                "\n",
                "data_loader_samples = SampleDataLoader(replay_buffer, batch_size_samples, shuffle_key)\n",
                "\n",
                "data_loader_weights = WeightsDataLoader(weights_buffer, batch_size_weights, shuffle_key)\n",
                "data_loader_weights_linear = WeightsDataLoader(weights_buffer, batch_size_weights_linear, shuffle_key)\n",
                "\n",
                "pbo = LinearPBO(pbo_network_key, q, learning_rate, max_bellman_iterations, add_infinity, importance_iteration)\n",
                "pbo_optimal = OptimalTablePBO(q, env.apply_bellman_operator, optimal_q, importance_iteration)\n",
                "pbo_optimal_linear = WeightsOptimalLinearTablePBO(pbo_network_key, q.weights_dimension, learning_rate_optimal_linear, pbo_optimal, max_bellman_iterations, add_infinity, importance_iteration)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train the optimal linear PBO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "linear l2 loss: 10.150428 Linear l2 grad loss: 4.694937586784363\n",
                        "linear l2 loss: 10.124077 Linear l2 grad loss: 4.647210955619812\n",
                        "linear l2 loss: 10.098327 Linear l2 grad loss: 4.601182341575623\n",
                        "linear l2 loss: 10.073145 Linear l2 grad loss: 4.556792140007019\n",
                        "linear l2 loss: 10.048509 Linear l2 grad loss: 4.513984322547913\n",
                        "linear l2 loss: 10.02439 Linear l2 grad loss: 4.472703695297241\n",
                        "linear l2 loss: 10.000767 Linear l2 grad loss: 4.432901382446289\n",
                        "linear l2 loss: 9.977615 Linear l2 grad loss: 4.394529223442078\n",
                        "linear l2 loss: 9.954913 Linear l2 grad loss: 4.357540965080261\n",
                        "linear l2 loss: 9.932642 Linear l2 grad loss: 4.3218947649002075\n",
                        "linear l2 loss: 9.910779 Linear l2 grad loss: 4.287550210952759\n",
                        "linear l2 loss: 9.889307 Linear l2 grad loss: 4.254467964172363\n",
                        "linear l2 loss: 9.86821 Linear l2 grad loss: 4.222612977027893\n",
                        "linear l2 loss: 9.847467 Linear l2 grad loss: 4.191949963569641\n",
                        "linear l2 loss: 9.827064 Linear l2 grad loss: 4.1624473333358765\n",
                        "linear l2 loss: 9.806983 Linear l2 grad loss: 4.134074091911316\n",
                        "linear l2 loss: 9.787213 Linear l2 grad loss: 4.106800675392151\n",
                        "linear l2 loss: 9.767738 Linear l2 grad loss: 4.080600619316101\n",
                        "linear l2 loss: 9.748543 Linear l2 grad loss: 4.055447578430176\n",
                        "linear l2 loss: 9.729615 Linear l2 grad loss: 4.031316757202148\n",
                        "linear l2 loss: 9.710942 Linear l2 grad loss: 4.008185863494873\n",
                        "linear l2 loss: 9.692512 Linear l2 grad loss: 3.9860323667526245\n",
                        "linear l2 loss: 9.674311 Linear l2 grad loss: 3.964835524559021\n",
                        "linear l2 loss: 9.656329 Linear l2 grad loss: 3.944576382637024\n",
                        "linear l2 loss: 9.638556 Linear l2 grad loss: 3.925236463546753\n",
                        "linear l2 loss: 9.620978 Linear l2 grad loss: 3.9067981243133545\n",
                        "linear l2 loss: 9.603588 Linear l2 grad loss: 3.889245867729187\n",
                        "linear l2 loss: 9.586376 Linear l2 grad loss: 3.872563600540161\n",
                        "linear l2 loss: 9.569331 Linear l2 grad loss: 3.856737971305847\n",
                        "linear l2 loss: 9.5524435 Linear l2 grad loss: 3.841754913330078\n",
                        "linear l2 loss: 9.535704 Linear l2 grad loss: 3.827601194381714\n",
                        "linear l2 loss: 9.519103 Linear l2 grad loss: 3.814266562461853\n",
                        "linear l2 loss: 9.502635 Linear l2 grad loss: 3.801738739013672\n",
                        "linear l2 loss: 9.486289 Linear l2 grad loss: 3.790008306503296\n",
                        "linear l2 loss: 9.470057 Linear l2 grad loss: 3.7790650129318237\n",
                        "linear l2 loss: 9.453929 Linear l2 grad loss: 3.7689002752304077\n",
                        "linear l2 loss: 9.4379015 Linear l2 grad loss: 3.7595064640045166\n",
                        "linear l2 loss: 9.421964 Linear l2 grad loss: 3.750875949859619\n",
                        "linear l2 loss: 9.406108 Linear l2 grad loss: 3.7430009841918945\n",
                        "linear l2 loss: 9.390327 Linear l2 grad loss: 3.7358760833740234\n",
                        "linear l2 loss: 9.374615 Linear l2 grad loss: 3.7294952869415283\n",
                        "linear l2 loss: 9.358962 Linear l2 grad loss: 3.7238537073135376\n",
                        "linear l2 loss: 9.343364 Linear l2 grad loss: 3.7189462184906006\n",
                        "linear l2 loss: 9.32781 Linear l2 grad loss: 3.7147696018218994\n",
                        "linear l2 loss: 9.312294 Linear l2 grad loss: 3.7113194465637207\n",
                        "linear l2 loss: 9.296813 Linear l2 grad loss: 3.708593010902405\n",
                        "linear l2 loss: 9.281355 Linear l2 grad loss: 3.7065871953964233\n",
                        "linear l2 loss: 9.265914 Linear l2 grad loss: 3.705299973487854\n",
                        "linear l2 loss: 9.250485 Linear l2 grad loss: 3.7047303915023804\n",
                        "linear l2 loss: 9.235061 Linear l2 grad loss: 3.7048765420913696\n",
                        "linear l2 loss: 9.219633 Linear l2 grad loss: 3.7057374715805054\n",
                        "linear l2 loss: 9.204196 Linear l2 grad loss: 3.70731258392334\n",
                        "linear l2 loss: 9.188743 Linear l2 grad loss: 3.7096025943756104\n",
                        "linear l2 loss: 9.173266 Linear l2 grad loss: 3.712607264518738\n",
                        "linear l2 loss: 9.15776 Linear l2 grad loss: 3.7163270711898804\n",
                        "linear l2 loss: 9.142215 Linear l2 grad loss: 3.7207632064819336\n",
                        "linear l2 loss: 9.126627 Linear l2 grad loss: 3.7259174585342407\n",
                        "linear l2 loss: 9.110988 Linear l2 grad loss: 3.731790781021118\n",
                        "linear l2 loss: 9.09529 Linear l2 grad loss: 3.7383859157562256\n",
                        "linear l2 loss: 9.079527 Linear l2 grad loss: 3.745704770088196\n",
                        "linear l2 loss: 9.063692 Linear l2 grad loss: 3.7537506818771362\n",
                        "linear l2 loss: 9.047775 Linear l2 grad loss: 3.762525796890259\n",
                        "linear l2 loss: 9.031773 Linear l2 grad loss: 3.772033929824829\n",
                        "linear l2 loss: 9.015675 Linear l2 grad loss: 3.7822779417037964\n",
                        "linear l2 loss: 8.999473 Linear l2 grad loss: 3.7932623624801636\n",
                        "linear l2 loss: 8.983162 Linear l2 grad loss: 3.804991126060486\n",
                        "linear l2 loss: 8.966733 Linear l2 grad loss: 3.8174678087234497\n",
                        "linear l2 loss: 8.950176 Linear l2 grad loss: 3.8306970596313477\n",
                        "linear l2 loss: 8.933487 Linear l2 grad loss: 3.8446840047836304\n",
                        "linear l2 loss: 8.916654 Linear l2 grad loss: 3.8594330549240112\n",
                        "linear l2 loss: 8.899669 Linear l2 grad loss: 3.8749488592147827\n",
                        "linear l2 loss: 8.882524 Linear l2 grad loss: 3.891237258911133\n",
                        "linear l2 loss: 8.865211 Linear l2 grad loss: 3.9083025455474854\n",
                        "linear l2 loss: 8.847718 Linear l2 grad loss: 3.9261504411697388\n",
                        "linear l2 loss: 8.830039 Linear l2 grad loss: 3.944785952568054\n",
                        "linear l2 loss: 8.812162 Linear l2 grad loss: 3.964214324951172\n",
                        "linear l2 loss: 8.79408 Linear l2 grad loss: 3.984440803527832\n",
                        "linear l2 loss: 8.775779 Linear l2 grad loss: 4.0054707527160645\n",
                        "linear l2 loss: 8.757254 Linear l2 grad loss: 4.027308702468872\n",
                        "linear l2 loss: 8.738488 Linear l2 grad loss: 4.0499595403671265\n",
                        "linear l2 loss: 8.719477 Linear l2 grad loss: 4.073428153991699\n",
                        "linear l2 loss: 8.700206 Linear l2 grad loss: 4.097718715667725\n",
                        "linear l2 loss: 8.680665 Linear l2 grad loss: 4.1228346824646\n",
                        "linear l2 loss: 8.660842 Linear l2 grad loss: 4.1487802267074585\n",
                        "linear l2 loss: 8.640724 Linear l2 grad loss: 4.175557017326355\n",
                        "linear l2 loss: 8.620302 Linear l2 grad loss: 4.203168869018555\n",
                        "linear l2 loss: 8.599561 Linear l2 grad loss: 4.231616377830505\n",
                        "linear l2 loss: 8.578488 Linear l2 grad loss: 4.260900139808655\n",
                        "linear l2 loss: 8.557074 Linear l2 grad loss: 4.2910202741622925\n",
                        "linear l2 loss: 8.5353 Linear l2 grad loss: 4.321974992752075\n",
                        "linear l2 loss: 8.513156 Linear l2 grad loss: 4.353761553764343\n",
                        "linear l2 loss: 8.490627 Linear l2 grad loss: 4.3863765001297\n",
                        "linear l2 loss: 8.4677 Linear l2 grad loss: 4.419813632965088\n",
                        "linear l2 loss: 8.444359 Linear l2 grad loss: 4.4540650844573975\n",
                        "linear l2 loss: 8.420589 Linear l2 grad loss: 4.489121913909912\n",
                        "linear l2 loss: 8.396378 Linear l2 grad loss: 4.524972081184387\n",
                        "linear l2 loss: 8.371707 Linear l2 grad loss: 4.561601042747498\n",
                        "linear l2 loss: 8.346562 Linear l2 grad loss: 4.598992347717285\n",
                        "linear l2 loss: 8.32093 Linear l2 grad loss: 4.637126088142395\n",
                        "linear l2 loss: 8.29479 Linear l2 grad loss: 4.675978064537048\n",
                        "linear l2 loss: 8.26813 Linear l2 grad loss: 4.715521574020386\n",
                        "linear l2 loss: 8.240935 Linear l2 grad loss: 4.755724906921387\n",
                        "linear l2 loss: 8.213186 Linear l2 grad loss: 4.796553254127502\n",
                        "linear l2 loss: 8.18487 Linear l2 grad loss: 4.837965726852417\n",
                        "linear l2 loss: 8.155969 Linear l2 grad loss: 4.879917621612549\n",
                        "linear l2 loss: 8.126469 Linear l2 grad loss: 4.922356724739075\n",
                        "linear l2 loss: 8.0963545 Linear l2 grad loss: 4.96522581577301\n",
                        "linear l2 loss: 8.065612 Linear l2 grad loss: 5.008461594581604\n",
                        "linear l2 loss: 8.0342245 Linear l2 grad loss: 5.051993727684021\n",
                        "linear l2 loss: 8.002182 Linear l2 grad loss: 5.095743179321289\n",
                        "linear l2 loss: 7.9694705 Linear l2 grad loss: 5.139624834060669\n",
                        "linear l2 loss: 7.936078 Linear l2 grad loss: 5.1835445165634155\n",
                        "linear l2 loss: 7.901994 Linear l2 grad loss: 5.227400541305542\n",
                        "linear l2 loss: 7.8672104 Linear l2 grad loss: 5.27108097076416\n",
                        "linear l2 loss: 7.831718 Linear l2 grad loss: 5.31446647644043\n",
                        "linear l2 loss: 7.7955117 Linear l2 grad loss: 5.357428431510925\n",
                        "linear l2 loss: 7.758587 Linear l2 grad loss: 5.399829030036926\n",
                        "linear l2 loss: 7.720943 Linear l2 grad loss: 5.441521525382996\n",
                        "linear l2 loss: 7.682579 Linear l2 grad loss: 5.482353091239929\n",
                        "linear l2 loss: 7.6434975 Linear l2 grad loss: 5.522160887718201\n",
                        "linear l2 loss: 7.603706 Linear l2 grad loss: 5.560777187347412\n",
                        "linear l2 loss: 7.563211 Linear l2 grad loss: 5.598029136657715\n",
                        "linear l2 loss: 7.5220246 Linear l2 grad loss: 5.633740305900574\n",
                        "linear l2 loss: 7.4801636 Linear l2 grad loss: 5.667732000350952\n",
                        "linear l2 loss: 7.437645 Linear l2 grad loss: 5.699828505516052\n",
                        "linear l2 loss: 7.3944917 Linear l2 grad loss: 5.7298572063446045\n",
                        "linear l2 loss: 7.350727 Linear l2 grad loss: 5.7576515674591064\n",
                        "linear l2 loss: 7.306381 Linear l2 grad loss: 5.783056974411011\n",
                        "linear l2 loss: 7.261487 Linear l2 grad loss: 5.805934309959412\n",
                        "linear l2 loss: 7.2160783 Linear l2 grad loss: 5.8261624574661255\n",
                        "linear l2 loss: 7.1701927 Linear l2 grad loss: 5.843645453453064\n",
                        "linear l2 loss: 7.123871 Linear l2 grad loss: 5.8583128452301025\n",
                        "linear l2 loss: 7.0771556 Linear l2 grad loss: 5.8701266050338745\n",
                        "linear l2 loss: 7.03009 Linear l2 grad loss: 5.879084587097168\n",
                        "linear l2 loss: 6.982717 Linear l2 grad loss: 5.885222434997559\n",
                        "linear l2 loss: 6.935083 Linear l2 grad loss: 5.888615250587463\n",
                        "linear l2 loss: 6.8872313 Linear l2 grad loss: 5.889381051063538\n",
                        "linear l2 loss: 6.839203 Linear l2 grad loss: 5.887675523757935\n",
                        "linear l2 loss: 6.7910395 Linear l2 grad loss: 5.883692264556885\n",
                        "linear l2 loss: 6.7427773 Linear l2 grad loss: 5.877657055854797\n",
                        "linear l2 loss: 6.6944504 Linear l2 grad loss: 5.869828581809998\n",
                        "linear l2 loss: 6.646089 Linear l2 grad loss: 5.860482335090637\n",
                        "linear l2 loss: 6.5977206 Linear l2 grad loss: 5.84990668296814\n",
                        "linear l2 loss: 6.5493655 Linear l2 grad loss: 5.838397264480591\n",
                        "linear l2 loss: 6.501039 Linear l2 grad loss: 5.82624077796936\n",
                        "linear l2 loss: 6.4527574 Linear l2 grad loss: 5.813714385032654\n",
                        "linear l2 loss: 6.4045258 Linear l2 grad loss: 5.801070809364319\n",
                        "linear l2 loss: 6.3563514 Linear l2 grad loss: 5.788535475730896\n",
                        "linear l2 loss: 6.308234 Linear l2 grad loss: 5.776300668716431\n",
                        "linear l2 loss: 6.260174 Linear l2 grad loss: 5.764522075653076\n",
                        "linear l2 loss: 6.2121654 Linear l2 grad loss: 5.753317773342133\n",
                        "linear l2 loss: 6.164204 Linear l2 grad loss: 5.742765963077545\n",
                        "linear l2 loss: 6.1162834 Linear l2 grad loss: 5.732914745807648\n",
                        "linear l2 loss: 6.0683975 Linear l2 grad loss: 5.723775565624237\n",
                        "linear l2 loss: 6.0205383 Linear l2 grad loss: 5.715334236621857\n",
                        "linear l2 loss: 5.9726987 Linear l2 grad loss: 5.707553863525391\n",
                        "linear l2 loss: 5.924874 Linear l2 grad loss: 5.70038115978241\n",
                        "linear l2 loss: 5.877057 Linear l2 grad loss: 5.693749785423279\n",
                        "linear l2 loss: 5.8292456 Linear l2 grad loss: 5.687582731246948\n",
                        "linear l2 loss: 5.7814345 Linear l2 grad loss: 5.681801795959473\n",
                        "linear l2 loss: 5.733623 Linear l2 grad loss: 5.676327407360077\n",
                        "linear l2 loss: 5.6858106 Linear l2 grad loss: 5.671082377433777\n",
                        "linear l2 loss: 5.637996 Linear l2 grad loss: 5.665990769863129\n",
                        "linear l2 loss: 5.590181 Linear l2 grad loss: 5.660986006259918\n",
                        "linear l2 loss: 5.542368 Linear l2 grad loss: 5.656002342700958\n",
                        "linear l2 loss: 5.49456 Linear l2 grad loss: 5.650986552238464\n",
                        "linear l2 loss: 5.446759 Linear l2 grad loss: 5.645887613296509\n",
                        "linear l2 loss: 5.398971 Linear l2 grad loss: 5.640661776065826\n",
                        "linear l2 loss: 5.3511996 Linear l2 grad loss: 5.63527238368988\n",
                        "linear l2 loss: 5.3034515 Linear l2 grad loss: 5.6296886801719666\n",
                        "linear l2 loss: 5.255732 Linear l2 grad loss: 5.6238826513290405\n",
                        "linear l2 loss: 5.208046 Linear l2 grad loss: 5.617835581302643\n",
                        "linear l2 loss: 5.1604 Linear l2 grad loss: 5.611526429653168\n",
                        "linear l2 loss: 5.1128006 Linear l2 grad loss: 5.604944229125977\n",
                        "linear l2 loss: 5.0652537 Linear l2 grad loss: 5.59807676076889\n",
                        "linear l2 loss: 5.017766 Linear l2 grad loss: 5.590916931629181\n",
                        "linear l2 loss: 4.970344 Linear l2 grad loss: 5.583458840847015\n",
                        "linear l2 loss: 4.922994 Linear l2 grad loss: 5.5756988525390625\n",
                        "linear l2 loss: 4.8757224 Linear l2 grad loss: 5.567636609077454\n",
                        "linear l2 loss: 4.828537 Linear l2 grad loss: 5.559270203113556\n",
                        "linear l2 loss: 4.781442 Linear l2 grad loss: 5.5506022572517395\n",
                        "linear l2 loss: 4.734445 Linear l2 grad loss: 5.541634559631348\n",
                        "linear l2 loss: 4.687552 Linear l2 grad loss: 5.532371461391449\n",
                        "linear l2 loss: 4.6407695 Linear l2 grad loss: 5.522815704345703\n",
                        "linear l2 loss: 4.594101 Linear l2 grad loss: 5.512972891330719\n",
                        "linear l2 loss: 4.547555 Linear l2 grad loss: 5.502847492694855\n",
                        "linear l2 loss: 4.501136 Linear l2 grad loss: 5.492445647716522\n",
                        "linear l2 loss: 4.45485 Linear l2 grad loss: 5.4817721247673035\n",
                        "linear l2 loss: 4.4087014 Linear l2 grad loss: 5.470833241939545\n",
                        "linear l2 loss: 4.362697 Linear l2 grad loss: 5.459634363651276\n",
                        "linear l2 loss: 4.3168406 Linear l2 grad loss: 5.448180913925171\n",
                        "linear l2 loss: 4.2711377 Linear l2 grad loss: 5.436478316783905\n",
                        "linear l2 loss: 4.2255926 Linear l2 grad loss: 5.424531638622284\n",
                        "linear l2 loss: 4.18021 Linear l2 grad loss: 5.4123440980911255\n",
                        "linear l2 loss: 4.134996 Linear l2 grad loss: 5.399921000003815\n",
                        "linear l2 loss: 4.089954 Linear l2 grad loss: 5.387264013290405\n",
                        "linear l2 loss: 4.045088 Linear l2 grad loss: 5.374376356601715\n",
                        "linear l2 loss: 4.000403 Linear l2 grad loss: 5.3612595200538635\n",
                        "linear l2 loss: 3.9559038 Linear l2 grad loss: 5.347914636135101\n",
                        "linear l2 loss: 3.9115937 Linear l2 grad loss: 5.334340453147888\n",
                        "linear l2 loss: 3.8674767 Linear l2 grad loss: 5.320536434650421\n",
                        "linear l2 loss: 3.823558 Linear l2 grad loss: 5.306500792503357\n",
                        "linear l2 loss: 3.7798417 Linear l2 grad loss: 5.292229652404785\n",
                        "linear l2 loss: 3.7363317 Linear l2 grad loss: 5.277719795703888\n",
                        "linear l2 loss: 3.693032 Linear l2 grad loss: 5.26296329498291\n",
                        "linear l2 loss: 3.6499481 Linear l2 grad loss: 5.247956871986389\n",
                        "linear l2 loss: 3.607083 Linear l2 grad loss: 5.232691764831543\n",
                        "linear l2 loss: 3.5644429 Linear l2 grad loss: 5.217158615589142\n",
                        "linear l2 loss: 3.5220306 Linear l2 grad loss: 5.201348185539246\n",
                        "linear l2 loss: 3.479852 Linear l2 grad loss: 5.18524968624115\n",
                        "linear l2 loss: 3.4379125 Linear l2 grad loss: 5.168852508068085\n",
                        "linear l2 loss: 3.3962152 Linear l2 grad loss: 5.152142286300659\n",
                        "linear l2 loss: 3.3547676 Linear l2 grad loss: 5.13510537147522\n",
                        "linear l2 loss: 3.3135736 Linear l2 grad loss: 5.117727518081665\n",
                        "linear l2 loss: 3.2726388 Linear l2 grad loss: 5.0999932289123535\n",
                        "linear l2 loss: 3.2319703 Linear l2 grad loss: 5.081884264945984\n",
                        "linear l2 loss: 3.191573 Linear l2 grad loss: 5.063385426998138\n",
                        "linear l2 loss: 3.1514533 Linear l2 grad loss: 5.044476866722107\n",
                        "linear l2 loss: 3.1116176 Linear l2 grad loss: 5.025139927864075\n",
                        "linear l2 loss: 3.0720727 Linear l2 grad loss: 5.005355060100555\n",
                        "linear l2 loss: 3.0328255 Linear l2 grad loss: 4.985100746154785\n",
                        "linear l2 loss: 2.9938834 Linear l2 grad loss: 4.964356958866119\n",
                        "linear l2 loss: 2.9552534 Linear l2 grad loss: 4.943101525306702\n",
                        "linear l2 loss: 2.9169436 Linear l2 grad loss: 4.921310663223267\n",
                        "linear l2 loss: 2.8789613 Linear l2 grad loss: 4.898963689804077\n",
                        "linear l2 loss: 2.8413155 Linear l2 grad loss: 4.87603622674942\n",
                        "linear l2 loss: 2.804014 Linear l2 grad loss: 4.852502703666687\n",
                        "linear l2 loss: 2.7670655 Linear l2 grad loss: 4.828341543674469\n",
                        "linear l2 loss: 2.730479 Linear l2 grad loss: 4.803525745868683\n",
                        "linear l2 loss: 2.6942637 Linear l2 grad loss: 4.778031170368195\n",
                        "linear l2 loss: 2.6584282 Linear l2 grad loss: 4.751832604408264\n",
                        "linear l2 loss: 2.6229832 Linear l2 grad loss: 4.724904835224152\n",
                        "linear l2 loss: 2.5879374 Linear l2 grad loss: 4.697222113609314\n",
                        "linear l2 loss: 2.5533016 Linear l2 grad loss: 4.6687580943107605\n",
                        "linear l2 loss: 2.5190847 Linear l2 grad loss: 4.63948917388916\n",
                        "linear l2 loss: 2.485298 Linear l2 grad loss: 4.60938835144043\n",
                        "linear l2 loss: 2.451951 Linear l2 grad loss: 4.578431487083435\n",
                        "linear l2 loss: 2.4190547 Linear l2 grad loss: 4.546594202518463\n",
                        "linear l2 loss: 2.3866198 Linear l2 grad loss: 4.513851523399353\n",
                        "linear l2 loss: 2.3546562 Linear l2 grad loss: 4.48018091917038\n",
                        "linear l2 loss: 2.323175 Linear l2 grad loss: 4.445559561252594\n",
                        "linear l2 loss: 2.2921865 Linear l2 grad loss: 4.409966707229614\n",
                        "linear l2 loss: 2.2617013 Linear l2 grad loss: 4.373381316661835\n",
                        "linear l2 loss: 2.2317302 Linear l2 grad loss: 4.335786163806915\n",
                        "linear l2 loss: 2.2022827 Linear l2 grad loss: 4.297163486480713\n",
                        "linear l2 loss: 2.1733701 Linear l2 grad loss: 4.257498741149902\n",
                        "linear l2 loss: 2.1450012 Linear l2 grad loss: 4.216780960559845\n",
                        "linear l2 loss: 2.117186 Linear l2 grad loss: 4.175000011920929\n",
                        "linear l2 loss: 2.089934 Linear l2 grad loss: 4.132150202989578\n",
                        "linear l2 loss: 2.063254 Linear l2 grad loss: 4.088229030370712\n",
                        "linear l2 loss: 2.0371547 Linear l2 grad loss: 4.04323810338974\n",
                        "linear l2 loss: 2.0116432 Linear l2 grad loss: 3.997184097766876\n",
                        "linear l2 loss: 1.986727 Linear l2 grad loss: 3.9500776827335358\n",
                        "linear l2 loss: 1.9624132 Linear l2 grad loss: 3.9019359052181244\n",
                        "linear l2 loss: 1.9387074 Linear l2 grad loss: 3.852781802415848\n",
                        "linear l2 loss: 1.9156144 Linear l2 grad loss: 3.8026433289051056\n",
                        "linear l2 loss: 1.8931383 Linear l2 grad loss: 3.7515565156936646\n",
                        "linear l2 loss: 1.8712822 Linear l2 grad loss: 3.699566513299942\n",
                        "linear l2 loss: 1.8500489 Linear l2 grad loss: 3.6467199325561523\n",
                        "linear l2 loss: 1.8294386 Linear l2 grad loss: 3.5930767953395844\n",
                        "linear l2 loss: 1.8094511 Linear l2 grad loss: 3.5387010872364044\n",
                        "linear l2 loss: 1.7900858 Linear l2 grad loss: 3.4836652874946594\n",
                        "linear l2 loss: 1.7713388 Linear l2 grad loss: 3.428049147129059\n",
                        "linear l2 loss: 1.7532074 Linear l2 grad loss: 3.3719378411769867\n",
                        "linear l2 loss: 1.7356857 Linear l2 grad loss: 3.3154243528842926\n",
                        "linear l2 loss: 1.7187674 Linear l2 grad loss: 3.258601278066635\n",
                        "linear l2 loss: 1.702445 Linear l2 grad loss: 3.2015713155269623\n",
                        "linear l2 loss: 1.6867093 Linear l2 grad loss: 3.144435554742813\n",
                        "linear l2 loss: 1.6715502 Linear l2 grad loss: 3.087299346923828\n",
                        "linear l2 loss: 1.6569566 Linear l2 grad loss: 3.030267655849457\n",
                        "linear l2 loss: 1.6429163 Linear l2 grad loss: 2.9734418988227844\n",
                        "linear l2 loss: 1.6294163 Linear l2 grad loss: 2.9169224202632904\n",
                        "linear l2 loss: 1.6164427 Linear l2 grad loss: 2.86080601811409\n",
                        "linear l2 loss: 1.6039807 Linear l2 grad loss: 2.805186450481415\n",
                        "linear l2 loss: 1.5920156 Linear l2 grad loss: 2.7501506209373474\n",
                        "linear l2 loss: 1.5805314 Linear l2 grad loss: 2.695780426263809\n",
                        "linear l2 loss: 1.5695122 Linear l2 grad loss: 2.642149716615677\n",
                        "linear l2 loss: 1.558942 Linear l2 grad loss: 2.589324414730072\n",
                        "linear l2 loss: 1.5488045 Linear l2 grad loss: 2.5373682975769043\n",
                        "linear l2 loss: 1.5390835 Linear l2 grad loss: 2.486334949731827\n",
                        "linear l2 loss: 1.5297624 Linear l2 grad loss: 2.4362754821777344\n",
                        "linear l2 loss: 1.5208254 Linear l2 grad loss: 2.387230694293976\n",
                        "linear l2 loss: 1.5122564 Linear l2 grad loss: 2.3392408192157745\n",
                        "linear l2 loss: 1.5040394 Linear l2 grad loss: 2.2923368513584137\n",
                        "linear l2 loss: 1.4961594 Linear l2 grad loss: 2.2465488016605377\n",
                        "linear l2 loss: 1.4886016 Linear l2 grad loss: 2.201902002096176\n",
                        "linear l2 loss: 1.4813505 Linear l2 grad loss: 2.158419817686081\n",
                        "linear l2 loss: 1.4743919 Linear l2 grad loss: 2.116122305393219\n",
                        "linear l2 loss: 1.4677123 Linear l2 grad loss: 2.075027257204056\n",
                        "linear l2 loss: 1.461298 Linear l2 grad loss: 2.0351445376873016\n",
                        "linear l2 loss: 1.4551353 Linear l2 grad loss: 1.9964846670627594\n",
                        "linear l2 loss: 1.4492122 Linear l2 grad loss: 1.9590602219104767\n",
                        "linear l2 loss: 1.4435158 Linear l2 grad loss: 1.9228831231594086\n",
                        "linear l2 loss: 1.4380347 Linear l2 grad loss: 1.887949138879776\n",
                        "linear l2 loss: 1.4327569 Linear l2 grad loss: 1.8542572259902954\n",
                        "linear l2 loss: 1.4276719 Linear l2 grad loss: 1.8218066096305847\n",
                        "linear l2 loss: 1.4227687 Linear l2 grad loss: 1.7905882596969604\n",
                        "linear l2 loss: 1.4180374 Linear l2 grad loss: 1.7605902254581451\n",
                        "linear l2 loss: 1.4134682 Linear l2 grad loss: 1.731800377368927\n",
                        "linear l2 loss: 1.4090521 Linear l2 grad loss: 1.7041972279548645\n",
                        "linear l2 loss: 1.4047798 Linear l2 grad loss: 1.6777594089508057\n",
                        "linear l2 loss: 1.4006432 Linear l2 grad loss: 1.6524604260921478\n",
                        "linear l2 loss: 1.3966341 Linear l2 grad loss: 1.6282674074172974\n",
                        "linear l2 loss: 1.3927456 Linear l2 grad loss: 1.6051505208015442\n",
                        "linear l2 loss: 1.3889698 Linear l2 grad loss: 1.5830717533826828\n",
                        "linear l2 loss: 1.3853005 Linear l2 grad loss: 1.5619947910308838\n",
                        "linear l2 loss: 1.3817309 Linear l2 grad loss: 1.5418804734945297\n",
                        "linear l2 loss: 1.3782561 Linear l2 grad loss: 1.5226886570453644\n",
                        "linear l2 loss: 1.3748693 Linear l2 grad loss: 1.504376471042633\n",
                        "linear l2 loss: 1.3715664 Linear l2 grad loss: 1.4869063049554825\n",
                        "linear l2 loss: 1.3683418 Linear l2 grad loss: 1.4702379405498505\n",
                        "linear l2 loss: 1.3651913 Linear l2 grad loss: 1.454343542456627\n",
                        "linear l2 loss: 1.3621109 Linear l2 grad loss: 1.4392012059688568\n",
                        "linear l2 loss: 1.3590964 Linear l2 grad loss: 1.4248223006725311\n",
                        "linear l2 loss: 1.3561442 Linear l2 grad loss: 1.4112785160541534\n",
                        "linear l2 loss: 1.3532515 Linear l2 grad loss: 1.3987593799829483\n",
                        "linear l2 loss: 1.3504149 Linear l2 grad loss: 1.3877284824848175\n",
                        "linear l2 loss: 1.3476318 Linear l2 grad loss: 1.3791622966527939\n",
                        "linear l2 loss: 1.344901 Linear l2 grad loss: 1.3751903474330902\n",
                        "linear l2 loss: 1.3422221 Linear l2 grad loss: 1.3801670521497726\n",
                        "linear l2 loss: 1.3395966 Linear l2 grad loss: 1.4027820825576782\n",
                        "linear l2 loss: 1.3370311 Linear l2 grad loss: 1.4594235122203827\n",
                        "linear l2 loss: 1.334541 Linear l2 grad loss: 1.5785175412893295\n",
                        "linear l2 loss: 1.3321594 Linear l2 grad loss: 1.8024697303771973\n",
                        "linear l2 loss: 1.3299509 Linear l2 grad loss: 2.1827224791049957\n",
                        "linear l2 loss: 1.32803 Linear l2 grad loss: 2.7651642858982086\n",
                        "linear l2 loss: 1.3265694 Linear l2 grad loss: 3.5670854449272156\n",
                        "linear l2 loss: 1.3257385 Linear l2 grad loss: 4.543734133243561\n",
                        "linear l2 loss: 1.3255119 Linear l2 grad loss: 5.560201644897461\n",
                        "linear l2 loss: 1.3254685 Linear l2 grad loss: 6.426268100738525\n",
                        "linear l2 loss: 1.3249835 Linear l2 grad loss: 7.006335496902466\n",
                        "linear l2 loss: 1.3237225 Linear l2 grad loss: 7.292343735694885\n",
                        "linear l2 loss: 1.3217818 Linear l2 grad loss: 7.363520264625549\n",
                        "linear l2 loss: 1.3194274 Linear l2 grad loss: 7.309169173240662\n",
                        "linear l2 loss: 1.3168806 Linear l2 grad loss: 7.1921669244766235\n",
                        "linear l2 loss: 1.3142741 Linear l2 grad loss: 7.048896670341492\n",
                        "linear l2 loss: 1.3116753 Linear l2 grad loss: 6.897964954376221\n",
                        "linear l2 loss: 1.3091146 Linear l2 grad loss: 6.7485671043396\n",
                        "linear l2 loss: 1.3066031 Linear l2 grad loss: 6.604509472846985\n",
                        "linear l2 loss: 1.3041428 Linear l2 grad loss: 6.467020630836487\n",
                        "linear l2 loss: 1.3017323 Linear l2 grad loss: 6.33640456199646\n",
                        "linear l2 loss: 1.2993698 Linear l2 grad loss: 6.2124727964401245\n",
                        "linear l2 loss: 1.2970517 Linear l2 grad loss: 6.09476363658905\n",
                        "linear l2 loss: 1.2947751 Linear l2 grad loss: 5.982918441295624\n",
                        "linear l2 loss: 1.2925371 Linear l2 grad loss: 5.876258909702301\n",
                        "linear l2 loss: 1.2903363 Linear l2 grad loss: 5.774729073047638\n",
                        "linear l2 loss: 1.2881691 Linear l2 grad loss: 5.677602767944336\n",
                        "linear l2 loss: 1.286034 Linear l2 grad loss: 5.584593117237091\n",
                        "linear l2 loss: 1.2839297 Linear l2 grad loss: 5.495697259902954\n",
                        "linear l2 loss: 1.2818542 Linear l2 grad loss: 5.410237550735474\n",
                        "linear l2 loss: 1.2798059 Linear l2 grad loss: 5.32819128036499\n",
                        "linear l2 loss: 1.2777842 Linear l2 grad loss: 5.24949723482132\n",
                        "linear l2 loss: 1.2757874 Linear l2 grad loss: 5.173672556877136\n",
                        "linear l2 loss: 1.2738142 Linear l2 grad loss: 5.1006098985672\n",
                        "linear l2 loss: 1.2718643 Linear l2 grad loss: 5.0302974581718445\n",
                        "linear l2 loss: 1.2699361 Linear l2 grad loss: 4.9624263644218445\n",
                        "linear l2 loss: 1.2680296 Linear l2 grad loss: 4.8970208168029785\n",
                        "linear l2 loss: 1.2661432 Linear l2 grad loss: 4.833876013755798\n",
                        "linear l2 loss: 1.2642773 Linear l2 grad loss: 4.773170530796051\n",
                        "linear l2 loss: 1.2624305 Linear l2 grad loss: 4.714492201805115\n",
                        "linear l2 loss: 1.2606018 Linear l2 grad loss: 4.657866895198822\n",
                        "linear l2 loss: 1.2587913 Linear l2 grad loss: 4.603169322013855\n",
                        "linear l2 loss: 1.2569984 Linear l2 grad loss: 4.550509214401245\n",
                        "linear l2 loss: 1.2552222 Linear l2 grad loss: 4.499619543552399\n",
                        "linear l2 loss: 1.2534622 Linear l2 grad loss: 4.45041686296463\n",
                        "linear l2 loss: 1.2517185 Linear l2 grad loss: 4.402999639511108\n",
                        "linear l2 loss: 1.2499903 Linear l2 grad loss: 4.357238352298737\n",
                        "linear l2 loss: 1.2482775 Linear l2 grad loss: 4.313164293766022\n",
                        "linear l2 loss: 1.246579 Linear l2 grad loss: 4.2705413699150085\n",
                        "linear l2 loss: 1.2448949 Linear l2 grad loss: 4.229381859302521\n",
                        "linear l2 loss: 1.2432251 Linear l2 grad loss: 4.1898834109306335\n",
                        "linear l2 loss: 1.2415688 Linear l2 grad loss: 4.151731252670288\n",
                        "linear l2 loss: 1.2399261 Linear l2 grad loss: 4.1151041984558105\n",
                        "linear l2 loss: 1.2382964 Linear l2 grad loss: 4.0798280239105225\n",
                        "linear l2 loss: 1.2366793 Linear l2 grad loss: 4.0457916259765625\n",
                        "linear l2 loss: 1.2350748 Linear l2 grad loss: 4.01314514875412\n",
                        "linear l2 loss: 1.2334824 Linear l2 grad loss: 3.981797218322754\n",
                        "linear l2 loss: 1.231902 Linear l2 grad loss: 3.9516759514808655\n",
                        "linear l2 loss: 1.2303336 Linear l2 grad loss: 3.922916531562805\n",
                        "linear l2 loss: 1.2287768 Linear l2 grad loss: 3.8952993750572205\n",
                        "linear l2 loss: 1.2272309 Linear l2 grad loss: 3.8689597845077515\n",
                        "linear l2 loss: 1.2256963 Linear l2 grad loss: 3.843912661075592\n",
                        "linear l2 loss: 1.2241725 Linear l2 grad loss: 3.8198665976524353\n",
                        "linear l2 loss: 1.2226595 Linear l2 grad loss: 3.7970287203788757\n",
                        "linear l2 loss: 1.2211565 Linear l2 grad loss: 3.775166869163513\n",
                        "linear l2 loss: 1.2196635 Linear l2 grad loss: 3.7542832493782043\n",
                        "linear l2 loss: 1.218181 Linear l2 grad loss: 3.7346718311309814\n",
                        "linear l2 loss: 1.2167081 Linear l2 grad loss: 3.7161223888397217\n",
                        "linear l2 loss: 1.2152456 Linear l2 grad loss: 3.698780357837677\n",
                        "linear l2 loss: 1.2137922 Linear l2 grad loss: 3.6823280453681946\n",
                        "linear l2 loss: 1.2123482 Linear l2 grad loss: 3.6669230461120605\n",
                        "linear l2 loss: 1.2109134 Linear l2 grad loss: 3.652453660964966\n",
                        "linear l2 loss: 1.2094878 Linear l2 grad loss: 3.6390401124954224\n",
                        "linear l2 loss: 1.2080711 Linear l2 grad loss: 3.6265450716018677\n",
                        "linear l2 loss: 1.2066629 Linear l2 grad loss: 3.6149566769599915\n",
                        "linear l2 loss: 1.2052633 Linear l2 grad loss: 3.604154109954834\n",
                        "linear l2 loss: 1.2038723 Linear l2 grad loss: 3.594468355178833\n",
                        "linear l2 loss: 1.2024897 Linear l2 grad loss: 3.5856258869171143\n",
                        "linear l2 loss: 1.2011155 Linear l2 grad loss: 3.5777993202209473\n",
                        "linear l2 loss: 1.1997494 Linear l2 grad loss: 3.570796549320221\n",
                        "linear l2 loss: 1.1983912 Linear l2 grad loss: 3.5646509528160095\n",
                        "linear l2 loss: 1.1970407 Linear l2 grad loss: 3.5592427253723145\n",
                        "linear l2 loss: 1.1956978 Linear l2 grad loss: 3.554568886756897\n",
                        "linear l2 loss: 1.194363 Linear l2 grad loss: 3.550960958003998\n",
                        "linear l2 loss: 1.1930355 Linear l2 grad loss: 3.5480443239212036\n",
                        "linear l2 loss: 1.1917156 Linear l2 grad loss: 3.5460192561149597\n",
                        "linear l2 loss: 1.1904027 Linear l2 grad loss: 3.544721305370331\n",
                        "linear l2 loss: 1.1890972 Linear l2 grad loss: 3.5441936254501343\n",
                        "linear l2 loss: 1.1877989 Linear l2 grad loss: 3.544390916824341\n",
                        "linear l2 loss: 1.1865073 Linear l2 grad loss: 3.5453161001205444\n",
                        "linear l2 loss: 1.1852226 Linear l2 grad loss: 3.5469307899475098\n",
                        "linear l2 loss: 1.183945 Linear l2 grad loss: 3.5491379499435425\n",
                        "linear l2 loss: 1.1826738 Linear l2 grad loss: 3.5522003769874573\n",
                        "linear l2 loss: 1.1814094 Linear l2 grad loss: 3.5558937788009644\n",
                        "linear l2 loss: 1.1801517 Linear l2 grad loss: 3.5603588819503784\n",
                        "linear l2 loss: 1.1789002 Linear l2 grad loss: 3.5653867721557617\n",
                        "linear l2 loss: 1.1776552 Linear l2 grad loss: 3.571047067642212\n",
                        "linear l2 loss: 1.176416 Linear l2 grad loss: 3.577221632003784\n",
                        "linear l2 loss: 1.1751835 Linear l2 grad loss: 3.5840792059898376\n",
                        "linear l2 loss: 1.1739571 Linear l2 grad loss: 3.591631054878235\n",
                        "linear l2 loss: 1.1727365 Linear l2 grad loss: 3.599690794944763\n",
                        "linear l2 loss: 1.171522 Linear l2 grad loss: 3.6083972454071045\n",
                        "linear l2 loss: 1.1703132 Linear l2 grad loss: 3.6175114512443542\n",
                        "linear l2 loss: 1.1691104 Linear l2 grad loss: 3.6272775530815125\n",
                        "linear l2 loss: 1.1679133 Linear l2 grad loss: 3.637543797492981\n",
                        "linear l2 loss: 1.1667216 Linear l2 grad loss: 3.64830482006073\n",
                        "linear l2 loss: 1.1655353 Linear l2 grad loss: 3.6594953536987305\n",
                        "linear l2 loss: 1.1643547 Linear l2 grad loss: 3.671110510826111\n",
                        "linear l2 loss: 1.1631793 Linear l2 grad loss: 3.6832019686698914\n",
                        "linear l2 loss: 1.1620091 Linear l2 grad loss: 3.6956880688667297\n",
                        "linear l2 loss: 1.1608442 Linear l2 grad loss: 3.7086594104766846\n",
                        "linear l2 loss: 1.1596848 Linear l2 grad loss: 3.722145438194275\n",
                        "linear l2 loss: 1.1585302 Linear l2 grad loss: 3.735891103744507\n",
                        "linear l2 loss: 1.1573806 Linear l2 grad loss: 3.7499690651893616\n",
                        "linear l2 loss: 1.1562359 Linear l2 grad loss: 3.7644042372703552\n",
                        "linear l2 loss: 1.1550959 Linear l2 grad loss: 3.779117703437805\n",
                        "linear l2 loss: 1.1539606 Linear l2 grad loss: 3.7941691875457764\n",
                        "linear l2 loss: 1.1528305 Linear l2 grad loss: 3.8097320795059204\n",
                        "linear l2 loss: 1.1517048 Linear l2 grad loss: 3.8254493474960327\n",
                        "linear l2 loss: 1.1505835 Linear l2 grad loss: 3.8414156436920166\n",
                        "linear l2 loss: 1.1494673 Linear l2 grad loss: 3.857971966266632\n",
                        "linear l2 loss: 1.148355 Linear l2 grad loss: 3.8744407296180725\n",
                        "linear l2 loss: 1.1472472 Linear l2 grad loss: 3.8911423087120056\n",
                        "linear l2 loss: 1.1461434 Linear l2 grad loss: 3.908160924911499\n",
                        "linear l2 loss: 1.1450443 Linear l2 grad loss: 3.9253510236740112\n",
                        "linear l2 loss: 1.1439493 Linear l2 grad loss: 3.9428436756134033\n",
                        "linear l2 loss: 1.1428587 Linear l2 grad loss: 3.960714042186737\n",
                        "linear l2 loss: 1.1417718 Linear l2 grad loss: 3.9785525798797607\n",
                        "linear l2 loss: 1.1406893 Linear l2 grad loss: 3.996658205986023\n",
                        "linear l2 loss: 1.1396104 Linear l2 grad loss: 4.014917016029358\n",
                        "linear l2 loss: 1.1385354 Linear l2 grad loss: 4.033256947994232\n",
                        "linear l2 loss: 1.137464 Linear l2 grad loss: 4.0517643094062805\n",
                        "linear l2 loss: 1.1363966 Linear l2 grad loss: 4.070364773273468\n",
                        "linear l2 loss: 1.1353331 Linear l2 grad loss: 4.089179992675781\n",
                        "linear l2 loss: 1.1342734 Linear l2 grad loss: 4.108157634735107\n",
                        "linear l2 loss: 1.133217 Linear l2 grad loss: 4.12714296579361\n",
                        "linear l2 loss: 1.1321641 Linear l2 grad loss: 4.1462321281433105\n",
                        "linear l2 loss: 1.131115 Linear l2 grad loss: 4.16558164358139\n",
                        "linear l2 loss: 1.1300691 Linear l2 grad loss: 4.184910416603088\n",
                        "linear l2 loss: 1.1290268 Linear l2 grad loss: 4.20438951253891\n",
                        "linear l2 loss: 1.1279879 Linear l2 grad loss: 4.223907947540283\n",
                        "linear l2 loss: 1.1269525 Linear l2 grad loss: 4.243622720241547\n",
                        "linear l2 loss: 1.1259199 Linear l2 grad loss: 4.263313114643097\n",
                        "linear l2 loss: 1.1248908 Linear l2 grad loss: 4.283157229423523\n",
                        "linear l2 loss: 1.1238654 Linear l2 grad loss: 4.303118348121643\n",
                        "linear l2 loss: 1.1228427 Linear l2 grad loss: 4.323224604129791\n",
                        "linear l2 loss: 1.1218232 Linear l2 grad loss: 4.3432939648628235\n",
                        "linear l2 loss: 1.1208066 Linear l2 grad loss: 4.3632731437683105\n",
                        "linear l2 loss: 1.1197933 Linear l2 grad loss: 4.383572161197662\n",
                        "linear l2 loss: 1.118783 Linear l2 grad loss: 4.403895437717438\n",
                        "linear l2 loss: 1.1177758 Linear l2 grad loss: 4.4242730140686035\n",
                        "linear l2 loss: 1.116771 Linear l2 grad loss: 4.444571912288666\n",
                        "linear l2 loss: 1.115769 Linear l2 grad loss: 4.464844822883606\n",
                        "linear l2 loss: 1.1147703 Linear l2 grad loss: 4.485419750213623\n",
                        "linear l2 loss: 1.1137742 Linear l2 grad loss: 4.5059428215026855\n",
                        "linear l2 loss: 1.1127809 Linear l2 grad loss: 4.526453077793121\n",
                        "linear l2 loss: 1.1117905 Linear l2 grad loss: 4.547164082527161\n",
                        "linear l2 loss: 1.1108025 Linear l2 grad loss: 4.567686021327972\n",
                        "linear l2 loss: 1.1098171 Linear l2 grad loss: 4.588320255279541\n",
                        "linear l2 loss: 1.1088344 Linear l2 grad loss: 4.608962655067444\n",
                        "linear l2 loss: 1.1078542 Linear l2 grad loss: 4.629578649997711\n",
                        "linear l2 loss: 1.1068769 Linear l2 grad loss: 4.650352120399475\n",
                        "linear l2 loss: 1.1059015 Linear l2 grad loss: 4.671082258224487\n",
                        "linear l2 loss: 1.1049291 Linear l2 grad loss: 4.691898584365845\n",
                        "linear l2 loss: 1.103959 Linear l2 grad loss: 4.712737321853638\n",
                        "linear l2 loss: 1.1029913 Linear l2 grad loss: 4.73358690738678\n",
                        "linear l2 loss: 1.102026 Linear l2 grad loss: 4.754467129707336\n",
                        "linear l2 loss: 1.101063 Linear l2 grad loss: 4.775380611419678\n",
                        "linear l2 loss: 1.1001027 Linear l2 grad loss: 4.796407520771027\n",
                        "linear l2 loss: 1.0991441 Linear l2 grad loss: 4.817302584648132\n",
                        "linear l2 loss: 1.0981878 Linear l2 grad loss: 4.838177382946014\n",
                        "linear l2 loss: 1.097234 Linear l2 grad loss: 4.859196126461029\n",
                        "linear l2 loss: 1.0962824 Linear l2 grad loss: 4.8801411390304565\n",
                        "linear l2 loss: 1.0953325 Linear l2 grad loss: 4.901077926158905\n",
                        "linear l2 loss: 1.094385 Linear l2 grad loss: 4.922018885612488\n",
                        "linear l2 loss: 1.09344 Linear l2 grad loss: 4.943183600902557\n",
                        "linear l2 loss: 1.0924969 Linear l2 grad loss: 4.964257061481476\n",
                        "linear l2 loss: 1.0915557 Linear l2 grad loss: 4.985353648662567\n",
                        "linear l2 loss: 1.0906171 Linear l2 grad loss: 5.006475448608398\n",
                        "linear l2 loss: 1.08968 Linear l2 grad loss: 5.027661919593811\n",
                        "linear l2 loss: 1.0887454 Linear l2 grad loss: 5.048919081687927\n",
                        "linear l2 loss: 1.0878118 Linear l2 grad loss: 5.069929599761963\n",
                        "linear l2 loss: 1.0868808 Linear l2 grad loss: 5.091100752353668\n",
                        "linear l2 loss: 1.085952 Linear l2 grad loss: 5.112411379814148\n",
                        "linear l2 loss: 1.0850245 Linear l2 grad loss: 5.133580327033997\n",
                        "linear l2 loss: 1.0840987 Linear l2 grad loss: 5.154632806777954\n",
                        "linear l2 loss: 1.0831751 Linear l2 grad loss: 5.175750494003296\n",
                        "linear l2 loss: 1.0822529 Linear l2 grad loss: 5.196838796138763\n",
                        "linear l2 loss: 1.0813324 Linear l2 grad loss: 5.217851877212524\n",
                        "linear l2 loss: 1.0804138 Linear l2 grad loss: 5.238957107067108\n",
                        "linear l2 loss: 1.0794975 Linear l2 grad loss: 5.26014631986618\n",
                        "linear l2 loss: 1.0785825 Linear l2 grad loss: 5.281427502632141\n",
                        "linear l2 loss: 1.0776695 Linear l2 grad loss: 5.302714407444\n",
                        "linear l2 loss: 1.0767583 Linear l2 grad loss: 5.324053943157196\n",
                        "linear l2 loss: 1.0758487 Linear l2 grad loss: 5.345351457595825\n",
                        "linear l2 loss: 1.0749404 Linear l2 grad loss: 5.366547882556915\n",
                        "linear l2 loss: 1.074034 Linear l2 grad loss: 5.387830197811127\n",
                        "linear l2 loss: 1.0731293 Linear l2 grad loss: 5.409219145774841\n",
                        "linear l2 loss: 1.0722259 Linear l2 grad loss: 5.430513381958008\n",
                        "linear l2 loss: 1.0713241 Linear l2 grad loss: 5.451754450798035\n",
                        "linear l2 loss: 1.0704237 Linear l2 grad loss: 5.472990572452545\n",
                        "linear l2 loss: 1.0695256 Linear l2 grad loss: 5.494514644145966\n",
                        "linear l2 loss: 1.0686288 Linear l2 grad loss: 5.515959799289703\n",
                        "linear l2 loss: 1.0677332 Linear l2 grad loss: 5.53728574514389\n",
                        "linear l2 loss: 1.0668389 Linear l2 grad loss: 5.55865091085434\n",
                        "linear l2 loss: 1.0659462 Linear l2 grad loss: 5.580009162425995\n",
                        "linear l2 loss: 1.0650547 Linear l2 grad loss: 5.601265132427216\n",
                        "linear l2 loss: 1.0641649 Linear l2 grad loss: 5.622598886489868\n",
                        "linear l2 loss: 1.0632763 Linear l2 grad loss: 5.6439009308815\n",
                        "linear l2 loss: 1.0623891 Linear l2 grad loss: 5.665193855762482\n",
                        "linear l2 loss: 1.0615028 Linear l2 grad loss: 5.686435580253601\n",
                        "linear l2 loss: 1.0606183 Linear l2 grad loss: 5.7076826095581055\n",
                        "linear l2 loss: 1.059735 Linear l2 grad loss: 5.728966295719147\n",
                        "linear l2 loss: 1.0588531 Linear l2 grad loss: 5.75027322769165\n",
                        "linear l2 loss: 1.0579724 Linear l2 grad loss: 5.771587491035461\n",
                        "linear l2 loss: 1.0570935 Linear l2 grad loss: 5.792979300022125\n",
                        "linear l2 loss: 1.0562155 Linear l2 grad loss: 5.814404308795929\n",
                        "linear l2 loss: 1.0553387 Linear l2 grad loss: 5.835702836513519\n",
                        "linear l2 loss: 1.054463 Linear l2 grad loss: 5.85691910982132\n",
                        "linear l2 loss: 1.0535889 Linear l2 grad loss: 5.878355085849762\n",
                        "linear l2 loss: 1.0527159 Linear l2 grad loss: 5.899706244468689\n",
                        "linear l2 loss: 1.051844 Linear l2 grad loss: 5.9210256934165955\n",
                        "linear l2 loss: 1.0509737 Linear l2 grad loss: 5.942435920238495\n",
                        "linear l2 loss: 1.0501041 Linear l2 grad loss: 5.963810086250305\n",
                        "linear l2 loss: 1.0492359 Linear l2 grad loss: 5.985220313072205\n",
                        "linear l2 loss: 1.0483688 Linear l2 grad loss: 6.006654322147369\n",
                        "linear l2 loss: 1.047503 Linear l2 grad loss: 6.027986288070679\n",
                        "linear l2 loss: 1.0466379 Linear l2 grad loss: 6.049365520477295\n",
                        "linear l2 loss: 1.0457743 Linear l2 grad loss: 6.070736229419708\n",
                        "linear l2 loss: 1.0449116 Linear l2 grad loss: 6.092101335525513\n",
                        "linear l2 loss: 1.04405 Linear l2 grad loss: 6.11346298456192\n",
                        "linear l2 loss: 1.0431898 Linear l2 grad loss: 6.134929955005646\n",
                        "linear l2 loss: 1.0423301 Linear l2 grad loss: 6.156318306922913\n",
                        "linear l2 loss: 1.0414717 Linear l2 grad loss: 6.177614390850067\n",
                        "linear l2 loss: 1.0406142 Linear l2 grad loss: 6.198977887630463\n",
                        "linear l2 loss: 1.0397575 Linear l2 grad loss: 6.2201738357543945\n",
                        "linear l2 loss: 1.0389022 Linear l2 grad loss: 6.241547107696533\n",
                        "linear l2 loss: 1.0380478 Linear l2 grad loss: 6.262953341007233\n",
                        "linear l2 loss: 1.0371951 Linear l2 grad loss: 6.284456431865692\n",
                        "linear l2 loss: 1.0363427 Linear l2 grad loss: 6.305826902389526\n",
                        "linear l2 loss: 1.0354913 Linear l2 grad loss: 6.327215731143951\n",
                        "linear l2 loss: 1.0346414 Linear l2 grad loss: 6.348670661449432\n",
                        "linear l2 loss: 1.0337921 Linear l2 grad loss: 6.370141863822937\n",
                        "linear l2 loss: 1.0329437 Linear l2 grad loss: 6.391493141651154\n",
                        "linear l2 loss: 1.032096 Linear l2 grad loss: 6.412813663482666\n",
                        "linear l2 loss: 1.0312493 Linear l2 grad loss: 6.434116542339325\n",
                        "linear l2 loss: 1.0304039 Linear l2 grad loss: 6.455558359622955\n",
                        "linear l2 loss: 1.0295594 Linear l2 grad loss: 6.477021753787994\n",
                        "linear l2 loss: 1.0287162 Linear l2 grad loss: 6.498557388782501\n",
                        "linear l2 loss: 1.0278736 Linear l2 grad loss: 6.520035684108734\n",
                        "linear l2 loss: 1.0270318 Linear l2 grad loss: 6.541503429412842\n",
                        "linear l2 loss: 1.0261911 Linear l2 grad loss: 6.562978029251099\n",
                        "linear l2 loss: 1.0253509 Linear l2 grad loss: 6.584348499774933\n",
                        "linear l2 loss: 1.024512 Linear l2 grad loss: 6.605895638465881\n",
                        "linear l2 loss: 1.0236733 Linear l2 grad loss: 6.627158284187317\n",
                        "linear l2 loss: 1.0228356 Linear l2 grad loss: 6.648562073707581\n",
                        "linear l2 loss: 1.0219996 Linear l2 grad loss: 6.670125126838684\n",
                        "linear l2 loss: 1.021164 Linear l2 grad loss: 6.691612005233765\n",
                        "linear l2 loss: 1.0203289 Linear l2 grad loss: 6.712990760803223\n",
                        "linear l2 loss: 1.0194949 Linear l2 grad loss: 6.734442949295044\n",
                        "linear l2 loss: 1.0186619 Linear l2 grad loss: 6.755928039550781\n",
                        "linear l2 loss: 1.0178297 Linear l2 grad loss: 6.777413964271545\n",
                        "linear l2 loss: 1.016998 Linear l2 grad loss: 6.798827528953552\n",
                        "linear l2 loss: 1.0161673 Linear l2 grad loss: 6.820273160934448\n",
                        "linear l2 loss: 1.0153368 Linear l2 grad loss: 6.841520309448242\n",
                        "linear l2 loss: 1.0145072 Linear l2 grad loss: 6.862810134887695\n",
                        "linear l2 loss: 1.0136789 Linear l2 grad loss: 6.884347677230835\n",
                        "linear l2 loss: 1.0128518 Linear l2 grad loss: 6.905884027481079\n",
                        "linear l2 loss: 1.012025 Linear l2 grad loss: 6.927372574806213\n",
                        "linear l2 loss: 1.0111994 Linear l2 grad loss: 6.948852777481079\n",
                        "linear l2 loss: 1.0103737 Linear l2 grad loss: 6.970174193382263\n",
                        "linear l2 loss: 1.009549 Linear l2 grad loss: 6.991517186164856\n",
                        "linear l2 loss: 1.0087254 Linear l2 grad loss: 7.012983441352844\n",
                        "linear l2 loss: 1.0079024 Linear l2 grad loss: 7.034418344497681\n",
                        "linear l2 loss: 1.0070803 Linear l2 grad loss: 7.055835247039795\n",
                        "linear l2 loss: 1.0062588 Linear l2 grad loss: 7.077340602874756\n",
                        "linear l2 loss: 1.0054384 Linear l2 grad loss: 7.098816514015198\n",
                        "linear l2 loss: 1.0046185 Linear l2 grad loss: 7.1202603578567505\n",
                        "linear l2 loss: 1.0037993 Linear l2 grad loss: 7.141740560531616\n",
                        "linear l2 loss: 1.0029813 Linear l2 grad loss: 7.1632877588272095\n",
                        "linear l2 loss: 1.002164 Linear l2 grad loss: 7.184847354888916\n",
                        "linear l2 loss: 1.0013473 Linear l2 grad loss: 7.20636248588562\n",
                        "linear l2 loss: 1.0005316 Linear l2 grad loss: 7.2279417514801025\n",
                        "linear l2 loss: 0.9997158 Linear l2 grad loss: 7.249364852905273\n",
                        "linear l2 loss: 0.99890155 Linear l2 grad loss: 7.270955204963684\n",
                        "linear l2 loss: 0.9980875 Linear l2 grad loss: 7.292444229125977\n",
                        "linear l2 loss: 0.9972742 Linear l2 grad loss: 7.313875794410706\n",
                        "linear l2 loss: 0.9964617 Linear l2 grad loss: 7.335385203361511\n",
                        "linear l2 loss: 0.9956498 Linear l2 grad loss: 7.356855750083923\n",
                        "linear l2 loss: 0.99483913 Linear l2 grad loss: 7.378441333770752\n",
                        "linear l2 loss: 0.9940287 Linear l2 grad loss: 7.39992368221283\n",
                        "linear l2 loss: 0.9932191 Linear l2 grad loss: 7.421441316604614\n",
                        "linear l2 loss: 0.9924102 Linear l2 grad loss: 7.442983269691467\n",
                        "linear l2 loss: 0.99160236 Linear l2 grad loss: 7.464568138122559\n",
                        "linear l2 loss: 0.9907946 Linear l2 grad loss: 7.48603081703186\n",
                        "linear l2 loss: 0.9899875 Linear l2 grad loss: 7.5074542760849\n",
                        "linear l2 loss: 0.98918116 Linear l2 grad loss: 7.528908133506775\n",
                        "linear l2 loss: 0.98837596 Linear l2 grad loss: 7.55044412612915\n",
                        "linear l2 loss: 0.98757076 Linear l2 grad loss: 7.571874141693115\n",
                        "linear l2 loss: 0.98676664 Linear l2 grad loss: 7.593409419059753\n",
                        "linear l2 loss: 0.9859633 Linear l2 grad loss: 7.614893913269043\n",
                        "linear l2 loss: 0.98516065 Linear l2 grad loss: 7.636438012123108\n",
                        "linear l2 loss: 0.98435897 Linear l2 grad loss: 7.6580891609191895\n",
                        "linear l2 loss: 0.98355716 Linear l2 grad loss: 7.679425120353699\n",
                        "linear l2 loss: 0.9827566 Linear l2 grad loss: 7.7010194063186646\n",
                        "linear l2 loss: 0.98195714 Linear l2 grad loss: 7.722648978233337\n",
                        "linear l2 loss: 0.9811581 Linear l2 grad loss: 7.744293212890625\n",
                        "linear l2 loss: 0.9803597 Linear l2 grad loss: 7.765837073326111\n",
                        "linear l2 loss: 0.97956175 Linear l2 grad loss: 7.787378430366516\n",
                        "linear l2 loss: 0.97876453 Linear l2 grad loss: 7.808949708938599\n",
                        "linear l2 loss: 0.9779679 Linear l2 grad loss: 7.83045506477356\n",
                        "linear l2 loss: 0.9771719 Linear l2 grad loss: 7.8519556522369385\n",
                        "linear l2 loss: 0.97637665 Linear l2 grad loss: 7.873496890068054\n",
                        "linear l2 loss: 0.9755821 Linear l2 grad loss: 7.895019888877869\n",
                        "linear l2 loss: 0.974788 Linear l2 grad loss: 7.916506052017212\n",
                        "linear l2 loss: 0.9739947 Linear l2 grad loss: 7.937977194786072\n",
                        "linear l2 loss: 0.97320175 Linear l2 grad loss: 7.959446668624878\n",
                        "linear l2 loss: 0.9724097 Linear l2 grad loss: 7.980915784835815\n",
                        "linear l2 loss: 0.97161883 Linear l2 grad loss: 8.002578735351562\n",
                        "linear l2 loss: 0.97082835 Linear l2 grad loss: 8.024150490760803\n",
                        "linear l2 loss: 0.9700387 Linear l2 grad loss: 8.045750617980957\n",
                        "linear l2 loss: 0.9692499 Linear l2 grad loss: 8.067425608634949\n",
                        "linear l2 loss: 0.9684617 Linear l2 grad loss: 8.089028477668762\n",
                        "linear l2 loss: 0.96767396 Linear l2 grad loss: 8.110676169395447\n",
                        "linear l2 loss: 0.96688694 Linear l2 grad loss: 8.132246613502502\n",
                        "linear l2 loss: 0.9661002 Linear l2 grad loss: 8.15376591682434\n",
                        "linear l2 loss: 0.96531457 Linear l2 grad loss: 8.175337672233582\n",
                        "linear l2 loss: 0.9645295 Linear l2 grad loss: 8.1969735622406\n",
                        "linear l2 loss: 0.9637453 Linear l2 grad loss: 8.218580842018127\n",
                        "linear l2 loss: 0.962961 Linear l2 grad loss: 8.240087985992432\n",
                        "linear l2 loss: 0.96217775 Linear l2 grad loss: 8.261627197265625\n",
                        "linear l2 loss: 0.9613954 Linear l2 grad loss: 8.283187866210938\n",
                        "linear l2 loss: 0.96061337 Linear l2 grad loss: 8.304744720458984\n",
                        "linear l2 loss: 0.9598322 Linear l2 grad loss: 8.326359152793884\n",
                        "linear l2 loss: 0.9590518 Linear l2 grad loss: 8.347942113876343\n",
                        "linear l2 loss: 0.95827216 Linear l2 grad loss: 8.369553327560425\n",
                        "linear l2 loss: 0.9574931 Linear l2 grad loss: 8.391150832176208\n",
                        "linear l2 loss: 0.95671445 Linear l2 grad loss: 8.412724018096924\n",
                        "linear l2 loss: 0.9559366 Linear l2 grad loss: 8.434255003929138\n",
                        "linear l2 loss: 0.95515996 Linear l2 grad loss: 8.455987811088562\n",
                        "linear l2 loss: 0.9543834 Linear l2 grad loss: 8.47752845287323\n",
                        "linear l2 loss: 0.9536077 Linear l2 grad loss: 8.49911618232727\n",
                        "linear l2 loss: 0.95283234 Linear l2 grad loss: 8.520697474479675\n",
                        "linear l2 loss: 0.95205754 Linear l2 grad loss: 8.542197704315186\n",
                        "linear l2 loss: 0.95128393 Linear l2 grad loss: 8.563809752464294\n",
                        "linear l2 loss: 0.950511 Linear l2 grad loss: 8.585458755493164\n",
                        "linear l2 loss: 0.9497394 Linear l2 grad loss: 8.607233881950378\n",
                        "linear l2 loss: 0.94896716 Linear l2 grad loss: 8.62873113155365\n",
                        "linear l2 loss: 0.94819605 Linear l2 grad loss: 8.6503164768219\n",
                        "linear l2 loss: 0.94742596 Linear l2 grad loss: 8.671982884407043\n",
                        "linear l2 loss: 0.94665635 Linear l2 grad loss: 8.693592429161072\n",
                        "linear l2 loss: 0.9458866 Linear l2 grad loss: 8.715060353279114\n",
                        "linear l2 loss: 0.9451186 Linear l2 grad loss: 8.736754059791565\n",
                        "linear l2 loss: 0.9443504 Linear l2 grad loss: 8.758203506469727\n",
                        "linear l2 loss: 0.94358313 Linear l2 grad loss: 8.779738068580627\n",
                        "linear l2 loss: 0.94281644 Linear l2 grad loss: 8.801251649856567\n",
                        "linear l2 loss: 0.94205076 Linear l2 grad loss: 8.822882533073425\n",
                        "linear l2 loss: 0.9412861 Linear l2 grad loss: 8.844545602798462\n",
                        "linear l2 loss: 0.9405219 Linear l2 grad loss: 8.866158127784729\n",
                        "linear l2 loss: 0.9397578 Linear l2 grad loss: 8.887672543525696\n",
                        "linear l2 loss: 0.938995 Linear l2 grad loss: 8.909307956695557\n",
                        "linear l2 loss: 0.9382332 Linear l2 grad loss: 8.93099594116211\n",
                        "linear l2 loss: 0.9374717 Linear l2 grad loss: 8.95261561870575\n",
                        "linear l2 loss: 0.9367108 Linear l2 grad loss: 8.974192380905151\n",
                        "linear l2 loss: 0.9359495 Linear l2 grad loss: 8.995585799217224\n",
                        "linear l2 loss: 0.9351902 Linear l2 grad loss: 9.017231106758118\n",
                        "linear l2 loss: 0.93443125 Linear l2 grad loss: 9.038782477378845\n",
                        "linear l2 loss: 0.933673 Linear l2 grad loss: 9.060365796089172\n",
                        "linear l2 loss: 0.93291533 Linear l2 grad loss: 9.081901550292969\n",
                        "linear l2 loss: 0.9321589 Linear l2 grad loss: 9.103589534759521\n",
                        "linear l2 loss: 0.9314026 Linear l2 grad loss: 9.125121593475342\n",
                        "linear l2 loss: 0.93064654 Linear l2 grad loss: 9.146556973457336\n",
                        "linear l2 loss: 0.9298911 Linear l2 grad loss: 9.16799259185791\n",
                        "linear l2 loss: 0.92913705 Linear l2 grad loss: 9.189577579498291\n",
                        "linear l2 loss: 0.9283835 Linear l2 grad loss: 9.211109638214111\n",
                        "linear l2 loss: 0.9276309 Linear l2 grad loss: 9.232713222503662\n",
                        "linear l2 loss: 0.92687875 Linear l2 grad loss: 9.254214882850647\n",
                        "linear l2 loss: 0.92612785 Linear l2 grad loss: 9.275841474533081\n",
                        "linear l2 loss: 0.92537755 Linear l2 grad loss: 9.297508478164673\n",
                        "linear l2 loss: 0.92462796 Linear l2 grad loss: 9.319166779518127\n",
                        "linear l2 loss: 0.9238788 Linear l2 grad loss: 9.34071695804596\n",
                        "linear l2 loss: 0.9231295 Linear l2 grad loss: 9.362119793891907\n",
                        "linear l2 loss: 0.9223817 Linear l2 grad loss: 9.383647322654724\n",
                        "linear l2 loss: 0.9216342 Linear l2 grad loss: 9.405147194862366\n",
                        "linear l2 loss: 0.92088777 Linear l2 grad loss: 9.426664471626282\n",
                        "linear l2 loss: 0.920142 Linear l2 grad loss: 9.448211908340454\n",
                        "linear l2 loss: 0.9193969 Linear l2 grad loss: 9.46973741054535\n",
                        "linear l2 loss: 0.91865224 Linear l2 grad loss: 9.49121880531311\n",
                        "linear l2 loss: 0.9179082 Linear l2 grad loss: 9.51265025138855\n",
                        "linear l2 loss: 0.91716546 Linear l2 grad loss: 9.534207344055176\n",
                        "linear l2 loss: 0.91642326 Linear l2 grad loss: 9.555737972259521\n",
                        "linear l2 loss: 0.91568154 Linear l2 grad loss: 9.577250719070435\n",
                        "linear l2 loss: 0.9149408 Linear l2 grad loss: 9.598745942115784\n",
                        "linear l2 loss: 0.91419995 Linear l2 grad loss: 9.62013828754425\n",
                        "linear l2 loss: 0.9134607 Linear l2 grad loss: 9.641650319099426\n",
                        "linear l2 loss: 0.9127223 Linear l2 grad loss: 9.663239359855652\n",
                        "linear l2 loss: 0.91198397 Linear l2 grad loss: 9.684646725654602\n",
                        "linear l2 loss: 0.9112468 Linear l2 grad loss: 9.706159710884094\n",
                        "linear l2 loss: 0.91051006 Linear l2 grad loss: 9.727623224258423\n",
                        "linear l2 loss: 0.9097745 Linear l2 grad loss: 9.749186158180237\n",
                        "linear l2 loss: 0.9090392 Linear l2 grad loss: 9.770639300346375\n",
                        "linear l2 loss: 0.9083045 Linear l2 grad loss: 9.792047500610352\n",
                        "linear l2 loss: 0.90757066 Linear l2 grad loss: 9.81346845626831\n",
                        "linear l2 loss: 0.90683746 Linear l2 grad loss: 9.834880709648132\n",
                        "linear l2 loss: 0.90610516 Linear l2 grad loss: 9.85632598400116\n",
                        "linear l2 loss: 0.9053732 Linear l2 grad loss: 9.877723693847656\n",
                        "linear l2 loss: 0.9046422 Linear l2 grad loss: 9.89915418624878\n",
                        "linear l2 loss: 0.90391177 Linear l2 grad loss: 9.92051887512207\n",
                        "linear l2 loss: 0.90318173 Linear l2 grad loss: 9.941856622695923\n",
                        "linear l2 loss: 0.90245306 Linear l2 grad loss: 9.96327269077301\n",
                        "linear l2 loss: 0.9017248 Linear l2 grad loss: 9.984644412994385\n",
                        "linear l2 loss: 0.90099746 Linear l2 grad loss: 10.006018280982971\n",
                        "linear l2 loss: 0.9002711 Linear l2 grad loss: 10.027492880821228\n",
                        "linear l2 loss: 0.899545 Linear l2 grad loss: 10.048821687698364\n",
                        "linear l2 loss: 0.8988191 Linear l2 grad loss: 10.070031762123108\n",
                        "linear l2 loss: 0.8980939 Linear l2 grad loss: 10.091245532035828\n",
                        "linear l2 loss: 0.89737016 Linear l2 grad loss: 10.112598776817322\n",
                        "linear l2 loss: 0.8966471 Linear l2 grad loss: 10.133971810340881\n",
                        "linear l2 loss: 0.89592487 Linear l2 grad loss: 10.155319094657898\n",
                        "linear l2 loss: 0.89520305 Linear l2 grad loss: 10.176599860191345\n",
                        "linear l2 loss: 0.89448255 Linear l2 grad loss: 10.19803524017334\n",
                        "linear l2 loss: 0.8937618 Linear l2 grad loss: 10.219231724739075\n",
                        "linear l2 loss: 0.893042 Linear l2 grad loss: 10.240455985069275\n",
                        "linear l2 loss: 0.89232296 Linear l2 grad loss: 10.261699199676514\n",
                        "linear l2 loss: 0.89160585 Linear l2 grad loss: 10.283166646957397\n",
                        "linear l2 loss: 0.89088887 Linear l2 grad loss: 10.304508447647095\n",
                        "linear l2 loss: 0.8901723 Linear l2 grad loss: 10.32574188709259\n",
                        "linear l2 loss: 0.8894558 Linear l2 grad loss: 10.346887826919556\n",
                        "linear l2 loss: 0.88874084 Linear l2 grad loss: 10.368184924125671\n",
                        "linear l2 loss: 0.8880264 Linear l2 grad loss: 10.38937258720398\n",
                        "linear l2 loss: 0.887313 Linear l2 grad loss: 10.410660982131958\n",
                        "linear l2 loss: 0.8865997 Linear l2 grad loss: 10.431837677955627\n",
                        "linear l2 loss: 0.88588756 Linear l2 grad loss: 10.45302927494049\n",
                        "linear l2 loss: 0.88517565 Linear l2 grad loss: 10.474153757095337\n",
                        "linear l2 loss: 0.8844649 Linear l2 grad loss: 10.495341658592224\n",
                        "linear l2 loss: 0.88375455 Linear l2 grad loss: 10.516441822052002\n",
                        "linear l2 loss: 0.8830455 Linear l2 grad loss: 10.537645816802979\n",
                        "linear l2 loss: 0.8823368 Linear l2 grad loss: 10.558812856674194\n",
                        "linear l2 loss: 0.8816288 Linear l2 grad loss: 10.57991874217987\n",
                        "linear l2 loss: 0.8809213 Linear l2 grad loss: 10.600913166999817\n",
                        "linear l2 loss: 0.8802147 Linear l2 grad loss: 10.622005581855774\n",
                        "linear l2 loss: 0.87950945 Linear l2 grad loss: 10.643195986747742\n",
                        "linear l2 loss: 0.878804 Linear l2 grad loss: 10.664166688919067\n",
                        "linear l2 loss: 0.87809974 Linear l2 grad loss: 10.685233473777771\n",
                        "linear l2 loss: 0.8773965 Linear l2 grad loss: 10.706345796585083\n",
                        "linear l2 loss: 0.8766937 Linear l2 grad loss: 10.727384567260742\n",
                        "linear l2 loss: 0.8759918 Linear l2 grad loss: 10.748464584350586\n",
                        "linear l2 loss: 0.8752905 Linear l2 grad loss: 10.769468307495117\n",
                        "linear l2 loss: 0.87459004 Linear l2 grad loss: 10.790530800819397\n",
                        "linear l2 loss: 0.8738904 Linear l2 grad loss: 10.811540722846985\n",
                        "linear l2 loss: 0.8731909 Linear l2 grad loss: 10.832467198371887\n",
                        "linear l2 loss: 0.8724929 Linear l2 grad loss: 10.853477835655212\n",
                        "linear l2 loss: 0.871795 Linear l2 grad loss: 10.874414920806885\n",
                        "linear l2 loss: 0.87109774 Linear l2 grad loss: 10.895277380943298\n",
                        "linear l2 loss: 0.8704014 Linear l2 grad loss: 10.916154265403748\n",
                        "linear l2 loss: 0.869706 Linear l2 grad loss: 10.93710482120514\n",
                        "linear l2 loss: 0.8690112 Linear l2 grad loss: 10.958003997802734\n",
                        "linear l2 loss: 0.8683171 Linear l2 grad loss: 10.978839755058289\n",
                        "linear l2 loss: 0.8676239 Linear l2 grad loss: 10.9997478723526\n",
                        "linear l2 loss: 0.8669319 Linear l2 grad loss: 11.020693182945251\n",
                        "linear l2 loss: 0.8662403 Linear l2 grad loss: 11.041553854942322\n",
                        "linear l2 loss: 0.8655495 Linear l2 grad loss: 11.062419533729553\n",
                        "linear l2 loss: 0.86485875 Linear l2 grad loss: 11.083151698112488\n",
                        "linear l2 loss: 0.8641693 Linear l2 grad loss: 11.10398542881012\n",
                        "linear l2 loss: 0.86348003 Linear l2 grad loss: 11.124686241149902\n",
                        "linear l2 loss: 0.862792 Linear l2 grad loss: 11.14547848701477\n",
                        "linear l2 loss: 0.8621049 Linear l2 grad loss: 11.16627037525177\n",
                        "linear l2 loss: 0.86141795 Linear l2 grad loss: 11.186945796012878\n",
                        "linear l2 loss: 0.860732 Linear l2 grad loss: 11.207688689231873\n",
                        "linear l2 loss: 0.86004657 Linear l2 grad loss: 11.228310108184814\n",
                        "linear l2 loss: 0.85936177 Linear l2 grad loss: 11.248934507369995\n",
                        "linear l2 loss: 0.85867894 Linear l2 grad loss: 11.269760966300964\n",
                        "linear l2 loss: 0.8579957 Linear l2 grad loss: 11.290371775627136\n",
                        "linear l2 loss: 0.8573131 Linear l2 grad loss: 11.310946941375732\n",
                        "linear l2 loss: 0.8566327 Linear l2 grad loss: 11.331772804260254\n",
                        "linear l2 loss: 0.8559518 Linear l2 grad loss: 11.352387189865112\n",
                        "linear l2 loss: 0.855272 Linear l2 grad loss: 11.373034358024597\n",
                        "linear l2 loss: 0.8545926 Linear l2 grad loss: 11.393564105033875\n",
                        "linear l2 loss: 0.8539133 Linear l2 grad loss: 11.414003014564514\n",
                        "linear l2 loss: 0.85323524 Linear l2 grad loss: 11.43446695804596\n",
                        "linear l2 loss: 0.8525588 Linear l2 grad loss: 11.455129384994507\n",
                        "linear l2 loss: 0.8518829 Linear l2 grad loss: 11.475697994232178\n",
                        "linear l2 loss: 0.8512072 Linear l2 grad loss: 11.496192932128906\n",
                        "linear l2 loss: 0.8505314 Linear l2 grad loss: 11.51649010181427\n",
                        "linear l2 loss: 0.8498575 Linear l2 grad loss: 11.537000894546509\n",
                        "linear l2 loss: 0.84918374 Linear l2 grad loss: 11.55738914012909\n",
                        "linear l2 loss: 0.8485114 Linear l2 grad loss: 11.577856659889221\n",
                        "linear l2 loss: 0.8478398 Linear l2 grad loss: 11.598298192024231\n",
                        "linear l2 loss: 0.84716874 Linear l2 grad loss: 11.618736624717712\n",
                        "linear l2 loss: 0.84649783 Linear l2 grad loss: 11.639055013656616\n",
                        "linear l2 loss: 0.8458282 Linear l2 grad loss: 11.65937602519989\n",
                        "linear l2 loss: 0.8451592 Linear l2 grad loss: 11.679709911346436\n",
                        "linear l2 loss: 0.8444907 Linear l2 grad loss: 11.700010418891907\n",
                        "linear l2 loss: 0.8438234 Linear l2 grad loss: 11.72032630443573\n",
                        "linear l2 loss: 0.8431563 Linear l2 grad loss: 11.7405446767807\n",
                        "linear l2 loss: 0.8424897 Linear l2 grad loss: 11.760726571083069\n",
                        "linear l2 loss: 0.84182435 Linear l2 grad loss: 11.780974745750427\n",
                        "linear l2 loss: 0.84115964 Linear l2 grad loss: 11.80117928981781\n",
                        "linear l2 loss: 0.8404965 Linear l2 grad loss: 11.821512341499329\n",
                        "linear l2 loss: 0.8398337 Linear l2 grad loss: 11.841735601425171\n",
                        "linear l2 loss: 0.8391716 Linear l2 grad loss: 11.861951231956482\n",
                        "linear l2 loss: 0.83850974 Linear l2 grad loss: 11.88206934928894\n",
                        "linear l2 loss: 0.8378483 Linear l2 grad loss: 11.902115225791931\n",
                        "linear l2 loss: 0.8371879 Linear l2 grad loss: 11.92216944694519\n",
                        "linear l2 loss: 0.83652794 Linear l2 grad loss: 11.942196249961853\n",
                        "linear l2 loss: 0.8358697 Linear l2 grad loss: 11.962340116500854\n",
                        "linear l2 loss: 0.8352116 Linear l2 grad loss: 11.98236620426178\n",
                        "linear l2 loss: 0.83455384 Linear l2 grad loss: 12.0023113489151\n",
                        "linear l2 loss: 0.83389753 Linear l2 grad loss: 12.022351622581482\n",
                        "linear l2 loss: 0.83324164 Linear l2 grad loss: 12.042327642440796\n",
                        "linear l2 loss: 0.83258647 Linear l2 grad loss: 12.062304496765137\n",
                        "linear l2 loss: 0.83193225 Linear l2 grad loss: 12.082271456718445\n",
                        "linear l2 loss: 0.8312783 Linear l2 grad loss: 12.102160215377808\n",
                        "linear l2 loss: 0.830625 Linear l2 grad loss: 12.12198805809021\n",
                        "linear l2 loss: 0.82997257 Linear l2 grad loss: 12.141838788986206\n",
                        "linear l2 loss: 0.8293211 Linear l2 grad loss: 12.161697030067444\n",
                        "linear l2 loss: 0.82867044 Linear l2 grad loss: 12.181586980819702\n",
                        "linear l2 loss: 0.8280207 Linear l2 grad loss: 12.20143473148346\n",
                        "linear l2 loss: 0.8273708 Linear l2 grad loss: 12.221157670021057\n",
                        "linear l2 loss: 0.82672215 Linear l2 grad loss: 12.240890979766846\n",
                        "linear l2 loss: 0.8260743 Linear l2 grad loss: 12.26064920425415\n",
                        "linear l2 loss: 0.8254269 Linear l2 grad loss: 12.280366897583008\n",
                        "linear l2 loss: 0.82478076 Linear l2 grad loss: 12.30012571811676\n",
                        "linear l2 loss: 0.82413465 Linear l2 grad loss: 12.319728255271912\n",
                        "linear l2 loss: 0.8234896 Linear l2 grad loss: 12.339381575584412\n",
                        "linear l2 loss: 0.82284576 Linear l2 grad loss: 12.359111547470093\n",
                        "linear l2 loss: 0.8222017 Linear l2 grad loss: 12.378685355186462\n",
                        "linear l2 loss: 0.8215591 Linear l2 grad loss: 12.39829134941101\n",
                        "linear l2 loss: 0.8209162 Linear l2 grad loss: 12.417758107185364\n",
                        "linear l2 loss: 0.82027453 Linear l2 grad loss: 12.437282681465149\n",
                        "linear l2 loss: 0.8196339 Linear l2 grad loss: 12.456830978393555\n",
                        "linear l2 loss: 0.81899405 Linear l2 grad loss: 12.476359724998474\n",
                        "linear l2 loss: 0.8183548 Linear l2 grad loss: 12.495844006538391\n",
                        "linear l2 loss: 0.8177163 Linear l2 grad loss: 12.515342235565186\n",
                        "linear l2 loss: 0.8170784 Linear l2 grad loss: 12.534769177436829\n",
                        "linear l2 loss: 0.81644183 Linear l2 grad loss: 12.554282188415527\n",
                        "linear l2 loss: 0.81580526 Linear l2 grad loss: 12.573660135269165\n",
                        "linear l2 loss: 0.81516886 Linear l2 grad loss: 12.592927098274231\n",
                        "linear l2 loss: 0.81453365 Linear l2 grad loss: 12.612224698066711\n",
                        "linear l2 loss: 0.81389964 Linear l2 grad loss: 12.631612300872803\n",
                        "linear l2 loss: 0.813266 Linear l2 grad loss: 12.650923728942871\n",
                        "linear l2 loss: 0.8126327 Linear l2 grad loss: 12.670138597488403\n",
                        "linear l2 loss: 0.8120003 Linear l2 grad loss: 12.689348340034485\n",
                        "linear l2 loss: 0.8113689 Linear l2 grad loss: 12.708593964576721\n",
                        "linear l2 loss: 0.81073827 Linear l2 grad loss: 12.727840662002563\n",
                        "linear l2 loss: 0.8101083 Linear l2 grad loss: 12.747030019760132\n",
                        "linear l2 loss: 0.80947953 Linear l2 grad loss: 12.76630699634552\n",
                        "linear l2 loss: 0.8088507 Linear l2 grad loss: 12.785381078720093\n",
                        "linear l2 loss: 0.80822164 Linear l2 grad loss: 12.804360628128052\n",
                        "linear l2 loss: 0.8075943 Linear l2 grad loss: 12.823434352874756\n",
                        "linear l2 loss: 0.8069672 Linear l2 grad loss: 12.842400789260864\n",
                        "linear l2 loss: 0.8063411 Linear l2 grad loss: 12.861401915550232\n",
                        "linear l2 loss: 0.80571586 Linear l2 grad loss: 12.880402684211731\n",
                        "linear l2 loss: 0.8050923 Linear l2 grad loss: 12.899535536766052\n",
                        "linear l2 loss: 0.804468 Linear l2 grad loss: 12.91845178604126\n",
                        "linear l2 loss: 0.80384505 Linear l2 grad loss: 12.93740999698639\n",
                        "linear l2 loss: 0.8032223 Linear l2 grad loss: 12.956268787384033\n",
                        "linear l2 loss: 0.8026005 Linear l2 grad loss: 12.975141406059265\n",
                        "linear l2 loss: 0.80197954 Linear l2 grad loss: 12.994027733802795\n",
                        "linear l2 loss: 0.801359 Linear l2 grad loss: 13.012838959693909\n",
                        "linear l2 loss: 0.8007396 Linear l2 grad loss: 13.03169047832489\n",
                        "linear l2 loss: 0.8001203 Linear l2 grad loss: 13.050434112548828\n",
                        "linear l2 loss: 0.79950273 Linear l2 grad loss: 13.069292306900024\n",
                        "linear l2 loss: 0.798885 Linear l2 grad loss: 13.088019013404846\n",
                        "linear l2 loss: 0.798268 Linear l2 grad loss: 13.106736063957214\n",
                        "linear l2 loss: 0.79765147 Linear l2 grad loss: 13.12537956237793\n",
                        "linear l2 loss: 0.7970355 Linear l2 grad loss: 13.143936157226562\n",
                        "linear l2 loss: 0.7964205 Linear l2 grad loss: 13.16259217262268\n",
                        "linear l2 loss: 0.79580635 Linear l2 grad loss: 13.181200981140137\n",
                        "linear l2 loss: 0.7951934 Linear l2 grad loss: 13.199861407279968\n",
                        "linear l2 loss: 0.79458064 Linear l2 grad loss: 13.21842622756958\n",
                        "linear l2 loss: 0.7939682 Linear l2 grad loss: 13.236917614936829\n",
                        "linear l2 loss: 0.79335624 Linear l2 grad loss: 13.25536334514618\n",
                        "linear l2 loss: 0.7927453 Linear l2 grad loss: 13.27380657196045\n",
                        "linear l2 loss: 0.79213583 Linear l2 grad loss: 13.292349696159363\n",
                        "linear l2 loss: 0.7915263 Linear l2 grad loss: 13.310791015625\n",
                        "linear l2 loss: 0.79091763 Linear l2 grad loss: 13.329210638999939\n",
                        "linear l2 loss: 0.7903099 Linear l2 grad loss: 13.347620487213135\n",
                        "linear l2 loss: 0.78970236 Linear l2 grad loss: 13.365966320037842\n",
                        "linear l2 loss: 0.7890954 Linear l2 grad loss: 13.384257316589355\n",
                        "linear l2 loss: 0.7884895 Linear l2 grad loss: 13.402580261230469\n",
                        "linear l2 loss: 0.7878848 Linear l2 grad loss: 13.420961141586304\n",
                        "linear l2 loss: 0.7872795 Linear l2 grad loss: 13.439114093780518\n",
                        "linear l2 loss: 0.7866753 Linear l2 grad loss: 13.457314491271973\n",
                        "linear l2 loss: 0.7860721 Linear l2 grad loss: 13.475521802902222\n",
                        "linear l2 loss: 0.7854687 Linear l2 grad loss: 13.493609189987183\n",
                        "linear l2 loss: 0.7848672 Linear l2 grad loss: 13.511843204498291\n",
                        "linear l2 loss: 0.78426594 Linear l2 grad loss: 13.529981136322021\n",
                        "linear l2 loss: 0.7836652 Linear l2 grad loss: 13.548075199127197\n",
                        "linear l2 loss: 0.78306496 Linear l2 grad loss: 13.566086292266846\n",
                        "linear l2 loss: 0.78246605 Linear l2 grad loss: 13.584208250045776\n",
                        "linear l2 loss: 0.7818667 Linear l2 grad loss: 13.602136373519897\n",
                        "linear l2 loss: 0.7812691 Linear l2 grad loss: 13.620182275772095\n",
                        "linear l2 loss: 0.78067154 Linear l2 grad loss: 13.638116598129272\n",
                        "linear l2 loss: 0.78007424 Linear l2 grad loss: 13.65597152709961\n",
                        "linear l2 loss: 0.77947843 Linear l2 grad loss: 13.673930644989014\n",
                        "linear l2 loss: 0.77888316 Linear l2 grad loss: 13.691833734512329\n",
                        "linear l2 loss: 0.7782882 Linear l2 grad loss: 13.709648847579956\n",
                        "linear l2 loss: 0.7776947 Linear l2 grad loss: 13.727572441101074\n",
                        "linear l2 loss: 0.777101 Linear l2 grad loss: 13.745319366455078\n",
                        "linear l2 loss: 0.7765079 Linear l2 grad loss: 13.763089179992676\n",
                        "linear l2 loss: 0.77591634 Linear l2 grad loss: 13.78091835975647\n",
                        "linear l2 loss: 0.7753246 Linear l2 grad loss: 13.798601388931274\n",
                        "linear l2 loss: 0.7747334 Linear l2 grad loss: 13.816246032714844\n",
                        "linear l2 loss: 0.7741433 Linear l2 grad loss: 13.8339524269104\n",
                        "linear l2 loss: 0.77355486 Linear l2 grad loss: 13.851747274398804\n",
                        "linear l2 loss: 0.77296585 Linear l2 grad loss: 13.869333744049072\n",
                        "linear l2 loss: 0.772377 Linear l2 grad loss: 13.886852741241455\n",
                        "linear l2 loss: 0.7717891 Linear l2 grad loss: 13.904351472854614\n",
                        "linear l2 loss: 0.7712034 Linear l2 grad loss: 13.922046899795532\n",
                        "linear l2 loss: 0.7706166 Linear l2 grad loss: 13.939541101455688\n",
                        "linear l2 loss: 0.77002996 Linear l2 grad loss: 13.95687460899353\n",
                        "linear l2 loss: 0.7694456 Linear l2 grad loss: 13.9744234085083\n",
                        "linear l2 loss: 0.7688607 Linear l2 grad loss: 13.991780996322632\n",
                        "linear l2 loss: 0.76827717 Linear l2 grad loss: 14.009222745895386\n",
                        "linear l2 loss: 0.76769394 Linear l2 grad loss: 14.026564121246338\n",
                        "linear l2 loss: 0.7671114 Linear l2 grad loss: 14.043929100036621\n",
                        "linear l2 loss: 0.76653004 Linear l2 grad loss: 14.061321258544922\n",
                        "linear l2 loss: 0.76594865 Linear l2 grad loss: 14.078573226928711\n",
                        "linear l2 loss: 0.7653679 Linear l2 grad loss: 14.095823764801025\n",
                        "linear l2 loss: 0.7647885 Linear l2 grad loss: 14.113141775131226\n",
                        "linear l2 loss: 0.7642094 Linear l2 grad loss: 14.130383968353271\n",
                        "linear l2 loss: 0.7636307 Linear l2 grad loss: 14.14759635925293\n",
                        "linear l2 loss: 0.76305205 Linear l2 grad loss: 14.164673566818237\n",
                        "linear l2 loss: 0.7624748 Linear l2 grad loss: 14.181828260421753\n",
                        "linear l2 loss: 0.7618976 Linear l2 grad loss: 14.198889255523682\n",
                        "linear l2 loss: 0.76132077 Linear l2 grad loss: 14.215847969055176\n",
                        "linear l2 loss: 0.7607464 Linear l2 grad loss: 14.23308801651001\n",
                        "linear l2 loss: 0.76017183 Linear l2 grad loss: 14.25017762184143\n",
                        "linear l2 loss: 0.7595975 Linear l2 grad loss: 14.26714539527893\n",
                        "linear l2 loss: 0.75902396 Linear l2 grad loss: 14.284151077270508\n",
                        "linear l2 loss: 0.7584506 Linear l2 grad loss: 14.301050186157227\n",
                        "linear l2 loss: 0.7578778 Linear l2 grad loss: 14.31792163848877\n",
                        "linear l2 loss: 0.75730616 Linear l2 grad loss: 14.334843158721924\n",
                        "linear l2 loss: 0.75673497 Linear l2 grad loss: 14.351722002029419\n",
                        "linear l2 loss: 0.7561653 Linear l2 grad loss: 14.368674278259277\n",
                        "linear l2 loss: 0.75559473 Linear l2 grad loss: 14.385408401489258\n",
                        "linear l2 loss: 0.7550251 Linear l2 grad loss: 14.40215516090393\n",
                        "linear l2 loss: 0.75445646 Linear l2 grad loss: 14.418930768966675\n",
                        "linear l2 loss: 0.7538883 Linear l2 grad loss: 14.43564248085022\n",
                        "linear l2 loss: 0.753321 Linear l2 grad loss: 14.45238447189331\n",
                        "linear l2 loss: 0.7527546 Linear l2 grad loss: 14.469140768051147\n",
                        "linear l2 loss: 0.75218767 Linear l2 grad loss: 14.48565936088562\n",
                        "linear l2 loss: 0.75162184 Linear l2 grad loss: 14.502284288406372\n",
                        "linear l2 loss: 0.75105715 Linear l2 grad loss: 14.518945455551147\n",
                        "linear l2 loss: 0.7504922 Linear l2 grad loss: 14.535447359085083\n",
                        "linear l2 loss: 0.7499292 Linear l2 grad loss: 14.552096605300903\n",
                        "linear l2 loss: 0.7493653 Linear l2 grad loss: 14.568552732467651\n",
                        "linear l2 loss: 0.7488031 Linear l2 grad loss: 14.585084915161133\n",
                        "linear l2 loss: 0.74824136 Linear l2 grad loss: 14.601598501205444\n",
                        "linear l2 loss: 0.74768 Linear l2 grad loss: 14.618049383163452\n",
                        "linear l2 loss: 0.7471183 Linear l2 grad loss: 14.634330034255981\n",
                        "linear l2 loss: 0.74655795 Linear l2 grad loss: 14.650702476501465\n",
                        "linear l2 loss: 0.7459984 Linear l2 grad loss: 14.667084217071533\n",
                        "linear l2 loss: 0.74543965 Linear l2 grad loss: 14.683441638946533\n",
                        "linear l2 loss: 0.7448815 Linear l2 grad loss: 14.699799299240112\n",
                        "linear l2 loss: 0.74432427 Linear l2 grad loss: 14.716161966323853\n",
                        "linear l2 loss: 0.7437665 Linear l2 grad loss: 14.732332229614258\n",
                        "linear l2 loss: 0.7432089 Linear l2 grad loss: 14.748432397842407\n",
                        "linear l2 loss: 0.7426531 Linear l2 grad loss: 14.764647960662842\n",
                        "linear l2 loss: 0.74209833 Linear l2 grad loss: 14.780944347381592\n",
                        "linear l2 loss: 0.7415435 Linear l2 grad loss: 14.797075748443604\n",
                        "linear l2 loss: 0.7409887 Linear l2 grad loss: 14.813125133514404\n",
                        "linear l2 loss: 0.74043524 Linear l2 grad loss: 14.829249620437622\n",
                        "linear l2 loss: 0.73988134 Linear l2 grad loss: 14.845211267471313\n",
                        "linear l2 loss: 0.7393287 Linear l2 grad loss: 14.861247301101685\n",
                        "linear l2 loss: 0.7387766 Linear l2 grad loss: 14.877232789993286\n",
                        "linear l2 loss: 0.7382256 Linear l2 grad loss: 14.893256902694702\n",
                        "linear l2 loss: 0.737675 Linear l2 grad loss: 14.909231662750244\n",
                        "linear l2 loss: 0.7371251 Linear l2 grad loss: 14.925211668014526\n",
                        "linear l2 loss: 0.7365749 Linear l2 grad loss: 14.941059589385986\n",
                        "linear l2 loss: 0.7360266 Linear l2 grad loss: 14.95702052116394\n",
                        "linear l2 loss: 0.7354781 Linear l2 grad loss: 14.972875833511353\n",
                        "linear l2 loss: 0.73493004 Linear l2 grad loss: 14.988680839538574\n",
                        "linear l2 loss: 0.73438245 Linear l2 grad loss: 15.004436731338501\n",
                        "linear l2 loss: 0.73383605 Linear l2 grad loss: 15.020260095596313\n",
                        "linear l2 loss: 0.7332906 Linear l2 grad loss: 15.036083698272705\n",
                        "linear l2 loss: 0.7327439 Linear l2 grad loss: 15.051679849624634\n",
                        "linear l2 loss: 0.7321991 Linear l2 grad loss: 15.067378759384155\n",
                        "linear l2 loss: 0.73165435 Linear l2 grad loss: 15.083029747009277\n",
                        "linear l2 loss: 0.7311101 Linear l2 grad loss: 15.098621129989624\n",
                        "linear l2 loss: 0.73056805 Linear l2 grad loss: 15.114385604858398\n",
                        "linear l2 loss: 0.73002535 Linear l2 grad loss: 15.12998080253601\n",
                        "linear l2 loss: 0.72948295 Linear l2 grad loss: 15.14552640914917\n",
                        "linear l2 loss: 0.72894025 Linear l2 grad loss: 15.160908699035645\n",
                        "linear l2 loss: 0.7283988 Linear l2 grad loss: 15.176354885101318\n",
                        "linear l2 loss: 0.72785884 Linear l2 grad loss: 15.191895246505737\n",
                        "linear l2 loss: 0.7273194 Linear l2 grad loss: 15.207435131072998\n",
                        "linear l2 loss: 0.7267789 Linear l2 grad loss: 15.222718238830566\n",
                        "linear l2 loss: 0.7262401 Linear l2 grad loss: 15.2381112575531\n",
                        "linear l2 loss: 0.725702 Linear l2 grad loss: 15.253525495529175\n",
                        "linear l2 loss: 0.72516423 Linear l2 grad loss: 15.268853425979614\n",
                        "linear l2 loss: 0.72462714 Linear l2 grad loss: 15.284183740615845\n",
                        "linear l2 loss: 0.72409075 Linear l2 grad loss: 15.299530982971191\n",
                        "linear l2 loss: 0.72355556 Linear l2 grad loss: 15.314905166625977\n",
                        "linear l2 loss: 0.7230193 Linear l2 grad loss: 15.330037355422974\n",
                        "linear l2 loss: 0.7224844 Linear l2 grad loss: 15.345252513885498\n",
                        "linear l2 loss: 0.72194976 Linear l2 grad loss: 15.360413312911987\n",
                        "linear l2 loss: 0.7214157 Linear l2 grad loss: 15.375564575195312\n",
                        "linear l2 loss: 0.72088283 Linear l2 grad loss: 15.39074182510376\n",
                        "linear l2 loss: 0.7203501 Linear l2 grad loss: 15.405869245529175\n",
                        "linear l2 loss: 0.71981734 Linear l2 grad loss: 15.420876026153564\n",
                        "linear l2 loss: 0.7192857 Linear l2 grad loss: 15.43595290184021\n",
                        "linear l2 loss: 0.7187544 Linear l2 grad loss: 15.450960636138916\n",
                        "linear l2 loss: 0.71822345 Linear l2 grad loss: 15.4659264087677\n",
                        "linear l2 loss: 0.71769375 Linear l2 grad loss: 15.480942010879517\n",
                        "linear l2 loss: 0.71716386 Linear l2 grad loss: 15.495845556259155\n",
                        "linear l2 loss: 0.716636 Linear l2 grad loss: 15.510922908782959\n",
                        "linear l2 loss: 0.7161073 Linear l2 grad loss: 15.525779485702515\n",
                        "linear l2 loss: 0.7155788 Linear l2 grad loss: 15.540597677230835\n",
                        "linear l2 loss: 0.7150512 Linear l2 grad loss: 15.555400848388672\n",
                        "linear l2 loss: 0.71452516 Linear l2 grad loss: 15.570337533950806\n",
                        "linear l2 loss: 0.7139993 Linear l2 grad loss: 15.585191249847412\n",
                        "linear l2 loss: 0.71347344 Linear l2 grad loss: 15.599958896636963\n",
                        "linear l2 loss: 0.7129474 Linear l2 grad loss: 15.614603519439697\n",
                        "linear l2 loss: 0.71242255 Linear l2 grad loss: 15.629308938980103\n",
                        "linear l2 loss: 0.711898 Linear l2 grad loss: 15.6439528465271\n",
                        "linear l2 loss: 0.71137494 Linear l2 grad loss: 15.658668518066406\n",
                        "linear l2 loss: 0.7108517 Linear l2 grad loss: 15.673338890075684\n",
                        "linear l2 loss: 0.7103301 Linear l2 grad loss: 15.688052654266357\n",
                        "linear l2 loss: 0.7098072 Linear l2 grad loss: 15.702550649642944\n",
                        "linear l2 loss: 0.7092852 Linear l2 grad loss: 15.717052221298218\n",
                        "linear l2 loss: 0.7087643 Linear l2 grad loss: 15.731611967086792\n",
                        "linear l2 loss: 0.7082433 Linear l2 grad loss: 15.746076583862305\n",
                        "linear l2 loss: 0.7077238 Linear l2 grad loss: 15.760626316070557\n",
                        "linear l2 loss: 0.70720446 Linear l2 grad loss: 15.775108337402344\n",
                        "linear l2 loss: 0.7066861 Linear l2 grad loss: 15.789606094360352\n",
                        "linear l2 loss: 0.7061669 Linear l2 grad loss: 15.80394983291626\n",
                        "linear l2 loss: 0.7056494 Linear l2 grad loss: 15.818371772766113\n",
                        "linear l2 loss: 0.7051323 Linear l2 grad loss: 15.832767009735107\n",
                        "linear l2 loss: 0.7046151 Linear l2 grad loss: 15.847070693969727\n",
                        "linear l2 loss: 0.7040987 Linear l2 grad loss: 15.861364841461182\n",
                        "linear l2 loss: 0.7035829 Linear l2 grad loss: 15.875656843185425\n",
                        "linear l2 loss: 0.7030679 Linear l2 grad loss: 15.889978647232056\n",
                        "linear l2 loss: 0.70255244 Linear l2 grad loss: 15.904134273529053\n",
                        "linear l2 loss: 0.70203805 Linear l2 grad loss: 15.918343305587769\n",
                        "linear l2 loss: 0.70152426 Linear l2 grad loss: 15.932518482208252\n",
                        "linear l2 loss: 0.7010113 Linear l2 grad loss: 15.946711540222168\n",
                        "linear l2 loss: 0.7004986 Linear l2 grad loss: 15.96086835861206\n",
                        "linear l2 loss: 0.6999866 Linear l2 grad loss: 15.975023984909058\n",
                        "linear l2 loss: 0.6994752 Linear l2 grad loss: 15.98915958404541\n",
                        "linear l2 loss: 0.6989642 Linear l2 grad loss: 16.00323748588562\n",
                        "linear l2 loss: 0.6984528 Linear l2 grad loss: 16.01719641685486\n",
                        "linear l2 loss: 0.6979421 Linear l2 grad loss: 16.031147956848145\n",
                        "linear l2 loss: 0.69743204 Linear l2 grad loss: 16.045101642608643\n",
                        "linear l2 loss: 0.6969226 Linear l2 grad loss: 16.05906581878662\n",
                        "linear l2 loss: 0.6964139 Linear l2 grad loss: 16.072997093200684\n",
                        "linear l2 loss: 0.69590557 Linear l2 grad loss: 16.086902618408203\n",
                        "linear l2 loss: 0.69539845 Linear l2 grad loss: 16.10086703300476\n",
                        "linear l2 loss: 0.6948909 Linear l2 grad loss: 16.114707469940186\n",
                        "linear l2 loss: 0.69438416 Linear l2 grad loss: 16.128548622131348\n",
                        "linear l2 loss: 0.69387865 Linear l2 grad loss: 16.14244842529297\n",
                        "linear l2 loss: 0.6933729 Linear l2 grad loss: 16.1562442779541\n",
                        "linear l2 loss: 0.69286674 Linear l2 grad loss: 16.169911861419678\n",
                        "linear l2 loss: 0.69236183 Linear l2 grad loss: 16.183655738830566\n",
                        "linear l2 loss: 0.69185793 Linear l2 grad loss: 16.197397708892822\n",
                        "linear l2 loss: 0.69135404 Linear l2 grad loss: 16.211095571517944\n",
                        "linear l2 loss: 0.6908507 Linear l2 grad loss: 16.224735260009766\n",
                        "linear l2 loss: 0.69034714 Linear l2 grad loss: 16.23832130432129\n",
                        "linear l2 loss: 0.68984604 Linear l2 grad loss: 16.252073049545288\n",
                        "linear l2 loss: 0.6893441 Linear l2 grad loss: 16.265666723251343\n",
                        "linear l2 loss: 0.6888429 Linear l2 grad loss: 16.27926993370056\n",
                        "linear l2 loss: 0.6883411 Linear l2 grad loss: 16.292699098587036\n",
                        "linear l2 loss: 0.6878411 Linear l2 grad loss: 16.306278705596924\n",
                        "linear l2 loss: 0.6873415 Linear l2 grad loss: 16.31980872154236\n",
                        "linear l2 loss: 0.68684155 Linear l2 grad loss: 16.333234310150146\n",
                        "linear l2 loss: 0.6863424 Linear l2 grad loss: 16.34666395187378\n",
                        "linear l2 loss: 0.6858446 Linear l2 grad loss: 16.36017107963562\n",
                        "linear l2 loss: 0.6853465 Linear l2 grad loss: 16.373568296432495\n",
                        "linear l2 loss: 0.68484825 Linear l2 grad loss: 16.386860847473145\n",
                        "linear l2 loss: 0.6843512 Linear l2 grad loss: 16.400218725204468\n",
                        "linear l2 loss: 0.68385524 Linear l2 grad loss: 16.413614511489868\n",
                        "linear l2 loss: 0.6833589 Linear l2 grad loss: 16.426915168762207\n",
                        "linear l2 loss: 0.68286335 Linear l2 grad loss: 16.440183401107788\n",
                        "linear l2 loss: 0.682367 Linear l2 grad loss: 16.453301429748535\n",
                        "linear l2 loss: 0.6818723 Linear l2 grad loss: 16.46653127670288\n",
                        "linear l2 loss: 0.6813784 Linear l2 grad loss: 16.479798316955566\n",
                        "linear l2 loss: 0.68088496 Linear l2 grad loss: 16.493017435073853\n",
                        "linear l2 loss: 0.6803911 Linear l2 grad loss: 16.506109714508057\n",
                        "linear l2 loss: 0.6798982 Linear l2 grad loss: 16.51925039291382\n",
                        "linear l2 loss: 0.67940694 Linear l2 grad loss: 16.532461404800415\n",
                        "linear l2 loss: 0.67891437 Linear l2 grad loss: 16.54550337791443\n",
                        "linear l2 loss: 0.67842335 Linear l2 grad loss: 16.558608531951904\n",
                        "linear l2 loss: 0.6779325 Linear l2 grad loss: 16.571685791015625\n",
                        "linear l2 loss: 0.6774418 Linear l2 grad loss: 16.58467721939087\n",
                        "linear l2 loss: 0.6769514 Linear l2 grad loss: 16.59763789176941\n",
                        "linear l2 loss: 0.67646205 Linear l2 grad loss: 16.610634565353394\n",
                        "linear l2 loss: 0.6759733 Linear l2 grad loss: 16.62362504005432\n",
                        "linear l2 loss: 0.67548364 Linear l2 grad loss: 16.636455059051514\n",
                        "linear l2 loss: 0.674996 Linear l2 grad loss: 16.649413108825684\n",
                        "linear l2 loss: 0.67450845 Linear l2 grad loss: 16.66230797767639\n",
                        "linear l2 loss: 0.67402095 Linear l2 grad loss: 16.675163745880127\n",
                        "linear l2 loss: 0.6735346 Linear l2 grad loss: 16.688042879104614\n",
                        "linear l2 loss: 0.673048 Linear l2 grad loss: 16.70084047317505\n",
                        "linear l2 loss: 0.67256236 Linear l2 grad loss: 16.71363854408264\n",
                        "linear l2 loss: 0.6720765 Linear l2 grad loss: 16.726372957229614\n",
                        "linear l2 loss: 0.67159075 Linear l2 grad loss: 16.73902130126953\n",
                        "linear l2 loss: 0.67110705 Linear l2 grad loss: 16.75184988975525\n",
                        "linear l2 loss: 0.67062277 Linear l2 grad loss: 16.76453447341919\n",
                        "linear l2 loss: 0.670139 Linear l2 grad loss: 16.777188301086426\n",
                        "linear l2 loss: 0.66965586 Linear l2 grad loss: 16.789832592010498\n",
                        "linear l2 loss: 0.6691742 Linear l2 grad loss: 16.802587270736694\n",
                        "linear l2 loss: 0.66869086 Linear l2 grad loss: 16.81508183479309\n",
                        "linear l2 loss: 0.6682091 Linear l2 grad loss: 16.827683210372925\n",
                        "linear l2 loss: 0.6677272 Linear l2 grad loss: 16.840176582336426\n",
                        "linear l2 loss: 0.66724694 Linear l2 grad loss: 16.852784633636475\n",
                        "linear l2 loss: 0.6667664 Linear l2 grad loss: 16.865299701690674\n",
                        "linear l2 loss: 0.66628593 Linear l2 grad loss: 16.877777338027954\n",
                        "linear l2 loss: 0.6658063 Linear l2 grad loss: 16.89023494720459\n",
                        "linear l2 loss: 0.6653275 Linear l2 grad loss: 16.902723789215088\n",
                        "linear l2 loss: 0.66484845 Linear l2 grad loss: 16.915131092071533\n",
                        "linear l2 loss: 0.66437036 Linear l2 grad loss: 16.927533864974976\n",
                        "linear l2 loss: 0.66389245 Linear l2 grad loss: 16.939918279647827\n",
                        "linear l2 loss: 0.66341543 Linear l2 grad loss: 16.952346324920654\n",
                        "linear l2 loss: 0.662938 Linear l2 grad loss: 16.964610815048218\n",
                        "linear l2 loss: 0.6624616 Linear l2 grad loss: 16.976941108703613\n",
                        "linear l2 loss: 0.661985 Linear l2 grad loss: 16.989174604415894\n",
                        "linear l2 loss: 0.6615097 Linear l2 grad loss: 17.001516342163086\n",
                        "linear l2 loss: 0.66103494 Linear l2 grad loss: 17.013789415359497\n",
                        "linear l2 loss: 0.6605594 Linear l2 grad loss: 17.025949001312256\n",
                        "linear l2 loss: 0.6600855 Linear l2 grad loss: 17.03819227218628\n",
                        "linear l2 loss: 0.65961105 Linear l2 grad loss: 17.05032181739807\n",
                        "linear l2 loss: 0.65913814 Linear l2 grad loss: 17.06255340576172\n",
                        "linear l2 loss: 0.658665 Linear l2 grad loss: 17.074687480926514\n",
                        "linear l2 loss: 0.6581918 Linear l2 grad loss: 17.08673119544983\n",
                        "linear l2 loss: 0.65772 Linear l2 grad loss: 17.098886728286743\n",
                        "linear l2 loss: 0.6572485 Linear l2 grad loss: 17.11099076271057\n",
                        "linear l2 loss: 0.6567766 Linear l2 grad loss: 17.122976064682007\n",
                        "linear l2 loss: 0.65630525 Linear l2 grad loss: 17.134953260421753\n",
                        "linear l2 loss: 0.6558348 Linear l2 grad loss: 17.14696979522705\n",
                        "linear l2 loss: 0.6553646 Linear l2 grad loss: 17.158945560455322\n",
                        "linear l2 loss: 0.6548958 Linear l2 grad loss: 17.171009302139282\n",
                        "linear l2 loss: 0.654427 Linear l2 grad loss: 17.183011531829834\n",
                        "linear l2 loss: 0.6539574 Linear l2 grad loss: 17.194817543029785\n",
                        "linear l2 loss: 0.65348893 Linear l2 grad loss: 17.206706523895264\n",
                        "linear l2 loss: 0.65302104 Linear l2 grad loss: 17.218612909317017\n",
                        "linear l2 loss: 0.6525536 Linear l2 grad loss: 17.230454206466675\n",
                        "linear l2 loss: 0.6520867 Linear l2 grad loss: 17.2423357963562\n",
                        "linear l2 loss: 0.6516207 Linear l2 grad loss: 17.254227876663208\n",
                        "linear l2 loss: 0.65115356 Linear l2 grad loss: 17.265928983688354\n",
                        "linear l2 loss: 0.6506884 Linear l2 grad loss: 17.277772903442383\n",
                        "linear l2 loss: 0.65022314 Linear l2 grad loss: 17.28953504562378\n",
                        "linear l2 loss: 0.6497578 Linear l2 grad loss: 17.30122709274292\n",
                        "linear l2 loss: 0.64929307 Linear l2 grad loss: 17.31292486190796\n",
                        "linear l2 loss: 0.64882857 Linear l2 grad loss: 17.324576377868652\n",
                        "linear l2 loss: 0.64836496 Linear l2 grad loss: 17.336255311965942\n",
                        "linear l2 loss: 0.6479013 Linear l2 grad loss: 17.347877025604248\n",
                        "linear l2 loss: 0.6474392 Linear l2 grad loss: 17.359594345092773\n",
                        "linear l2 loss: 0.6469771 Linear l2 grad loss: 17.371237754821777\n",
                        "linear l2 loss: 0.6465142 Linear l2 grad loss: 17.382755517959595\n",
                        "linear l2 loss: 0.6460521 Linear l2 grad loss: 17.39424967765808\n",
                        "linear l2 loss: 0.6455912 Linear l2 grad loss: 17.405842304229736\n",
                        "linear l2 loss: 0.6451307 Linear l2 grad loss: 17.417401790618896\n",
                        "linear l2 loss: 0.6446706 Linear l2 grad loss: 17.428915977478027\n",
                        "linear l2 loss: 0.6442095 Linear l2 grad loss: 17.4403018951416\n",
                        "linear l2 loss: 0.6437502 Linear l2 grad loss: 17.451812982559204\n",
                        "linear l2 loss: 0.64329165 Linear l2 grad loss: 17.463316917419434\n",
                        "linear l2 loss: 0.64283174 Linear l2 grad loss: 17.474629402160645\n",
                        "linear l2 loss: 0.64237326 Linear l2 grad loss: 17.486012935638428\n",
                        "linear l2 loss: 0.6419155 Linear l2 grad loss: 17.497427225112915\n",
                        "linear l2 loss: 0.6414578 Linear l2 grad loss: 17.508784532546997\n",
                        "linear l2 loss: 0.6410002 Linear l2 grad loss: 17.52007532119751\n",
                        "linear l2 loss: 0.6405436 Linear l2 grad loss: 17.53140640258789\n",
                        "linear l2 loss: 0.6400874 Linear l2 grad loss: 17.542735815048218\n",
                        "linear l2 loss: 0.63963217 Linear l2 grad loss: 17.554068565368652\n",
                        "linear l2 loss: 0.6391766 Linear l2 grad loss: 17.565338373184204\n",
                        "linear l2 loss: 0.6387212 Linear l2 grad loss: 17.576568603515625\n",
                        "linear l2 loss: 0.63826656 Linear l2 grad loss: 17.58779811859131\n",
                        "linear l2 loss: 0.63781196 Linear l2 grad loss: 17.59896206855774\n",
                        "linear l2 loss: 0.63735837 Linear l2 grad loss: 17.61017918586731\n",
                        "linear l2 loss: 0.6369042 Linear l2 grad loss: 17.621285438537598\n",
                        "linear l2 loss: 0.6364509 Linear l2 grad loss: 17.632397174835205\n",
                        "linear l2 loss: 0.63599783 Linear l2 grad loss: 17.64349865913391\n",
                        "linear l2 loss: 0.6355459 Linear l2 grad loss: 17.654634714126587\n",
                        "linear l2 loss: 0.6350941 Linear l2 grad loss: 17.665729522705078\n",
                        "linear l2 loss: 0.6346422 Linear l2 grad loss: 17.676748037338257\n",
                        "linear l2 loss: 0.63419086 Linear l2 grad loss: 17.68778133392334\n",
                        "linear l2 loss: 0.63373965 Linear l2 grad loss: 17.698745489120483\n",
                        "linear l2 loss: 0.63328993 Linear l2 grad loss: 17.70982074737549\n",
                        "linear l2 loss: 0.6328393 Linear l2 grad loss: 17.720736742019653\n",
                        "linear l2 loss: 0.63238984 Linear l2 grad loss: 17.731725692749023\n",
                        "linear l2 loss: 0.6319408 Linear l2 grad loss: 17.742672204971313\n",
                        "linear l2 loss: 0.63149136 Linear l2 grad loss: 17.753530263900757\n",
                        "linear l2 loss: 0.6310433 Linear l2 grad loss: 17.764473915100098\n",
                        "linear l2 loss: 0.63059556 Linear l2 grad loss: 17.77540612220764\n",
                        "linear l2 loss: 0.6301481 Linear l2 grad loss: 17.786269426345825\n",
                        "linear l2 loss: 0.62970036 Linear l2 grad loss: 17.79708456993103\n",
                        "linear l2 loss: 0.6292543 Linear l2 grad loss: 17.807981491088867\n",
                        "linear l2 loss: 0.6288071 Linear l2 grad loss: 17.81872248649597\n",
                        "linear l2 loss: 0.6283602 Linear l2 grad loss: 17.829421520233154\n",
                        "linear l2 loss: 0.6279146 Linear l2 grad loss: 17.84020709991455\n",
                        "linear l2 loss: 0.6274697 Linear l2 grad loss: 17.850996255874634\n",
                        "linear l2 loss: 0.62702507 Linear l2 grad loss: 17.86177134513855\n",
                        "linear l2 loss: 0.62657946 Linear l2 grad loss: 17.872356414794922\n",
                        "linear l2 loss: 0.6261364 Linear l2 grad loss: 17.88317108154297\n",
                        "linear l2 loss: 0.6256919 Linear l2 grad loss: 17.893752574920654\n",
                        "linear l2 loss: 0.62524927 Linear l2 grad loss: 17.90449571609497\n",
                        "linear l2 loss: 0.6248059 Linear l2 grad loss: 17.915104627609253\n",
                        "linear l2 loss: 0.62436265 Linear l2 grad loss: 17.925644874572754\n",
                        "linear l2 loss: 0.6239206 Linear l2 grad loss: 17.9362576007843\n",
                        "linear l2 loss: 0.6234788 Linear l2 grad loss: 17.946830987930298\n",
                        "linear l2 loss: 0.62303776 Linear l2 grad loss: 17.95745325088501\n",
                        "linear l2 loss: 0.6225961 Linear l2 grad loss: 17.967944860458374\n",
                        "linear l2 loss: 0.62215525 Linear l2 grad loss: 17.978452444076538\n",
                        "linear l2 loss: 0.621715 Linear l2 grad loss: 17.988967180252075\n",
                        "linear l2 loss: 0.6212747 Linear l2 grad loss: 17.99941849708557\n",
                        "linear l2 loss: 0.6208351 Linear l2 grad loss: 18.009876012802124\n",
                        "linear l2 loss: 0.6203951 Linear l2 grad loss: 18.020246982574463\n",
                        "linear l2 loss: 0.61995703 Linear l2 grad loss: 18.030752420425415\n",
                        "linear l2 loss: 0.61951894 Linear l2 grad loss: 18.041206121444702\n",
                        "linear l2 loss: 0.6190805 Linear l2 grad loss: 18.051570653915405\n",
                        "linear l2 loss: 0.6186427 Linear l2 grad loss: 18.06191086769104\n",
                        "linear l2 loss: 0.61820537 Linear l2 grad loss: 18.072280406951904\n",
                        "linear l2 loss: 0.6177683 Linear l2 grad loss: 18.082618474960327\n",
                        "linear l2 loss: 0.61733145 Linear l2 grad loss: 18.09290361404419\n",
                        "linear l2 loss: 0.6168947 Linear l2 grad loss: 18.1031551361084\n",
                        "linear l2 loss: 0.61646 Linear l2 grad loss: 18.113563776016235\n",
                        "linear l2 loss: 0.6160243 Linear l2 grad loss: 18.123794078826904\n",
                        "linear l2 loss: 0.6155894 Linear l2 grad loss: 18.134050130844116\n",
                        "linear l2 loss: 0.615154 Linear l2 grad loss: 18.144211292266846\n",
                        "linear l2 loss: 0.6147201 Linear l2 grad loss: 18.15447974205017\n",
                        "linear l2 loss: 0.61428535 Linear l2 grad loss: 18.16459035873413\n",
                        "linear l2 loss: 0.6138515 Linear l2 grad loss: 18.174745559692383\n",
                        "linear l2 loss: 0.6134183 Linear l2 grad loss: 18.18490982055664\n",
                        "linear l2 loss: 0.6129856 Linear l2 grad loss: 18.195075035095215\n",
                        "linear l2 loss: 0.61255306 Linear l2 grad loss: 18.20517897605896\n",
                        "linear l2 loss: 0.6121212 Linear l2 grad loss: 18.215306758880615\n",
                        "linear l2 loss: 0.61168903 Linear l2 grad loss: 18.225358963012695\n",
                        "linear l2 loss: 0.6112577 Linear l2 grad loss: 18.235429525375366\n",
                        "linear l2 loss: 0.61082715 Linear l2 grad loss: 18.24553656578064\n",
                        "linear l2 loss: 0.6103963 Linear l2 grad loss: 18.255568742752075\n",
                        "linear l2 loss: 0.6099657 Linear l2 grad loss: 18.2655348777771\n",
                        "linear l2 loss: 0.60953593 Linear l2 grad loss: 18.27556037902832\n",
                        "linear l2 loss: 0.6091067 Linear l2 grad loss: 18.28556776046753\n",
                        "linear l2 loss: 0.608677 Linear l2 grad loss: 18.295485496520996\n",
                        "linear l2 loss: 0.60824865 Linear l2 grad loss: 18.305492877960205\n",
                        "linear l2 loss: 0.6078195 Linear l2 grad loss: 18.315359830856323\n",
                        "linear l2 loss: 0.607392 Linear l2 grad loss: 18.325319290161133\n",
                        "linear l2 loss: 0.6069635 Linear l2 grad loss: 18.335153341293335\n",
                        "linear l2 loss: 0.60653603 Linear l2 grad loss: 18.345012426376343\n",
                        "linear l2 loss: 0.6061086 Linear l2 grad loss: 18.3548481464386\n",
                        "linear l2 loss: 0.6056823 Linear l2 grad loss: 18.36471962928772\n",
                        "linear l2 loss: 0.605256 Linear l2 grad loss: 18.37453293800354\n",
                        "linear l2 loss: 0.60482925 Linear l2 grad loss: 18.3842670917511\n",
                        "linear l2 loss: 0.60440433 Linear l2 grad loss: 18.394139766693115\n",
                        "linear l2 loss: 0.60397905 Linear l2 grad loss: 18.403897285461426\n",
                        "linear l2 loss: 0.6035534 Linear l2 grad loss: 18.413590669631958\n",
                        "linear l2 loss: 0.6031294 Linear l2 grad loss: 18.423386335372925\n",
                        "linear l2 loss: 0.6027045 Linear l2 grad loss: 18.433032989501953\n",
                        "linear l2 loss: 0.602281 Linear l2 grad loss: 18.44276475906372\n",
                        "linear l2 loss: 0.60185796 Linear l2 grad loss: 18.452502012252808\n",
                        "linear l2 loss: 0.6014347 Linear l2 grad loss: 18.462159156799316\n",
                        "linear l2 loss: 0.6010123 Linear l2 grad loss: 18.471837043762207\n",
                        "linear l2 loss: 0.6005902 Linear l2 grad loss: 18.481500387191772\n",
                        "linear l2 loss: 0.6001683 Linear l2 grad loss: 18.491137981414795\n",
                        "linear l2 loss: 0.5997462 Linear l2 grad loss: 18.500720977783203\n",
                        "linear l2 loss: 0.5993249 Linear l2 grad loss: 18.510286569595337\n",
                        "linear l2 loss: 0.5989036 Linear l2 grad loss: 18.51984405517578\n",
                        "linear l2 loss: 0.59848326 Linear l2 grad loss: 18.529427528381348\n",
                        "linear l2 loss: 0.5980639 Linear l2 grad loss: 18.539043426513672\n",
                        "linear l2 loss: 0.59764344 Linear l2 grad loss: 18.548509120941162\n",
                        "linear l2 loss: 0.597223 Linear l2 grad loss: 18.557954788208008\n",
                        "linear l2 loss: 0.5968039 Linear l2 grad loss: 18.567445755004883\n",
                        "linear l2 loss: 0.5963845 Linear l2 grad loss: 18.57687520980835\n",
                        "linear l2 loss: 0.59596694 Linear l2 grad loss: 18.586423635482788\n",
                        "linear l2 loss: 0.5955481 Linear l2 grad loss: 18.59577178955078\n",
                        "linear l2 loss: 0.59512997 Linear l2 grad loss: 18.60518217086792\n",
                        "linear l2 loss: 0.5947129 Linear l2 grad loss: 18.614622116088867\n",
                        "linear l2 loss: 0.5942968 Linear l2 grad loss: 18.624111890792847\n",
                        "linear l2 loss: 0.59387875 Linear l2 grad loss: 18.633384943008423\n",
                        "linear l2 loss: 0.59346265 Linear l2 grad loss: 18.642759561538696\n",
                        "linear l2 loss: 0.59304714 Linear l2 grad loss: 18.652164459228516\n",
                        "linear l2 loss: 0.5926308 Linear l2 grad loss: 18.661426782608032\n",
                        "linear l2 loss: 0.592216 Linear l2 grad loss: 18.670810222625732\n",
                        "linear l2 loss: 0.5918001 Linear l2 grad loss: 18.679982662200928\n",
                        "linear l2 loss: 0.5913863 Linear l2 grad loss: 18.689374685287476\n",
                        "linear l2 loss: 0.59097207 Linear l2 grad loss: 18.698625326156616\n",
                        "linear l2 loss: 0.5905574 Linear l2 grad loss: 18.707823276519775\n",
                        "linear l2 loss: 0.59014356 Linear l2 grad loss: 18.717043161392212\n",
                        "linear l2 loss: 0.5897307 Linear l2 grad loss: 18.726288080215454\n",
                        "linear l2 loss: 0.5893182 Linear l2 grad loss: 18.73555612564087\n",
                        "linear l2 loss: 0.588906 Linear l2 grad loss: 18.744757890701294\n",
                        "linear l2 loss: 0.5884933 Linear l2 grad loss: 18.753905057907104\n",
                        "linear l2 loss: 0.58808124 Linear l2 grad loss: 18.763023138046265\n",
                        "linear l2 loss: 0.58767 Linear l2 grad loss: 18.772213220596313\n",
                        "linear l2 loss: 0.58725834 Linear l2 grad loss: 18.7812979221344\n",
                        "linear l2 loss: 0.5868471 Linear l2 grad loss: 18.79035973548889\n",
                        "linear l2 loss: 0.58643687 Linear l2 grad loss: 18.79949402809143\n",
                        "linear l2 loss: 0.58602756 Linear l2 grad loss: 18.808639526367188\n",
                        "linear l2 loss: 0.58561707 Linear l2 grad loss: 18.817654132843018\n",
                        "linear l2 loss: 0.58520764 Linear l2 grad loss: 18.826699018478394\n",
                        "linear l2 loss: 0.5847974 Linear l2 grad loss: 18.835644721984863\n",
                        "linear l2 loss: 0.58438945 Linear l2 grad loss: 18.84474229812622\n",
                        "linear l2 loss: 0.58398074 Linear l2 grad loss: 18.853708028793335\n",
                        "linear l2 loss: 0.58357304 Linear l2 grad loss: 18.862756729125977\n",
                        "linear l2 loss: 0.5831655 Linear l2 grad loss: 18.871740579605103\n",
                        "linear l2 loss: 0.5827574 Linear l2 grad loss: 18.8806369304657\n",
                        "linear l2 loss: 0.5823502 Linear l2 grad loss: 18.889585733413696\n",
                        "linear l2 loss: 0.5819438 Linear l2 grad loss: 18.898534774780273\n",
                        "linear l2 loss: 0.58153766 Linear l2 grad loss: 18.907498836517334\n",
                        "linear l2 loss: 0.58113116 Linear l2 grad loss: 18.916350603103638\n",
                        "linear l2 loss: 0.5807263 Linear l2 grad loss: 18.925310373306274\n",
                        "linear l2 loss: 0.5803209 Linear l2 grad loss: 18.934178829193115\n",
                        "linear l2 loss: 0.5799152 Linear l2 grad loss: 18.9429771900177\n",
                        "linear l2 loss: 0.57950974 Linear l2 grad loss: 18.95175337791443\n",
                        "linear l2 loss: 0.57910544 Linear l2 grad loss: 18.96059274673462\n",
                        "linear l2 loss: 0.5787024 Linear l2 grad loss: 18.969497442245483\n",
                        "linear l2 loss: 0.57829833 Linear l2 grad loss: 18.978256702423096\n",
                        "linear l2 loss: 0.57789505 Linear l2 grad loss: 18.987045526504517\n",
                        "linear l2 loss: 0.5774923 Linear l2 grad loss: 18.995849132537842\n",
                        "linear l2 loss: 0.57708967 Linear l2 grad loss: 19.00461721420288\n",
                        "linear l2 loss: 0.57668716 Linear l2 grad loss: 19.01334547996521\n",
                        "linear l2 loss: 0.57628506 Linear l2 grad loss: 19.02205204963684\n",
                        "linear l2 loss: 0.57588357 Linear l2 grad loss: 19.030778169631958\n",
                        "linear l2 loss: 0.5754824 Linear l2 grad loss: 19.039502382278442\n",
                        "linear l2 loss: 0.5750807 Linear l2 grad loss: 19.048110485076904\n",
                        "linear l2 loss: 0.5746805 Linear l2 grad loss: 19.056824207305908\n",
                        "linear l2 loss: 0.5742797 Linear l2 grad loss: 19.065431594848633\n",
                        "linear l2 loss: 0.57387996 Linear l2 grad loss: 19.0741183757782\n",
                        "linear l2 loss: 0.5734803 Linear l2 grad loss: 19.082749843597412\n",
                        "linear l2 loss: 0.5730811 Linear l2 grad loss: 19.091362714767456\n",
                        "linear l2 loss: 0.57268184 Linear l2 grad loss: 19.09994649887085\n",
                        "linear l2 loss: 0.5722834 Linear l2 grad loss: 19.10855793952942\n",
                        "linear l2 loss: 0.5718846 Linear l2 grad loss: 19.11706805229187\n",
                        "linear l2 loss: 0.5714878 Linear l2 grad loss: 19.12574553489685\n",
                        "linear l2 loss: 0.5710895 Linear l2 grad loss: 19.13423752784729\n",
                        "linear l2 loss: 0.57069105 Linear l2 grad loss: 19.142685651779175\n",
                        "linear l2 loss: 0.57029486 Linear l2 grad loss: 19.15127682685852\n",
                        "linear l2 loss: 0.56989855 Linear l2 grad loss: 19.1597843170166\n",
                        "linear l2 loss: 0.56950206 Linear l2 grad loss: 19.168290853500366\n",
                        "linear l2 loss: 0.569106 Linear l2 grad loss: 19.176740407943726\n",
                        "linear l2 loss: 0.5687105 Linear l2 grad loss: 19.185227155685425\n",
                        "linear l2 loss: 0.5683147 Linear l2 grad loss: 19.193626403808594\n",
                        "linear l2 loss: 0.56791943 Linear l2 grad loss: 19.20203995704651\n",
                        "linear l2 loss: 0.56752485 Linear l2 grad loss: 19.210479974746704\n",
                        "linear l2 loss: 0.567132 Linear l2 grad loss: 19.21901774406433\n",
                        "linear l2 loss: 0.566737 Linear l2 grad loss: 19.227339506149292\n",
                        "linear l2 loss: 0.5663435 Linear l2 grad loss: 19.235715866088867\n",
                        "linear l2 loss: 0.56595033 Linear l2 grad loss: 19.2441143989563\n",
                        "linear l2 loss: 0.5655571 Linear l2 grad loss: 19.252460718154907\n",
                        "linear l2 loss: 0.56516474 Linear l2 grad loss: 19.260822296142578\n",
                        "linear l2 loss: 0.56477207 Linear l2 grad loss: 19.269142150878906\n",
                        "linear l2 loss: 0.5643801 Linear l2 grad loss: 19.277451753616333\n",
                        "linear l2 loss: 0.56398827 Linear l2 grad loss: 19.285749912261963\n",
                        "linear l2 loss: 0.5635965 Linear l2 grad loss: 19.29402756690979\n",
                        "linear l2 loss: 0.5632057 Linear l2 grad loss: 19.302324056625366\n",
                        "linear l2 loss: 0.5628154 Linear l2 grad loss: 19.31063938140869\n",
                        "linear l2 loss: 0.5624253 Linear l2 grad loss: 19.318889141082764\n",
                        "linear l2 loss: 0.5620345 Linear l2 grad loss: 19.327073097229004\n",
                        "linear l2 loss: 0.5616452 Linear l2 grad loss: 19.33533239364624\n",
                        "linear l2 loss: 0.5612553 Linear l2 grad loss: 19.34350562095642\n",
                        "linear l2 loss: 0.56086725 Linear l2 grad loss: 19.351823806762695\n",
                        "linear l2 loss: 0.5604779 Linear l2 grad loss: 19.359928607940674\n",
                        "linear l2 loss: 0.56008893 Linear l2 grad loss: 19.368078231811523\n",
                        "linear l2 loss: 0.5597012 Linear l2 grad loss: 19.376270532608032\n",
                        "linear l2 loss: 0.5593141 Linear l2 grad loss: 19.384496212005615\n",
                        "linear l2 loss: 0.5589264 Linear l2 grad loss: 19.39260196685791\n",
                        "linear l2 loss: 0.55853957 Linear l2 grad loss: 19.40074586868286\n",
                        "linear l2 loss: 0.5581526 Linear l2 grad loss: 19.408864498138428\n",
                        "linear l2 loss: 0.5577659 Linear l2 grad loss: 19.41692328453064\n",
                        "linear l2 loss: 0.55738074 Linear l2 grad loss: 19.425123691558838\n",
                        "linear l2 loss: 0.5569942 Linear l2 grad loss: 19.4331271648407\n",
                        "linear l2 loss: 0.5566091 Linear l2 grad loss: 19.441235303878784\n",
                        "linear l2 loss: 0.55622345 Linear l2 grad loss: 19.449251890182495\n",
                        "linear l2 loss: 0.55583805 Linear l2 grad loss: 19.457236289978027\n",
                        "linear l2 loss: 0.55545455 Linear l2 grad loss: 19.46536135673523\n",
                        "linear l2 loss: 0.5550705 Linear l2 grad loss: 19.47339653968811\n",
                        "linear l2 loss: 0.55468684 Linear l2 grad loss: 19.4814236164093\n",
                        "linear l2 loss: 0.55430347 Linear l2 grad loss: 19.489418506622314\n",
                        "linear l2 loss: 0.55392003 Linear l2 grad loss: 19.497381925582886\n",
                        "linear l2 loss: 0.5535366 Linear l2 grad loss: 19.505303621292114\n",
                        "linear l2 loss: 0.55315506 Linear l2 grad loss: 19.51336407661438\n",
                        "linear l2 loss: 0.5527725 Linear l2 grad loss: 19.521297693252563\n",
                        "linear l2 loss: 0.55239075 Linear l2 grad loss: 19.529225826263428\n",
                        "linear l2 loss: 0.55200857 Linear l2 grad loss: 19.537098169326782\n",
                        "linear l2 loss: 0.55162793 Linear l2 grad loss: 19.54508352279663\n",
                        "linear l2 loss: 0.55124736 Linear l2 grad loss: 19.553016662597656\n",
                        "linear l2 loss: 0.55086654 Linear l2 grad loss: 19.56088948249817\n",
                        "linear l2 loss: 0.5504858 Linear l2 grad loss: 19.568729400634766\n",
                        "linear l2 loss: 0.550106 Linear l2 grad loss: 19.576605081558228\n",
                        "linear l2 loss: 0.5497266 Linear l2 grad loss: 19.584474802017212\n",
                        "linear l2 loss: 0.5493475 Linear l2 grad loss: 19.592350482940674\n",
                        "linear l2 loss: 0.5489688 Linear l2 grad loss: 19.60020160675049\n",
                        "linear l2 loss: 0.5485899 Linear l2 grad loss: 19.608004570007324\n",
                        "linear l2 loss: 0.5482114 Linear l2 grad loss: 19.61581587791443\n",
                        "linear l2 loss: 0.54783374 Linear l2 grad loss: 19.62364959716797\n",
                        "linear l2 loss: 0.54745644 Linear l2 grad loss: 19.63150143623352\n",
                        "linear l2 loss: 0.5470785 Linear l2 grad loss: 19.639201879501343\n",
                        "linear l2 loss: 0.54670143 Linear l2 grad loss: 19.646960496902466\n",
                        "linear l2 loss: 0.5463254 Linear l2 grad loss: 19.6548171043396\n",
                        "linear l2 loss: 0.5459493 Linear l2 grad loss: 19.662543058395386\n",
                        "linear l2 loss: 0.5455736 Linear l2 grad loss: 19.670337915420532\n",
                        "linear l2 loss: 0.5451964 Linear l2 grad loss: 19.677932739257812\n",
                        "linear l2 loss: 0.5448222 Linear l2 grad loss: 19.68578314781189\n",
                        "linear l2 loss: 0.5444471 Linear l2 grad loss: 19.69348692893982\n",
                        "linear l2 loss: 0.54407233 Linear l2 grad loss: 19.701155424118042\n",
                        "linear l2 loss: 0.5436987 Linear l2 grad loss: 19.708922147750854\n",
                        "linear l2 loss: 0.5433242 Linear l2 grad loss: 19.716551780700684\n",
                        "linear l2 loss: 0.54295063 Linear l2 grad loss: 19.724244356155396\n",
                        "linear l2 loss: 0.54257673 Linear l2 grad loss: 19.731860876083374\n",
                        "linear l2 loss: 0.54220396 Linear l2 grad loss: 19.739531755447388\n",
                        "linear l2 loss: 0.5418314 Linear l2 grad loss: 19.7471981048584\n",
                        "linear l2 loss: 0.5414582 Linear l2 grad loss: 19.754743099212646\n",
                        "linear l2 loss: 0.54108685 Linear l2 grad loss: 19.76244616508484\n",
                        "linear l2 loss: 0.54071516 Linear l2 grad loss: 19.77005410194397\n",
                        "linear l2 loss: 0.54034406 Linear l2 grad loss: 19.77769446372986\n",
                        "linear l2 loss: 0.5399727 Linear l2 grad loss: 19.785274267196655\n",
                        "linear l2 loss: 0.53960216 Linear l2 grad loss: 19.792867422103882\n",
                        "linear l2 loss: 0.53923106 Linear l2 grad loss: 19.800400495529175\n",
                        "linear l2 loss: 0.53886205 Linear l2 grad loss: 19.808037996292114\n",
                        "linear l2 loss: 0.53849125 Linear l2 grad loss: 19.815518379211426\n",
                        "linear l2 loss: 0.5381227 Linear l2 grad loss: 19.82314705848694\n",
                        "linear l2 loss: 0.53775316 Linear l2 grad loss: 19.830660343170166\n",
                        "linear l2 loss: 0.537384 Linear l2 grad loss: 19.8381404876709\n",
                        "linear l2 loss: 0.5370153 Linear l2 grad loss: 19.8456449508667\n",
                        "linear l2 loss: 0.53664714 Linear l2 grad loss: 19.853161811828613\n",
                        "linear l2 loss: 0.5362797 Linear l2 grad loss: 19.860698461532593\n",
                        "linear l2 loss: 0.53591245 Linear l2 grad loss: 19.868215560913086\n",
                        "linear l2 loss: 0.53554565 Linear l2 grad loss: 19.875714540481567\n",
                        "linear l2 loss: 0.5351784 Linear l2 grad loss: 19.883166313171387\n",
                        "linear l2 loss: 0.5348115 Linear l2 grad loss: 19.890599966049194\n",
                        "linear l2 loss: 0.5344461 Linear l2 grad loss: 19.898128032684326\n",
                        "linear l2 loss: 0.53408045 Linear l2 grad loss: 19.90557837486267\n",
                        "linear l2 loss: 0.53371465 Linear l2 grad loss: 19.912999629974365\n",
                        "linear l2 loss: 0.5333498 Linear l2 grad loss: 19.92046570777893\n",
                        "linear l2 loss: 0.53298444 Linear l2 grad loss: 19.927842617034912\n",
                        "linear l2 loss: 0.5326204 Linear l2 grad loss: 19.935307025909424\n",
                        "linear l2 loss: 0.53225654 Linear l2 grad loss: 19.942742109298706\n",
                        "linear l2 loss: 0.53189224 Linear l2 grad loss: 19.95011615753174\n",
                        "linear l2 loss: 0.5315284 Linear l2 grad loss: 19.957469940185547\n",
                        "linear l2 loss: 0.5311662 Linear l2 grad loss: 19.964940547943115\n",
                        "linear l2 loss: 0.5308034 Linear l2 grad loss: 19.97232985496521\n",
                        "linear l2 loss: 0.5304403 Linear l2 grad loss: 19.979647397994995\n",
                        "linear l2 loss: 0.5300784 Linear l2 grad loss: 19.987051248550415\n",
                        "linear l2 loss: 0.5297157 Linear l2 grad loss: 19.994333028793335\n",
                        "linear l2 loss: 0.5293544 Linear l2 grad loss: 20.001694202423096\n",
                        "linear l2 loss: 0.5289936 Linear l2 grad loss: 20.009068965911865\n",
                        "linear l2 loss: 0.52863246 Linear l2 grad loss: 20.016388654708862\n",
                        "linear l2 loss: 0.5282721 Linear l2 grad loss: 20.02371859550476\n",
                        "linear l2 loss: 0.5279118 Linear l2 grad loss: 20.03104019165039\n",
                        "linear l2 loss: 0.5275517 Linear l2 grad loss: 20.03833317756653\n",
                        "linear l2 loss: 0.5271924 Linear l2 grad loss: 20.045650720596313\n",
                        "linear l2 loss: 0.5268329 Linear l2 grad loss: 20.052923679351807\n",
                        "linear l2 loss: 0.5264749 Linear l2 grad loss: 20.060296058654785\n",
                        "linear l2 loss: 0.5261163 Linear l2 grad loss: 20.06757378578186\n",
                        "linear l2 loss: 0.52575815 Linear l2 grad loss: 20.074859380722046\n",
                        "linear l2 loss: 0.5254006 Linear l2 grad loss: 20.082147359848022\n",
                        "linear l2 loss: 0.52504295 Linear l2 grad loss: 20.089407920837402\n",
                        "linear l2 loss: 0.5246855 Linear l2 grad loss: 20.09663724899292\n",
                        "linear l2 loss: 0.52432865 Linear l2 grad loss: 20.103895902633667\n",
                        "linear l2 loss: 0.52397245 Linear l2 grad loss: 20.11116671562195\n",
                        "linear l2 loss: 0.52361614 Linear l2 grad loss: 20.118396997451782\n",
                        "linear l2 loss: 0.52326053 Linear l2 grad loss: 20.125643730163574\n",
                        "linear l2 loss: 0.52290505 Linear l2 grad loss: 20.13287663459778\n",
                        "linear l2 loss: 0.5225492 Linear l2 grad loss: 20.140047073364258\n",
                        "linear l2 loss: 0.5221949 Linear l2 grad loss: 20.147291660308838\n",
                        "linear l2 loss: 0.5218409 Linear l2 grad loss: 20.154552221298218\n",
                        "linear l2 loss: 0.52148646 Linear l2 grad loss: 20.161728382110596\n",
                        "linear l2 loss: 0.5211336 Linear l2 grad loss: 20.169005393981934\n",
                        "linear l2 loss: 0.5207791 Linear l2 grad loss: 20.17611289024353\n",
                        "linear l2 loss: 0.52042735 Linear l2 grad loss: 20.183424472808838\n",
                        "linear l2 loss: 0.5200738 Linear l2 grad loss: 20.190531730651855\n",
                        "linear l2 loss: 0.5197224 Linear l2 grad loss: 20.197805643081665\n",
                        "linear l2 loss: 0.51937026 Linear l2 grad loss: 20.204969882965088\n",
                        "linear l2 loss: 0.51901853 Linear l2 grad loss: 20.212154388427734\n",
                        "linear l2 loss: 0.5186666 Linear l2 grad loss: 20.219265460968018\n",
                        "linear l2 loss: 0.5183158 Linear l2 grad loss: 20.226460695266724\n",
                        "linear l2 loss: 0.5179652 Linear l2 grad loss: 20.23361587524414\n",
                        "linear l2 loss: 0.5176148 Linear l2 grad loss: 20.240763187408447\n",
                        "linear l2 loss: 0.51726526 Linear l2 grad loss: 20.247947692871094\n",
                        "linear l2 loss: 0.5169165 Linear l2 grad loss: 20.25518798828125\n",
                        "linear l2 loss: 0.51656693 Linear l2 grad loss: 20.26229166984558\n",
                        "linear l2 loss: 0.5162181 Linear l2 grad loss: 20.269471645355225\n",
                        "linear l2 loss: 0.5158698 Linear l2 grad loss: 20.276612281799316\n",
                        "linear l2 loss: 0.51552147 Linear l2 grad loss: 20.28371787071228\n",
                        "linear l2 loss: 0.51517385 Linear l2 grad loss: 20.29087209701538\n",
                        "linear l2 loss: 0.51482683 Linear l2 grad loss: 20.29806113243103\n",
                        "linear l2 loss: 0.51447976 Linear l2 grad loss: 20.305193662643433\n",
                        "linear l2 loss: 0.51413363 Linear l2 grad loss: 20.312358617782593\n",
                        "linear l2 loss: 0.51378787 Linear l2 grad loss: 20.319526195526123\n",
                        "linear l2 loss: 0.51344067 Linear l2 grad loss: 20.32656455039978\n",
                        "linear l2 loss: 0.51309574 Linear l2 grad loss: 20.33372712135315\n",
                        "linear l2 loss: 0.51275057 Linear l2 grad loss: 20.340879201889038\n",
                        "linear l2 loss: 0.51240534 Linear l2 grad loss: 20.347960233688354\n",
                        "linear l2 loss: 0.5120612 Linear l2 grad loss: 20.35512065887451\n",
                        "linear l2 loss: 0.51171774 Linear l2 grad loss: 20.362285375595093\n",
                        "linear l2 loss: 0.51137346 Linear l2 grad loss: 20.36936640739441\n",
                        "linear l2 loss: 0.5110303 Linear l2 grad loss: 20.37651538848877\n",
                        "linear l2 loss: 0.5106876 Linear l2 grad loss: 20.38364815711975\n",
                        "linear l2 loss: 0.5103445 Linear l2 grad loss: 20.390730142593384\n",
                        "linear l2 loss: 0.51000255 Linear l2 grad loss: 20.39788317680359\n",
                        "linear l2 loss: 0.50966114 Linear l2 grad loss: 20.405035495758057\n",
                        "linear l2 loss: 0.50932074 Linear l2 grad loss: 20.412249326705933\n",
                        "linear l2 loss: 0.50897944 Linear l2 grad loss: 20.419350624084473\n",
                        "linear l2 loss: 0.50863785 Linear l2 grad loss: 20.426419973373413\n",
                        "linear l2 loss: 0.5082989 Linear l2 grad loss: 20.43363046646118\n",
                        "linear l2 loss: 0.50795865 Linear l2 grad loss: 20.440746545791626\n",
                        "linear l2 loss: 0.5076188 Linear l2 grad loss: 20.447835445404053\n",
                        "linear l2 loss: 0.50728 Linear l2 grad loss: 20.45499539375305\n",
                        "linear l2 loss: 0.5069417 Linear l2 grad loss: 20.46217393875122\n",
                        "linear l2 loss: 0.5066032 Linear l2 grad loss: 20.469289541244507\n",
                        "linear l2 loss: 0.5062653 Linear l2 grad loss: 20.476442098617554\n",
                        "linear l2 loss: 0.5059287 Linear l2 grad loss: 20.483649253845215\n",
                        "linear l2 loss: 0.50559133 Linear l2 grad loss: 20.49076819419861\n",
                        "linear l2 loss: 0.5052553 Linear l2 grad loss: 20.497981309890747\n",
                        "linear l2 loss: 0.5049196 Linear l2 grad loss: 20.505181074142456\n",
                        "linear l2 loss: 0.50458336 Linear l2 grad loss: 20.512333154678345\n",
                        "linear l2 loss: 0.50424767 Linear l2 grad loss: 20.519479513168335\n",
                        "linear l2 loss: 0.50391334 Linear l2 grad loss: 20.5267014503479\n",
                        "linear l2 loss: 0.50357944 Linear l2 grad loss: 20.533944129943848\n",
                        "linear l2 loss: 0.50324416 Linear l2 grad loss: 20.541017532348633\n",
                        "linear l2 loss: 0.50291127 Linear l2 grad loss: 20.548301935195923\n",
                        "linear l2 loss: 0.5025782 Linear l2 grad loss: 20.55550527572632\n",
                        "linear l2 loss: 0.50224584 Linear l2 grad loss: 20.562745809555054\n",
                        "linear l2 loss: 0.50191385 Linear l2 grad loss: 20.570003271102905\n",
                        "linear l2 loss: 0.50158185 Linear l2 grad loss: 20.57723116874695\n",
                        "linear l2 loss: 0.50124997 Linear l2 grad loss: 20.584428787231445\n",
                        "linear l2 loss: 0.500919 Linear l2 grad loss: 20.591668844223022\n",
                        "linear l2 loss: 0.50058895 Linear l2 grad loss: 20.598944664001465\n",
                        "linear l2 loss: 0.50025886 Linear l2 grad loss: 20.606245517730713\n",
                        "linear l2 loss: 0.49992988 Linear l2 grad loss: 20.6135516166687\n",
                        "linear l2 loss: 0.49960032 Linear l2 grad loss: 20.62080669403076\n",
                        "linear l2 loss: 0.49927145 Linear l2 grad loss: 20.628113508224487\n",
                        "linear l2 loss: 0.49894342 Linear l2 grad loss: 20.63541316986084\n",
                        "linear l2 loss: 0.49861643 Linear l2 grad loss: 20.64280939102173\n",
                        "linear l2 loss: 0.49828896 Linear l2 grad loss: 20.650135278701782\n",
                        "linear l2 loss: 0.4979622 Linear l2 grad loss: 20.657472610473633\n",
                        "linear l2 loss: 0.4976363 Linear l2 grad loss: 20.664865493774414\n",
                        "linear l2 loss: 0.49731022 Linear l2 grad loss: 20.672220706939697\n",
                        "linear l2 loss: 0.49698582 Linear l2 grad loss: 20.679672241210938\n",
                        "linear l2 loss: 0.49666014 Linear l2 grad loss: 20.687028646469116\n",
                        "linear l2 loss: 0.49633607 Linear l2 grad loss: 20.69446110725403\n",
                        "linear l2 loss: 0.49601302 Linear l2 grad loss: 20.70195698738098\n",
                        "linear l2 loss: 0.49569017 Linear l2 grad loss: 20.70945692062378\n",
                        "linear l2 loss: 0.49536762 Linear l2 grad loss: 20.716950178146362\n",
                        "linear l2 loss: 0.49504533 Linear l2 grad loss: 20.724428415298462\n",
                        "linear l2 loss: 0.49472362 Linear l2 grad loss: 20.731922149658203\n",
                        "linear l2 loss: 0.49440286 Linear l2 grad loss: 20.739495515823364\n",
                        "linear l2 loss: 0.4940829 Linear l2 grad loss: 20.747106075286865\n",
                        "linear l2 loss: 0.4937627 Linear l2 grad loss: 20.7546603679657\n",
                        "linear l2 loss: 0.49344411 Linear l2 grad loss: 20.762322664260864\n",
                        "linear l2 loss: 0.49312535 Linear l2 grad loss: 20.769954681396484\n",
                        "linear l2 loss: 0.49280617 Linear l2 grad loss: 20.777535915374756\n",
                        "linear l2 loss: 0.4924903 Linear l2 grad loss: 20.785341262817383\n",
                        "linear l2 loss: 0.49217257 Linear l2 grad loss: 20.79300022125244\n",
                        "linear l2 loss: 0.49185655 Linear l2 grad loss: 20.800766468048096\n",
                        "linear l2 loss: 0.49154106 Linear l2 grad loss: 20.80854821205139\n",
                        "linear l2 loss: 0.49122545 Linear l2 grad loss: 20.816316604614258\n",
                        "linear l2 loss: 0.49091163 Linear l2 grad loss: 20.824181079864502\n",
                        "linear l2 loss: 0.49059764 Linear l2 grad loss: 20.832041025161743\n",
                        "linear l2 loss: 0.49028516 Linear l2 grad loss: 20.839975357055664\n",
                        "linear l2 loss: 0.48997188 Linear l2 grad loss: 20.847839832305908\n",
                        "linear l2 loss: 0.4896597 Linear l2 grad loss: 20.855770111083984\n",
                        "linear l2 loss: 0.48935005 Linear l2 grad loss: 20.863908052444458\n",
                        "linear l2 loss: 0.4890401 Linear l2 grad loss: 20.871957302093506\n",
                        "linear l2 loss: 0.48872957 Linear l2 grad loss: 20.879977226257324\n",
                        "linear l2 loss: 0.48842087 Linear l2 grad loss: 20.888118028640747\n",
                        "linear l2 loss: 0.48811275 Linear l2 grad loss: 20.896283388137817\n",
                        "linear l2 loss: 0.48780558 Linear l2 grad loss: 20.904500246047974\n",
                        "linear l2 loss: 0.48749852 Linear l2 grad loss: 20.912734270095825\n",
                        "linear l2 loss: 0.4871926 Linear l2 grad loss: 20.921021699905396\n",
                        "linear l2 loss: 0.4868873 Linear l2 grad loss: 20.929365396499634\n",
                        "linear l2 loss: 0.48658282 Linear l2 grad loss: 20.93775749206543\n",
                        "linear l2 loss: 0.486279 Linear l2 grad loss: 20.946175575256348\n",
                        "linear l2 loss: 0.48597738 Linear l2 grad loss: 20.954759120941162\n",
                        "linear l2 loss: 0.485675 Linear l2 grad loss: 20.96327543258667\n",
                        "linear l2 loss: 0.48537442 Linear l2 grad loss: 20.971908807754517\n",
                        "linear l2 loss: 0.48507473 Linear l2 grad loss: 20.980607509613037\n",
                        "linear l2 loss: 0.48477513 Linear l2 grad loss: 20.989316701889038\n",
                        "linear l2 loss: 0.48447677 Linear l2 grad loss: 20.99808669090271\n",
                        "linear l2 loss: 0.48417902 Linear l2 grad loss: 21.00690507888794\n",
                        "linear l2 loss: 0.48388314 Linear l2 grad loss: 21.01587414741516\n",
                        "linear l2 loss: 0.4835875 Linear l2 grad loss: 21.024861097335815\n",
                        "linear l2 loss: 0.48329306 Linear l2 grad loss: 21.033931493759155\n",
                        "linear l2 loss: 0.48299915 Linear l2 grad loss: 21.043020963668823\n",
                        "linear l2 loss: 0.48270702 Linear l2 grad loss: 21.052260398864746\n",
                        "linear l2 loss: 0.48241577 Linear l2 grad loss: 21.0615553855896\n",
                        "linear l2 loss: 0.48212573 Linear l2 grad loss: 21.070939540863037\n",
                        "linear l2 loss: 0.481837 Linear l2 grad loss: 21.080429553985596\n",
                        "linear l2 loss: 0.48154902 Linear l2 grad loss: 21.089977502822876\n",
                        "linear l2 loss: 0.48126268 Linear l2 grad loss: 21.099647283554077\n",
                        "linear l2 loss: 0.48097643 Linear l2 grad loss: 21.109337329864502\n",
                        "linear l2 loss: 0.48069188 Linear l2 grad loss: 21.119139432907104\n",
                        "linear l2 loss: 0.48040915 Linear l2 grad loss: 21.129090070724487\n",
                        "linear l2 loss: 0.48012695 Linear l2 grad loss: 21.139087915420532\n",
                        "linear l2 loss: 0.47984582 Linear l2 grad loss: 21.149185180664062\n",
                        "linear l2 loss: 0.4795672 Linear l2 grad loss: 21.159464359283447\n",
                        "linear l2 loss: 0.4792891 Linear l2 grad loss: 21.1697998046875\n",
                        "linear l2 loss: 0.4790125 Linear l2 grad loss: 21.180270433425903\n",
                        "linear l2 loss: 0.47873774 Linear l2 grad loss: 21.19088077545166\n",
                        "linear l2 loss: 0.47846332 Linear l2 grad loss: 21.201521158218384\n",
                        "linear l2 loss: 0.47819135 Linear l2 grad loss: 21.21236824989319\n",
                        "linear l2 loss: 0.47792172 Linear l2 grad loss: 21.22342538833618\n",
                        "linear l2 loss: 0.4776523 Linear l2 grad loss: 21.234480142593384\n",
                        "linear l2 loss: 0.47738495 Linear l2 grad loss: 21.245734691619873\n",
                        "linear l2 loss: 0.47711954 Linear l2 grad loss: 21.257131099700928\n",
                        "linear l2 loss: 0.47685504 Linear l2 grad loss: 21.26861333847046\n",
                        "linear l2 loss: 0.47659326 Linear l2 grad loss: 21.280356645584106\n",
                        "linear l2 loss: 0.4763328 Linear l2 grad loss: 21.29220676422119\n",
                        "linear l2 loss: 0.47607422 Linear l2 grad loss: 21.30420422554016\n",
                        "linear l2 loss: 0.4758175 Linear l2 grad loss: 21.316396474838257\n",
                        "linear l2 loss: 0.47556257 Linear l2 grad loss: 21.328733444213867\n",
                        "linear l2 loss: 0.4753099 Linear l2 grad loss: 21.341247081756592\n",
                        "linear l2 loss: 0.4750598 Linear l2 grad loss: 21.35402274131775\n",
                        "linear l2 loss: 0.47481033 Linear l2 grad loss: 21.366846323013306\n",
                        "linear l2 loss: 0.47456366 Linear l2 grad loss: 21.379920721054077\n",
                        "linear l2 loss: 0.47431993 Linear l2 grad loss: 21.393245697021484\n",
                        "linear l2 loss: 0.4740776 Linear l2 grad loss: 21.40670371055603\n",
                        "linear l2 loss: 0.4738373 Linear l2 grad loss: 21.420357704162598\n",
                        "linear l2 loss: 0.4736 Linear l2 grad loss: 21.434255599975586\n",
                        "linear l2 loss: 0.47336507 Linear l2 grad loss: 21.448365926742554\n",
                        "linear l2 loss: 0.47313187 Linear l2 grad loss: 21.462653160095215\n",
                        "linear l2 loss: 0.4729019 Linear l2 grad loss: 21.477222204208374\n",
                        "linear l2 loss: 0.47267467 Linear l2 grad loss: 21.492003679275513\n",
                        "linear l2 loss: 0.47245046 Linear l2 grad loss: 21.50707197189331\n",
                        "linear l2 loss: 0.4722279 Linear l2 grad loss: 21.522294282913208\n",
                        "linear l2 loss: 0.4720078 Linear l2 grad loss: 21.537726402282715\n",
                        "linear l2 loss: 0.4717915 Linear l2 grad loss: 21.55347490310669\n",
                        "linear l2 loss: 0.4715777 Linear l2 grad loss: 21.569448709487915\n",
                        "linear l2 loss: 0.47136718 Linear l2 grad loss: 21.585692167282104\n",
                        "linear l2 loss: 0.4711581 Linear l2 grad loss: 21.602068424224854\n",
                        "linear l2 loss: 0.47095385 Linear l2 grad loss: 21.61883807182312\n",
                        "linear l2 loss: 0.47075137 Linear l2 grad loss: 21.63575291633606\n",
                        "linear l2 loss: 0.47055206 Linear l2 grad loss: 21.652940034866333\n",
                        "linear l2 loss: 0.47035596 Linear l2 grad loss: 21.670365571975708\n",
                        "linear l2 loss: 0.4701639 Linear l2 grad loss: 21.688109397888184\n",
                        "linear l2 loss: 0.4699745 Linear l2 grad loss: 21.706076860427856\n",
                        "linear l2 loss: 0.4697884 Linear l2 grad loss: 21.724271059036255\n",
                        "linear l2 loss: 0.46960527 Linear l2 grad loss: 21.742716312408447\n",
                        "linear l2 loss: 0.46942493 Linear l2 grad loss: 21.761332750320435\n",
                        "linear l2 loss: 0.4692483 Linear l2 grad loss: 21.780221223831177\n",
                        "linear l2 loss: 0.46907496 Linear l2 grad loss: 21.799348831176758\n",
                        "linear l2 loss: 0.46890536 Linear l2 grad loss: 21.81871485710144\n",
                        "linear l2 loss: 0.46873775 Linear l2 grad loss: 21.838194131851196\n",
                        "linear l2 loss: 0.46857476 Linear l2 grad loss: 21.85799288749695\n",
                        "linear l2 loss: 0.4684138 Linear l2 grad loss: 21.877867698669434\n",
                        "linear l2 loss: 0.46825668 Linear l2 grad loss: 21.89797806739807\n",
                        "linear l2 loss: 0.46810302 Linear l2 grad loss: 21.918297290802002\n",
                        "linear l2 loss: 0.46795294 Linear l2 grad loss: 21.93877935409546\n",
                        "linear l2 loss: 0.46780396 Linear l2 grad loss: 21.959270000457764\n",
                        "linear l2 loss: 0.46766004 Linear l2 grad loss: 21.98003911972046\n",
                        "linear l2 loss: 0.46751815 Linear l2 grad loss: 22.000863313674927\n",
                        "linear l2 loss: 0.46738052 Linear l2 grad loss: 22.02185869216919\n",
                        "linear l2 loss: 0.4672446 Linear l2 grad loss: 22.042868614196777\n",
                        "linear l2 loss: 0.46711126 Linear l2 grad loss: 22.063953161239624\n",
                        "linear l2 loss: 0.4669819 Linear l2 grad loss: 22.085163593292236\n",
                        "linear l2 loss: 0.46685344 Linear l2 grad loss: 22.106300830841064\n",
                        "linear l2 loss: 0.46672937 Linear l2 grad loss: 22.127612829208374\n",
                        "linear l2 loss: 0.4666067 Linear l2 grad loss: 22.148877143859863\n",
                        "linear l2 loss: 0.46648714 Linear l2 grad loss: 22.17014765739441\n",
                        "linear l2 loss: 0.46636906 Linear l2 grad loss: 22.19137954711914\n",
                        "linear l2 loss: 0.46625376 Linear l2 grad loss: 22.212613344192505\n",
                        "linear l2 loss: 0.46614143 Linear l2 grad loss: 22.23385190963745\n",
                        "linear l2 loss: 0.46603012 Linear l2 grad loss: 22.254981994628906\n",
                        "linear l2 loss: 0.46592003 Linear l2 grad loss: 22.27601718902588\n",
                        "linear l2 loss: 0.46581316 Linear l2 grad loss: 22.297039270401\n",
                        "linear l2 loss: 0.46570644 Linear l2 grad loss: 22.317861080169678\n",
                        "linear l2 loss: 0.46560332 Linear l2 grad loss: 22.338747024536133\n",
                        "linear l2 loss: 0.46550137 Linear l2 grad loss: 22.35948657989502\n",
                        "linear l2 loss: 0.46540043 Linear l2 grad loss: 22.380077600479126\n",
                        "linear l2 loss: 0.46530125 Linear l2 grad loss: 22.400535583496094\n",
                        "linear l2 loss: 0.465202 Linear l2 grad loss: 22.420798540115356\n",
                        "linear l2 loss: 0.4651052 Linear l2 grad loss: 22.440988779067993\n",
                        "linear l2 loss: 0.46500918 Linear l2 grad loss: 22.46100926399231\n",
                        "linear l2 loss: 0.46491513 Linear l2 grad loss: 22.48097801208496\n",
                        "linear l2 loss: 0.46482134 Linear l2 grad loss: 22.500676155090332\n",
                        "linear l2 loss: 0.464727 Linear l2 grad loss: 22.520142793655396\n",
                        "linear l2 loss: 0.4646369 Linear l2 grad loss: 22.539647579193115\n",
                        "linear l2 loss: 0.4645446 Linear l2 grad loss: 22.558790683746338\n",
                        "linear l2 loss: 0.4644547 Linear l2 grad loss: 22.577885150909424\n",
                        "linear l2 loss: 0.46436405 Linear l2 grad loss: 22.596689224243164\n",
                        "linear l2 loss: 0.46427485 Linear l2 grad loss: 22.61536169052124\n",
                        "linear l2 loss: 0.4641873 Linear l2 grad loss: 22.633958101272583\n",
                        "linear l2 loss: 0.46409822 Linear l2 grad loss: 22.652188777923584\n",
                        "linear l2 loss: 0.46400937 Linear l2 grad loss: 22.670264720916748\n",
                        "linear l2 loss: 0.46392202 Linear l2 grad loss: 22.688207387924194\n",
                        "linear l2 loss: 0.46383572 Linear l2 grad loss: 22.70598030090332\n",
                        "linear l2 loss: 0.46374866 Linear l2 grad loss: 22.7235209941864\n",
                        "linear l2 loss: 0.46366262 Linear l2 grad loss: 22.740915060043335\n",
                        "linear l2 loss: 0.46357584 Linear l2 grad loss: 22.758052587509155\n",
                        "linear l2 loss: 0.4634901 Linear l2 grad loss: 22.775062322616577\n",
                        "linear l2 loss: 0.4634038 Linear l2 grad loss: 22.791825771331787\n",
                        "linear l2 loss: 0.46331832 Linear l2 grad loss: 22.808423280715942\n",
                        "linear l2 loss: 0.46323332 Linear l2 grad loss: 22.82490849494934\n",
                        "linear l2 loss: 0.46314764 Linear l2 grad loss: 22.84109616279602\n",
                        "linear l2 loss: 0.46306187 Linear l2 grad loss: 22.857106685638428\n",
                        "linear l2 loss: 0.4629766 Linear l2 grad loss: 22.872954845428467\n",
                        "linear l2 loss: 0.4628918 Linear l2 grad loss: 22.888680458068848\n",
                        "linear l2 loss: 0.46280605 Linear l2 grad loss: 22.904104709625244\n",
                        "linear l2 loss: 0.46272215 Linear l2 grad loss: 22.91952872276306\n",
                        "linear l2 loss: 0.4626356 Linear l2 grad loss: 22.93453335762024\n",
                        "linear l2 loss: 0.46255016 Linear l2 grad loss: 22.94947385787964\n",
                        "linear l2 loss: 0.46246582 Linear l2 grad loss: 22.964305639266968\n",
                        "linear l2 loss: 0.46237928 Linear l2 grad loss: 22.978820085525513\n",
                        "linear l2 loss: 0.46229458 Linear l2 grad loss: 22.993322134017944\n",
                        "linear l2 loss: 0.46220842 Linear l2 grad loss: 23.007474184036255\n",
                        "linear l2 loss: 0.46212313 Linear l2 grad loss: 23.021562814712524\n",
                        "linear l2 loss: 0.46203658 Linear l2 grad loss: 23.035432815551758\n",
                        "linear l2 loss: 0.46195087 Linear l2 grad loss: 23.049164056777954\n",
                        "linear l2 loss: 0.461865 Linear l2 grad loss: 23.062723875045776\n",
                        "linear l2 loss: 0.46177885 Linear l2 grad loss: 23.07610845565796\n",
                        "linear l2 loss: 0.461692 Linear l2 grad loss: 23.08927845954895\n",
                        "linear l2 loss: 0.46160585 Linear l2 grad loss: 23.10240411758423\n",
                        "linear l2 loss: 0.46151972 Linear l2 grad loss: 23.115360975265503\n",
                        "linear l2 loss: 0.4614323 Linear l2 grad loss: 23.128005504608154\n",
                        "linear l2 loss: 0.46134588 Linear l2 grad loss: 23.14062809944153\n",
                        "linear l2 loss: 0.46125793 Linear l2 grad loss: 23.152994871139526\n",
                        "linear l2 loss: 0.46117088 Linear l2 grad loss: 23.165297269821167\n",
                        "linear l2 loss: 0.46108356 Linear l2 grad loss: 23.177433252334595\n",
                        "linear l2 loss: 0.46099558 Linear l2 grad loss: 23.189366340637207\n",
                        "linear l2 loss: 0.46090776 Linear l2 grad loss: 23.20118999481201\n",
                        "linear l2 loss: 0.46081954 Linear l2 grad loss: 23.21285343170166\n",
                        "linear l2 loss: 0.4607315 Linear l2 grad loss: 23.22441864013672\n",
                        "linear l2 loss: 0.46064225 Linear l2 grad loss: 23.2357234954834\n",
                        "linear l2 loss: 0.46055433 Linear l2 grad loss: 23.24699831008911\n",
                        "linear l2 loss: 0.46046486 Linear l2 grad loss: 23.258058786392212\n",
                        "linear l2 loss: 0.46037593 Linear l2 grad loss: 23.26900339126587\n",
                        "linear l2 loss: 0.46028644 Linear l2 grad loss: 23.279834032058716\n",
                        "linear l2 loss: 0.46019694 Linear l2 grad loss: 23.290516138076782\n",
                        "linear l2 loss: 0.4601074 Linear l2 grad loss: 23.30105972290039\n",
                        "linear l2 loss: 0.46001697 Linear l2 grad loss: 23.311494827270508\n",
                        "linear l2 loss: 0.45992708 Linear l2 grad loss: 23.321770429611206\n",
                        "linear l2 loss: 0.45983663 Linear l2 grad loss: 23.331900596618652\n",
                        "linear l2 loss: 0.45974618 Linear l2 grad loss: 23.34191131591797\n",
                        "linear l2 loss: 0.45965534 Linear l2 grad loss: 23.351797819137573\n",
                        "linear l2 loss: 0.45956436 Linear l2 grad loss: 23.361589431762695\n",
                        "linear l2 loss: 0.45947248 Linear l2 grad loss: 23.371229648590088\n",
                        "linear l2 loss: 0.45938167 Linear l2 grad loss: 23.38072943687439\n",
                        "linear l2 loss: 0.45929056 Linear l2 grad loss: 23.390223503112793\n",
                        "linear l2 loss: 0.45919776 Linear l2 grad loss: 23.399393796920776\n",
                        "linear l2 loss: 0.45910606 Linear l2 grad loss: 23.408571004867554\n",
                        "linear l2 loss: 0.45901355 Linear l2 grad loss: 23.417658805847168\n",
                        "linear l2 loss: 0.45892078 Linear l2 grad loss: 23.426528930664062\n",
                        "linear l2 loss: 0.45882893 Linear l2 grad loss: 23.435425281524658\n",
                        "linear l2 loss: 0.45873544 Linear l2 grad loss: 23.44407033920288\n",
                        "linear l2 loss: 0.45864254 Linear l2 grad loss: 23.452685117721558\n",
                        "linear l2 loss: 0.45854926 Linear l2 grad loss: 23.461199522018433\n",
                        "linear l2 loss: 0.4584546 Linear l2 grad loss: 23.469482898712158\n",
                        "linear l2 loss: 0.45836186 Linear l2 grad loss: 23.477871894836426\n",
                        "linear l2 loss: 0.45826754 Linear l2 grad loss: 23.485938549041748\n",
                        "linear l2 loss: 0.45817432 Linear l2 grad loss: 23.494102239608765\n",
                        "linear l2 loss: 0.4580799 Linear l2 grad loss: 23.50204110145569\n",
                        "linear l2 loss: 0.45798427 Linear l2 grad loss: 23.50972867012024\n",
                        "linear l2 loss: 0.4578898 Linear l2 grad loss: 23.517618656158447\n",
                        "linear l2 loss: 0.45779595 Linear l2 grad loss: 23.525296211242676\n",
                        "linear l2 loss: 0.4576995 Linear l2 grad loss: 23.532734632492065\n",
                        "linear l2 loss: 0.4576044 Linear l2 grad loss: 23.540252447128296\n",
                        "linear l2 loss: 0.4575088 Linear l2 grad loss: 23.547600269317627\n",
                        "linear l2 loss: 0.45741305 Linear l2 grad loss: 23.554820775985718\n",
                        "linear l2 loss: 0.45731696 Linear l2 grad loss: 23.562044382095337\n",
                        "linear l2 loss: 0.45722103 Linear l2 grad loss: 23.56912589073181\n",
                        "linear l2 loss: 0.45712492 Linear l2 grad loss: 23.57611584663391\n",
                        "linear l2 loss: 0.45702836 Linear l2 grad loss: 23.58293867111206\n",
                        "linear l2 loss: 0.45693237 Linear l2 grad loss: 23.589913845062256\n",
                        "linear l2 loss: 0.4568344 Linear l2 grad loss: 23.59640598297119\n",
                        "linear l2 loss: 0.45673773 Linear l2 grad loss: 23.603185653686523\n",
                        "linear l2 loss: 0.45664033 Linear l2 grad loss: 23.609631538391113\n",
                        "linear l2 loss: 0.45654303 Linear l2 grad loss: 23.61618661880493\n",
                        "linear l2 loss: 0.4564456 Linear l2 grad loss: 23.622456073760986\n",
                        "linear l2 loss: 0.45634896 Linear l2 grad loss: 23.628947257995605\n",
                        "linear l2 loss: 0.45625004 Linear l2 grad loss: 23.634947299957275\n",
                        "linear l2 loss: 0.45615205 Linear l2 grad loss: 23.641131162643433\n",
                        "linear l2 loss: 0.45605403 Linear l2 grad loss: 23.64720606803894\n",
                        "linear l2 loss: 0.45595607 Linear l2 grad loss: 23.653212547302246\n",
                        "linear l2 loss: 0.45585662 Linear l2 grad loss: 23.65897035598755\n",
                        "linear l2 loss: 0.45575887 Linear l2 grad loss: 23.664961338043213\n",
                        "linear l2 loss: 0.4556597 Linear l2 grad loss: 23.670619249343872\n",
                        "linear l2 loss: 0.45556137 Linear l2 grad loss: 23.676308631896973\n",
                        "linear l2 loss: 0.45546085 Linear l2 grad loss: 23.681819200515747\n",
                        "linear l2 loss: 0.45536247 Linear l2 grad loss: 23.687416791915894\n",
                        "linear l2 loss: 0.45526287 Linear l2 grad loss: 23.69277858734131\n",
                        "linear l2 loss: 0.4551637 Linear l2 grad loss: 23.69823908805847\n",
                        "linear l2 loss: 0.4550623 Linear l2 grad loss: 23.703314781188965\n",
                        "linear l2 loss: 0.4549636 Linear l2 grad loss: 23.70879292488098\n",
                        "linear l2 loss: 0.45486227 Linear l2 grad loss: 23.713685035705566\n",
                        "linear l2 loss: 0.4547629 Linear l2 grad loss: 23.719038486480713\n",
                        "linear l2 loss: 0.45466295 Linear l2 grad loss: 23.723933219909668\n",
                        "linear l2 loss: 0.45456153 Linear l2 grad loss: 23.728808879852295\n",
                        "linear l2 loss: 0.45446086 Linear l2 grad loss: 23.7337429523468\n",
                        "linear l2 loss: 0.45436078 Linear l2 grad loss: 23.73865270614624\n",
                        "linear l2 loss: 0.45425907 Linear l2 grad loss: 23.7431902885437\n",
                        "linear l2 loss: 0.45415893 Linear l2 grad loss: 23.748137950897217\n",
                        "linear l2 loss: 0.45405683 Linear l2 grad loss: 23.752517700195312\n",
                        "linear l2 loss: 0.4539565 Linear l2 grad loss: 23.757330894470215\n",
                        "linear l2 loss: 0.45385385 Linear l2 grad loss: 23.761510848999023\n",
                        "linear l2 loss: 0.45375443 Linear l2 grad loss: 23.766393661499023\n",
                        "linear l2 loss: 0.45365095 Linear l2 grad loss: 23.770264625549316\n",
                        "linear l2 loss: 0.45355064 Linear l2 grad loss: 23.775108575820923\n",
                        "linear l2 loss: 0.45344645 Linear l2 grad loss: 23.778692722320557\n",
                        "linear l2 loss: 0.45334688 Linear l2 grad loss: 23.78361964225769\n",
                        "linear l2 loss: 0.4532424 Linear l2 grad loss: 23.786998987197876\n",
                        "linear l2 loss: 0.45314327 Linear l2 grad loss: 23.79194951057434\n",
                        "linear l2 loss: 0.45303813 Linear l2 grad loss: 23.79504418373108\n",
                        "linear l2 loss: 0.45293903 Linear l2 grad loss: 23.80001401901245\n",
                        "linear l2 loss: 0.45283195 Linear l2 grad loss: 23.802582025527954\n",
                        "linear l2 loss: 0.45273456 Linear l2 grad loss: 23.80799102783203\n",
                        "linear l2 loss: 0.4526244 Linear l2 grad loss: 23.809597969055176\n",
                        "linear l2 loss: 0.4525323 Linear l2 grad loss: 23.815974712371826\n",
                        "linear l2 loss: 0.45241743 Linear l2 grad loss: 23.815603733062744\n",
                        "linear l2 loss: 0.4523327 Linear l2 grad loss: 23.823426723480225\n",
                        "linear l2 loss: 0.452209 Linear l2 grad loss: 23.819517612457275\n",
                        "linear l2 loss: 0.45213953 Linear l2 grad loss: 23.82934021949768\n",
                        "linear l2 loss: 0.4520044 Linear l2 grad loss: 23.819287538528442\n",
                        "linear l2 loss: 0.45195803 Linear l2 grad loss: 23.831597805023193\n",
                        "linear l2 loss: 0.4518044 Linear l2 grad loss: 23.815728187561035\n",
                        "linear l2 loss: 0.45177373 Linear l2 grad loss: 23.833358764648438\n",
                        "linear l2 loss: 0.45159322 Linear l2 grad loss: 23.819774627685547\n",
                        "linear l2 loss: 0.45154935 Linear l2 grad loss: 23.84083127975464\n",
                        "linear l2 loss: 0.45136705 Linear l2 grad loss: 23.83269762992859\n",
                        "linear l2 loss: 0.45130745 Linear l2 grad loss: 23.85024118423462\n",
                        "linear l2 loss: 0.45114723 Linear l2 grad loss: 23.844846487045288\n",
                        "linear l2 loss: 0.45108294 Linear l2 grad loss: 23.85869002342224\n",
                        "linear l2 loss: 0.45094043 Linear l2 grad loss: 23.85433268547058\n",
                        "linear l2 loss: 0.45087242 Linear l2 grad loss: 23.86591911315918\n",
                        "linear l2 loss: 0.45073882 Linear l2 grad loss: 23.86208486557007\n",
                        "linear l2 loss: 0.45066553 Linear l2 grad loss: 23.872170209884644\n",
                        "linear l2 loss: 0.4505355 Linear l2 grad loss: 23.868520736694336\n",
                        "linear l2 loss: 0.45046258 Linear l2 grad loss: 23.8780779838562\n",
                        "linear l2 loss: 0.4503312 Linear l2 grad loss: 23.873666286468506\n",
                        "linear l2 loss: 0.45025992 Linear l2 grad loss: 23.8835928440094\n",
                        "linear l2 loss: 0.45012593 Linear l2 grad loss: 23.87792181968689\n",
                        "linear l2 loss: 0.450056 Linear l2 grad loss: 23.888277053833008\n",
                        "linear l2 loss: 0.44991875 Linear l2 grad loss: 23.88154673576355\n",
                        "linear l2 loss: 0.4498516 Linear l2 grad loss: 23.892509698867798\n",
                        "linear l2 loss: 0.44971058 Linear l2 grad loss: 23.884810209274292\n",
                        "linear l2 loss: 0.44964534 Linear l2 grad loss: 23.89663553237915\n",
                        "linear l2 loss: 0.44949943 Linear l2 grad loss: 23.888489246368408\n",
                        "linear l2 loss: 0.44943544 Linear l2 grad loss: 23.90108847618103\n",
                        "linear l2 loss: 0.44928685 Linear l2 grad loss: 23.89316725730896\n",
                        "linear l2 loss: 0.44922063 Linear l2 grad loss: 23.90591835975647\n",
                        "linear l2 loss: 0.44907275 Linear l2 grad loss: 23.898658990859985\n",
                        "linear l2 loss: 0.44900498 Linear l2 grad loss: 23.910972595214844\n",
                        "linear l2 loss: 0.44886088 Linear l2 grad loss: 23.904489994049072\n",
                        "linear l2 loss: 0.44879013 Linear l2 grad loss: 23.915884017944336\n",
                        "linear l2 loss: 0.44865027 Linear l2 grad loss: 23.910030126571655\n",
                        "linear l2 loss: 0.4485773 Linear l2 grad loss: 23.9206702709198\n",
                        "linear l2 loss: 0.4484392 Linear l2 grad loss: 23.91509461402893\n",
                        "linear l2 loss: 0.448366 Linear l2 grad loss: 23.925169706344604\n",
                        "linear l2 loss: 0.44822922 Linear l2 grad loss: 23.919779300689697\n",
                        "linear l2 loss: 0.44815475 Linear l2 grad loss: 23.92940068244934\n",
                        "linear l2 loss: 0.4480199 Linear l2 grad loss: 23.924092054367065\n",
                        "linear l2 loss: 0.4479439 Linear l2 grad loss: 23.933349132537842\n",
                        "linear l2 loss: 0.44780922 Linear l2 grad loss: 23.928059577941895\n",
                        "linear l2 loss: 0.44773355 Linear l2 grad loss: 23.937265634536743\n",
                        "linear l2 loss: 0.44759867 Linear l2 grad loss: 23.931790113449097\n",
                        "linear l2 loss: 0.44752094 Linear l2 grad loss: 23.94087553024292\n",
                        "linear l2 loss: 0.44738704 Linear l2 grad loss: 23.935582637786865\n",
                        "linear l2 loss: 0.44730908 Linear l2 grad loss: 23.944257974624634\n",
                        "linear l2 loss: 0.44717455 Linear l2 grad loss: 23.93906545639038\n",
                        "linear l2 loss: 0.4470968 Linear l2 grad loss: 23.947852849960327\n",
                        "linear l2 loss: 0.44696265 Linear l2 grad loss: 23.94256091117859\n",
                        "linear l2 loss: 0.44688383 Linear l2 grad loss: 23.951231956481934\n",
                        "linear l2 loss: 0.4467492 Linear l2 grad loss: 23.94592571258545\n",
                        "linear l2 loss: 0.44667035 Linear l2 grad loss: 23.95455837249756\n",
                        "linear l2 loss: 0.4465363 Linear l2 grad loss: 23.94933819770813\n",
                        "linear l2 loss: 0.44645634 Linear l2 grad loss: 23.957696437835693\n",
                        "linear l2 loss: 0.44632313 Linear l2 grad loss: 23.952763319015503\n",
                        "linear l2 loss: 0.44624206 Linear l2 grad loss: 23.960789442062378\n",
                        "linear l2 loss: 0.4461096 Linear l2 grad loss: 23.956114530563354\n",
                        "linear l2 loss: 0.44602773 Linear l2 grad loss: 23.963760137557983\n",
                        "linear l2 loss: 0.44589788 Linear l2 grad loss: 23.959485292434692\n",
                        "linear l2 loss: 0.4458135 Linear l2 grad loss: 23.966455459594727\n",
                        "linear l2 loss: 0.44568545 Linear l2 grad loss: 23.962702989578247\n",
                        "linear l2 loss: 0.4455982 Linear l2 grad loss: 23.96910309791565\n",
                        "linear l2 loss: 0.44547248 Linear l2 grad loss: 23.96580743789673\n",
                        "linear l2 loss: 0.44538352 Linear l2 grad loss: 23.971593379974365\n",
                        "linear l2 loss: 0.44525915 Linear l2 grad loss: 23.968701362609863\n",
                        "linear l2 loss: 0.4451699 Linear l2 grad loss: 23.9741108417511\n",
                        "linear l2 loss: 0.44504616 Linear l2 grad loss: 23.97138738632202\n",
                        "linear l2 loss: 0.4449543 Linear l2 grad loss: 23.9763822555542\n",
                        "linear l2 loss: 0.44483325 Linear l2 grad loss: 23.97414255142212\n",
                        "linear l2 loss: 0.4447394 Linear l2 grad loss: 23.978463649749756\n",
                        "linear l2 loss: 0.44461948 Linear l2 grad loss: 23.976574420928955\n",
                        "linear l2 loss: 0.4445239 Linear l2 grad loss: 23.980488061904907\n",
                        "linear l2 loss: 0.44440627 Linear l2 grad loss: 23.978952407836914\n",
                        "linear l2 loss: 0.44430795 Linear l2 grad loss: 23.98233127593994\n",
                        "linear l2 loss: 0.44419256 Linear l2 grad loss: 23.981212615966797\n",
                        "linear l2 loss: 0.44409186 Linear l2 grad loss: 23.983869552612305\n",
                        "linear l2 loss: 0.44397745 Linear l2 grad loss: 23.983346223831177\n",
                        "linear l2 loss: 0.4438766 Linear l2 grad loss: 23.985578060150146\n",
                        "linear l2 loss: 0.44376326 Linear l2 grad loss: 23.985188245773315\n",
                        "linear l2 loss: 0.4436608 Linear l2 grad loss: 23.987091302871704\n",
                        "linear l2 loss: 0.44354865 Linear l2 grad loss: 23.986977338790894\n",
                        "linear l2 loss: 0.4434444 Linear l2 grad loss: 23.988523244857788\n",
                        "linear l2 loss: 0.443334 Linear l2 grad loss: 23.98860263824463\n",
                        "linear l2 loss: 0.44322845 Linear l2 grad loss: 23.989827156066895\n",
                        "linear l2 loss: 0.44311842 Linear l2 grad loss: 23.990089416503906\n",
                        "linear l2 loss: 0.44301224 Linear l2 grad loss: 23.991069078445435\n",
                        "linear l2 loss: 0.4429021 Linear l2 grad loss: 23.99139904975891\n",
                        "linear l2 loss: 0.4427955 Linear l2 grad loss: 23.9922878742218\n",
                        "linear l2 loss: 0.44268617 Linear l2 grad loss: 23.99262309074402\n",
                        "linear l2 loss: 0.44257876 Linear l2 grad loss: 23.99336338043213\n",
                        "linear l2 loss: 0.44247055 Linear l2 grad loss: 23.99381971359253\n",
                        "linear l2 loss: 0.4423619 Linear l2 grad loss: 23.99434208869934\n",
                        "linear l2 loss: 0.4422536 Linear l2 grad loss: 23.99480962753296\n",
                        "linear l2 loss: 0.44214544 Linear l2 grad loss: 23.995360612869263\n",
                        "linear l2 loss: 0.4420366 Linear l2 grad loss: 23.99568223953247\n",
                        "linear l2 loss: 0.44192782 Linear l2 grad loss: 23.996239185333252\n",
                        "linear l2 loss: 0.44181958 Linear l2 grad loss: 23.996603965759277\n",
                        "linear l2 loss: 0.44171053 Linear l2 grad loss: 23.99693274497986\n",
                        "linear l2 loss: 0.44160149 Linear l2 grad loss: 23.997355222702026\n",
                        "linear l2 loss: 0.4414936 Linear l2 grad loss: 23.99769902229309\n",
                        "linear l2 loss: 0.441385 Linear l2 grad loss: 23.99805998802185\n",
                        "linear l2 loss: 0.44127616 Linear l2 grad loss: 23.99835467338562\n",
                        "linear l2 loss: 0.44116727 Linear l2 grad loss: 23.998599529266357\n",
                        "linear l2 loss: 0.44105908 Linear l2 grad loss: 23.998981475830078\n",
                        "linear l2 loss: 0.44094917 Linear l2 grad loss: 23.99910879135132\n",
                        "linear l2 loss: 0.44084176 Linear l2 grad loss: 23.999499559402466\n",
                        "linear l2 loss: 0.44073197 Linear l2 grad loss: 23.999618530273438\n",
                        "linear l2 loss: 0.44062334 Linear l2 grad loss: 23.999869108200073\n",
                        "linear l2 loss: 0.44051433 Linear l2 grad loss: 23.99999737739563\n",
                        "linear l2 loss: 0.44040546 Linear l2 grad loss: 24.000242471694946\n",
                        "linear l2 loss: 0.44029647 Linear l2 grad loss: 24.000362396240234\n",
                        "linear l2 loss: 0.44018742 Linear l2 grad loss: 24.000497102737427\n",
                        "linear l2 loss: 0.44007865 Linear l2 grad loss: 24.00064706802368\n",
                        "linear l2 loss: 0.43997055 Linear l2 grad loss: 24.000777006149292\n",
                        "linear l2 loss: 0.43986043 Linear l2 grad loss: 24.000763416290283\n",
                        "linear l2 loss: 0.43975115 Linear l2 grad loss: 24.000901699066162\n",
                        "linear l2 loss: 0.43964228 Linear l2 grad loss: 24.00090456008911\n",
                        "linear l2 loss: 0.43953323 Linear l2 grad loss: 24.001004695892334\n",
                        "linear l2 loss: 0.4394253 Linear l2 grad loss: 24.001086950302124\n",
                        "linear l2 loss: 0.43931586 Linear l2 grad loss: 24.00101113319397\n",
                        "linear l2 loss: 0.43920645 Linear l2 grad loss: 24.00107192993164\n",
                        "linear l2 loss: 0.43909723 Linear l2 grad loss: 24.00098991394043\n",
                        "linear l2 loss: 0.43898827 Linear l2 grad loss: 24.000976085662842\n",
                        "linear l2 loss: 0.43887898 Linear l2 grad loss: 24.00093650817871\n",
                        "linear l2 loss: 0.43877003 Linear l2 grad loss: 24.000897645950317\n",
                        "linear l2 loss: 0.4386606 Linear l2 grad loss: 24.000758409500122\n",
                        "linear l2 loss: 0.4385519 Linear l2 grad loss: 24.0007381439209\n",
                        "linear l2 loss: 0.43844277 Linear l2 grad loss: 24.000587940216064\n",
                        "linear l2 loss: 0.43833327 Linear l2 grad loss: 24.000494480133057\n",
                        "linear l2 loss: 0.43822446 Linear l2 grad loss: 24.00038480758667\n",
                        "linear l2 loss: 0.43811503 Linear l2 grad loss: 24.000194549560547\n",
                        "linear l2 loss: 0.43800646 Linear l2 grad loss: 24.000104188919067\n",
                        "linear l2 loss: 0.43789673 Linear l2 grad loss: 23.999862909317017\n",
                        "linear l2 loss: 0.4377872 Linear l2 grad loss: 23.999706983566284\n",
                        "linear l2 loss: 0.43767807 Linear l2 grad loss: 23.99948740005493\n",
                        "linear l2 loss: 0.43756977 Linear l2 grad loss: 23.999332189559937\n",
                        "linear l2 loss: 0.4374601 Linear l2 grad loss: 23.99908471107483\n",
                        "linear l2 loss: 0.4373508 Linear l2 grad loss: 23.998844146728516\n",
                        "linear l2 loss: 0.43724185 Linear l2 grad loss: 23.998620748519897\n",
                        "linear l2 loss: 0.43713236 Linear l2 grad loss: 23.998321294784546\n",
                        "linear l2 loss: 0.43702266 Linear l2 grad loss: 23.99803352355957\n",
                        "linear l2 loss: 0.4369133 Linear l2 grad loss: 23.9977810382843\n",
                        "linear l2 loss: 0.43680477 Linear l2 grad loss: 23.997509956359863\n",
                        "linear l2 loss: 0.43669602 Linear l2 grad loss: 23.997211694717407\n",
                        "linear l2 loss: 0.43658563 Linear l2 grad loss: 23.996843338012695\n",
                        "linear l2 loss: 0.43647742 Linear l2 grad loss: 23.996598958969116\n",
                        "linear l2 loss: 0.43636742 Linear l2 grad loss: 23.996172666549683\n",
                        "linear l2 loss: 0.4362583 Linear l2 grad loss: 23.995830059051514\n",
                        "linear l2 loss: 0.43614826 Linear l2 grad loss: 23.995444297790527\n",
                        "linear l2 loss: 0.43603864 Linear l2 grad loss: 23.995073080062866\n",
                        "linear l2 loss: 0.43593025 Linear l2 grad loss: 23.99474048614502\n",
                        "linear l2 loss: 0.43582058 Linear l2 grad loss: 23.99428081512451\n",
                        "linear l2 loss: 0.4357116 Linear l2 grad loss: 23.993932008743286\n",
                        "linear l2 loss: 0.43560296 Linear l2 grad loss: 23.99353790283203\n",
                        "linear l2 loss: 0.4354933 Linear l2 grad loss: 23.99306058883667\n",
                        "linear l2 loss: 0.43538472 Linear l2 grad loss: 23.99267292022705\n",
                        "linear l2 loss: 0.4352745 Linear l2 grad loss: 23.9921932220459\n",
                        "linear l2 loss: 0.43516597 Linear l2 grad loss: 23.991754293441772\n",
                        "linear l2 loss: 0.43505594 Linear l2 grad loss: 23.99124836921692\n",
                        "linear l2 loss: 0.43494722 Linear l2 grad loss: 23.990837812423706\n",
                        "linear l2 loss: 0.4348372 Linear l2 grad loss: 23.990277767181396\n",
                        "linear l2 loss: 0.43472844 Linear l2 grad loss: 23.989840269088745\n",
                        "linear l2 loss: 0.43461925 Linear l2 grad loss: 23.989298820495605\n",
                        "linear l2 loss: 0.43450996 Linear l2 grad loss: 23.988803386688232\n",
                        "linear l2 loss: 0.43440056 Linear l2 grad loss: 23.988288640975952\n",
                        "linear l2 loss: 0.43429157 Linear l2 grad loss: 23.98775887489319\n",
                        "linear l2 loss: 0.43418232 Linear l2 grad loss: 23.98721742630005\n",
                        "linear l2 loss: 0.43407318 Linear l2 grad loss: 23.98667621612549\n",
                        "linear l2 loss: 0.43396312 Linear l2 grad loss: 23.986079692840576\n",
                        "linear l2 loss: 0.43385378 Linear l2 grad loss: 23.98551058769226\n",
                        "linear l2 loss: 0.43374512 Linear l2 grad loss: 23.98496985435486\n",
                        "linear l2 loss: 0.43363628 Linear l2 grad loss: 23.984378337860107\n",
                        "linear l2 loss: 0.43352672 Linear l2 grad loss: 23.98376727104187\n",
                        "linear l2 loss: 0.43341684 Linear l2 grad loss: 23.98315930366516\n",
                        "linear l2 loss: 0.43330795 Linear l2 grad loss: 23.98254632949829\n",
                        "linear l2 loss: 0.43319806 Linear l2 grad loss: 23.98190975189209\n",
                        "linear l2 loss: 0.43309012 Linear l2 grad loss: 23.981348276138306\n",
                        "linear l2 loss: 0.4329802 Linear l2 grad loss: 23.980672597885132\n",
                        "linear l2 loss: 0.432871 Linear l2 grad loss: 23.98002004623413\n",
                        "linear l2 loss: 0.43276292 Linear l2 grad loss: 23.97941255569458\n",
                        "linear l2 loss: 0.43265295 Linear l2 grad loss: 23.978732585906982\n",
                        "linear l2 loss: 0.43254352 Linear l2 grad loss: 23.978045225143433\n",
                        "linear l2 loss: 0.43243456 Linear l2 grad loss: 23.977412700653076\n",
                        "linear l2 loss: 0.43232533 Linear l2 grad loss: 23.976715326309204\n",
                        "linear l2 loss: 0.43221614 Linear l2 grad loss: 23.976033687591553\n",
                        "linear l2 loss: 0.43210718 Linear l2 grad loss: 23.97533369064331\n",
                        "linear l2 loss: 0.43199787 Linear l2 grad loss: 23.974619388580322\n",
                        "linear l2 loss: 0.43188882 Linear l2 grad loss: 23.973905563354492\n",
                        "linear l2 loss: 0.43177938 Linear l2 grad loss: 23.97319746017456\n",
                        "linear l2 loss: 0.43166968 Linear l2 grad loss: 23.972466468811035\n",
                        "linear l2 loss: 0.43156075 Linear l2 grad loss: 23.971721649169922\n",
                        "linear l2 loss: 0.43145204 Linear l2 grad loss: 23.971020221710205\n",
                        "linear l2 loss: 0.43134236 Linear l2 grad loss: 23.97024178504944\n",
                        "linear l2 loss: 0.4312335 Linear l2 grad loss: 23.96950650215149\n",
                        "linear l2 loss: 0.4311247 Linear l2 grad loss: 23.968756675720215\n",
                        "linear l2 loss: 0.43101558 Linear l2 grad loss: 23.968005418777466\n",
                        "linear l2 loss: 0.43090603 Linear l2 grad loss: 23.96720266342163\n",
                        "linear l2 loss: 0.43079734 Linear l2 grad loss: 23.966444492340088\n",
                        "linear l2 loss: 0.4306886 Linear l2 grad loss: 23.965683937072754\n",
                        "linear l2 loss: 0.43058023 Linear l2 grad loss: 23.96492075920105\n",
                        "linear l2 loss: 0.43047056 Linear l2 grad loss: 23.964092254638672\n",
                        "linear l2 loss: 0.43036136 Linear l2 grad loss: 23.963298797607422\n",
                        "linear l2 loss: 0.43025264 Linear l2 grad loss: 23.96251392364502\n",
                        "linear l2 loss: 0.4301428 Linear l2 grad loss: 23.96165132522583\n",
                        "linear l2 loss: 0.43003508 Linear l2 grad loss: 23.960886478424072\n",
                        "linear l2 loss: 0.4299251 Linear l2 grad loss: 23.96001935005188\n",
                        "linear l2 loss: 0.42981648 Linear l2 grad loss: 23.959219455718994\n",
                        "linear l2 loss: 0.42970696 Linear l2 grad loss: 23.958348035812378\n",
                        "linear l2 loss: 0.42959866 Linear l2 grad loss: 23.957545042037964\n",
                        "linear l2 loss: 0.4294892 Linear l2 grad loss: 23.956664323806763\n",
                        "linear l2 loss: 0.42938122 Linear l2 grad loss: 23.95585060119629\n",
                        "linear l2 loss: 0.42927232 Linear l2 grad loss: 23.95501708984375\n",
                        "linear l2 loss: 0.4291626 Linear l2 grad loss: 23.954094886779785\n",
                        "linear l2 loss: 0.42905417 Linear l2 grad loss: 23.953250885009766\n",
                        "linear l2 loss: 0.4289449 Linear l2 grad loss: 23.95237708091736\n",
                        "linear l2 loss: 0.4288362 Linear l2 grad loss: 23.95151424407959\n",
                        "linear l2 loss: 0.42872706 Linear l2 grad loss: 23.9506254196167\n",
                        "linear l2 loss: 0.42861912 Linear l2 grad loss: 23.949750900268555\n",
                        "linear l2 loss: 0.42850986 Linear l2 grad loss: 23.9488468170166\n",
                        "linear l2 loss: 0.42840084 Linear l2 grad loss: 23.94794011116028\n",
                        "linear l2 loss: 0.42829248 Linear l2 grad loss: 23.947064638137817\n",
                        "linear l2 loss: 0.42818376 Linear l2 grad loss: 23.946152210235596\n",
                        "linear l2 loss: 0.42807505 Linear l2 grad loss: 23.94525909423828\n",
                        "linear l2 loss: 0.4279656 Linear l2 grad loss: 23.944297790527344\n",
                        "linear l2 loss: 0.4278573 Linear l2 grad loss: 23.94341206550598\n",
                        "linear l2 loss: 0.42774853 Linear l2 grad loss: 23.942468643188477\n",
                        "linear l2 loss: 0.4276404 Linear l2 grad loss: 23.941585302352905\n",
                        "linear l2 loss: 0.42753124 Linear l2 grad loss: 23.940616846084595\n",
                        "linear l2 loss: 0.4274225 Linear l2 grad loss: 23.939684867858887\n",
                        "linear l2 loss: 0.42731324 Linear l2 grad loss: 23.93871784210205\n",
                        "linear l2 loss: 0.4272054 Linear l2 grad loss: 23.937809705734253\n",
                        "linear l2 loss: 0.42709634 Linear l2 grad loss: 23.936835765838623\n",
                        "linear l2 loss: 0.42698836 Linear l2 grad loss: 23.93590497970581\n",
                        "linear l2 loss: 0.42687938 Linear l2 grad loss: 23.934942483901978\n",
                        "linear l2 loss: 0.42677042 Linear l2 grad loss: 23.933956384658813\n",
                        "linear l2 loss: 0.42666125 Linear l2 grad loss: 23.932982683181763\n",
                        "linear l2 loss: 0.4265537 Linear l2 grad loss: 23.932047843933105\n",
                        "linear l2 loss: 0.42644492 Linear l2 grad loss: 23.93106698989868\n",
                        "linear l2 loss: 0.4263365 Linear l2 grad loss: 23.930064916610718\n",
                        "linear l2 loss: 0.42622814 Linear l2 grad loss: 23.929122924804688\n",
                        "linear l2 loss: 0.42612037 Linear l2 grad loss: 23.92815589904785\n",
                        "linear l2 loss: 0.4260111 Linear l2 grad loss: 23.927121877670288\n",
                        "linear l2 loss: 0.42590228 Linear l2 grad loss: 23.926129817962646\n",
                        "linear l2 loss: 0.4257938 Linear l2 grad loss: 23.925137042999268\n",
                        "linear l2 loss: 0.42568594 Linear l2 grad loss: 23.924147367477417\n",
                        "linear l2 loss: 0.42557707 Linear l2 grad loss: 23.923118352890015\n",
                        "linear l2 loss: 0.42546913 Linear l2 grad loss: 23.922154188156128\n",
                        "linear l2 loss: 0.4253604 Linear l2 grad loss: 23.921122550964355\n",
                        "linear l2 loss: 0.42525265 Linear l2 grad loss: 23.92014193534851\n",
                        "linear l2 loss: 0.4251441 Linear l2 grad loss: 23.91910696029663\n",
                        "linear l2 loss: 0.4250354 Linear l2 grad loss: 23.91805934906006\n",
                        "linear l2 loss: 0.42492804 Linear l2 grad loss: 23.91709041595459\n",
                        "linear l2 loss: 0.42481947 Linear l2 grad loss: 23.916057586669922\n",
                        "linear l2 loss: 0.42471096 Linear l2 grad loss: 23.915008306503296\n",
                        "linear l2 loss: 0.4246026 Linear l2 grad loss: 23.913984060287476\n",
                        "linear l2 loss: 0.42449436 Linear l2 grad loss: 23.9129421710968\n",
                        "linear l2 loss: 0.42438632 Linear l2 grad loss: 23.911898612976074\n",
                        "linear l2 loss: 0.42427883 Linear l2 grad loss: 23.9109148979187\n",
                        "linear l2 loss: 0.42416987 Linear l2 grad loss: 23.90981960296631\n",
                        "linear l2 loss: 0.4240625 Linear l2 grad loss: 23.90880560874939\n",
                        "linear l2 loss: 0.42395312 Linear l2 grad loss: 23.90771484375\n",
                        "linear l2 loss: 0.42384505 Linear l2 grad loss: 23.90666389465332\n",
                        "linear l2 loss: 0.42373735 Linear l2 grad loss: 23.90562629699707\n",
                        "linear l2 loss: 0.42362982 Linear l2 grad loss: 23.904601573944092\n",
                        "linear l2 loss: 0.42352152 Linear l2 grad loss: 23.903515100479126\n",
                        "linear l2 loss: 0.42341366 Linear l2 grad loss: 23.902464151382446\n",
                        "linear l2 loss: 0.42330524 Linear l2 grad loss: 23.90138339996338\n",
                        "linear l2 loss: 0.42319688 Linear l2 grad loss: 23.900304555892944\n",
                        "linear l2 loss: 0.42309007 Linear l2 grad loss: 23.89928102493286\n",
                        "linear l2 loss: 0.4229818 Linear l2 grad loss: 23.8981831073761\n",
                        "linear l2 loss: 0.4228736 Linear l2 grad loss: 23.89710783958435\n",
                        "linear l2 loss: 0.42276487 Linear l2 grad loss: 23.89599061012268\n",
                        "linear l2 loss: 0.42265797 Linear l2 grad loss: 23.894958972930908\n",
                        "linear l2 loss: 0.42255005 Linear l2 grad loss: 23.893874406814575\n",
                        "linear l2 loss: 0.4224422 Linear l2 grad loss: 23.892778635025024\n",
                        "linear l2 loss: 0.42233443 Linear l2 grad loss: 23.891703844070435\n",
                        "linear l2 loss: 0.42222592 Linear l2 grad loss: 23.890572547912598\n",
                        "linear l2 loss: 0.42211857 Linear l2 grad loss: 23.88951587677002\n",
                        "linear l2 loss: 0.42201072 Linear l2 grad loss: 23.888412714004517\n",
                        "linear l2 loss: 0.4219032 Linear l2 grad loss: 23.88731622695923\n",
                        "linear l2 loss: 0.42179656 Linear l2 grad loss: 23.886271953582764\n",
                        "linear l2 loss: 0.42168805 Linear l2 grad loss: 23.885138034820557\n",
                        "linear l2 loss: 0.42158046 Linear l2 grad loss: 23.88404941558838\n",
                        "linear l2 loss: 0.4214724 Linear l2 grad loss: 23.88291645050049\n",
                        "linear l2 loss: 0.42136535 Linear l2 grad loss: 23.881836414337158\n",
                        "linear l2 loss: 0.42125818 Linear l2 grad loss: 23.880740880966187\n",
                        "linear l2 loss: 0.42114994 Linear l2 grad loss: 23.879613399505615\n",
                        "linear l2 loss: 0.42104235 Linear l2 grad loss: 23.878500938415527\n",
                        "linear l2 loss: 0.42093453 Linear l2 grad loss: 23.87736964225769\n",
                        "linear l2 loss: 0.42082724 Linear l2 grad loss: 23.87626814842224\n",
                        "linear l2 loss: 0.42072043 Linear l2 grad loss: 23.87518072128296\n",
                        "linear l2 loss: 0.42061242 Linear l2 grad loss: 23.87404179573059\n",
                        "linear l2 loss: 0.42050478 Linear l2 grad loss: 23.872902631759644\n",
                        "linear l2 loss: 0.42039734 Linear l2 grad loss: 23.871788501739502\n",
                        "linear l2 loss: 0.4202896 Linear l2 grad loss: 23.870648622512817\n",
                        "linear l2 loss: 0.4201828 Linear l2 grad loss: 23.8695547580719\n",
                        "linear l2 loss: 0.42007524 Linear l2 grad loss: 23.868407011032104\n",
                        "linear l2 loss: 0.41996822 Linear l2 grad loss: 23.867309093475342\n",
                        "linear l2 loss: 0.41986036 Linear l2 grad loss: 23.866140127182007\n",
                        "linear l2 loss: 0.41975313 Linear l2 grad loss: 23.865017652511597\n",
                        "linear l2 loss: 0.41964564 Linear l2 grad loss: 23.863869428634644\n",
                        "linear l2 loss: 0.41953897 Linear l2 grad loss: 23.862763166427612\n",
                        "linear l2 loss: 0.4194317 Linear l2 grad loss: 23.861625909805298\n",
                        "linear l2 loss: 0.41932502 Linear l2 grad loss: 23.86050271987915\n",
                        "linear l2 loss: 0.4192174 Linear l2 grad loss: 23.859352827072144\n",
                        "linear l2 loss: 0.4191106 Linear l2 grad loss: 23.858217000961304\n",
                        "linear l2 loss: 0.41900283 Linear l2 grad loss: 23.85705041885376\n",
                        "linear l2 loss: 0.41889563 Linear l2 grad loss: 23.85589909553528\n",
                        "linear l2 loss: 0.41878882 Linear l2 grad loss: 23.854769945144653\n",
                        "linear l2 loss: 0.41868177 Linear l2 grad loss: 23.853623151779175\n",
                        "linear l2 loss: 0.41857472 Linear l2 grad loss: 23.85247039794922\n",
                        "linear l2 loss: 0.4184682 Linear l2 grad loss: 23.851335763931274\n",
                        "linear l2 loss: 0.4183612 Linear l2 grad loss: 23.850191593170166\n",
                        "linear l2 loss: 0.4182542 Linear l2 grad loss: 23.849038124084473\n",
                        "linear l2 loss: 0.4181466 Linear l2 grad loss: 23.847860097885132\n",
                        "linear l2 loss: 0.41804066 Linear l2 grad loss: 23.846746921539307\n",
                        "linear l2 loss: 0.41793272 Linear l2 grad loss: 23.845553636550903\n",
                        "linear l2 loss: 0.41782656 Linear l2 grad loss: 23.844412803649902\n",
                        "linear l2 loss: 0.41771984 Linear l2 grad loss: 23.84326720237732\n",
                        "linear l2 loss: 0.417612 Linear l2 grad loss: 23.842063426971436\n",
                        "linear l2 loss: 0.4175059 Linear l2 grad loss: 23.840932607650757\n",
                        "linear l2 loss: 0.41739893 Linear l2 grad loss: 23.83975577354431\n",
                        "linear l2 loss: 0.41729245 Linear l2 grad loss: 23.83860683441162\n",
                        "linear l2 loss: 0.41718552 Linear l2 grad loss: 23.837435245513916\n",
                        "linear l2 loss: 0.41707832 Linear l2 grad loss: 23.836253881454468\n",
                        "linear l2 loss: 0.4169724 Linear l2 grad loss: 23.83511972427368\n",
                        "linear l2 loss: 0.41686612 Linear l2 grad loss: 23.83395004272461\n",
                        "linear l2 loss: 0.4167587 Linear l2 grad loss: 23.832762718200684\n",
                        "linear l2 loss: 0.41665268 Linear l2 grad loss: 23.83161425590515\n",
                        "linear l2 loss: 0.41654578 Linear l2 grad loss: 23.830435514450073\n",
                        "linear l2 loss: 0.41643927 Linear l2 grad loss: 23.82926297187805\n",
                        "linear l2 loss: 0.4163331 Linear l2 grad loss: 23.828109741210938\n",
                        "linear l2 loss: 0.4162259 Linear l2 grad loss: 23.8269202709198\n",
                        "linear l2 loss: 0.41612002 Linear l2 grad loss: 23.825750827789307\n",
                        "linear l2 loss: 0.41601363 Linear l2 grad loss: 23.824576377868652\n",
                        "linear l2 loss: 0.4159069 Linear l2 grad loss: 23.823395252227783\n",
                        "linear l2 loss: 0.41580024 Linear l2 grad loss: 23.822221755981445\n",
                        "linear l2 loss: 0.4156948 Linear l2 grad loss: 23.821080207824707\n",
                        "linear l2 loss: 0.4155882 Linear l2 grad loss: 23.819870948791504\n",
                        "linear l2 loss: 0.4154813 Linear l2 grad loss: 23.818681001663208\n",
                        "linear l2 loss: 0.41537577 Linear l2 grad loss: 23.817540884017944\n",
                        "linear l2 loss: 0.41526914 Linear l2 grad loss: 23.81632900238037\n",
                        "linear l2 loss: 0.41516304 Linear l2 grad loss: 23.815157890319824\n",
                        "linear l2 loss: 0.41505605 Linear l2 grad loss: 23.813947200775146\n",
                        "linear l2 loss: 0.4149495 Linear l2 grad loss: 23.812756538391113\n",
                        "linear l2 loss: 0.4148441 Linear l2 grad loss: 23.811601877212524\n",
                        "linear l2 loss: 0.41473767 Linear l2 grad loss: 23.810397624969482\n",
                        "linear l2 loss: 0.41463223 Linear l2 grad loss: 23.809245109558105\n",
                        "linear l2 loss: 0.41452622 Linear l2 grad loss: 23.808062076568604\n",
                        "linear l2 loss: 0.4144199 Linear l2 grad loss: 23.806873559951782\n",
                        "linear l2 loss: 0.41431347 Linear l2 grad loss: 23.805673122406006\n",
                        "linear l2 loss: 0.4142073 Linear l2 grad loss: 23.80447769165039\n",
                        "linear l2 loss: 0.41410166 Linear l2 grad loss: 23.803303718566895\n",
                        "linear l2 loss: 0.41399604 Linear l2 grad loss: 23.802130460739136\n",
                        "linear l2 loss: 0.41388962 Linear l2 grad loss: 23.800909757614136\n",
                        "linear l2 loss: 0.41378418 Linear l2 grad loss: 23.7997465133667\n",
                        "linear l2 loss: 0.41367784 Linear l2 grad loss: 23.79853343963623\n",
                        "linear l2 loss: 0.41357213 Linear l2 grad loss: 23.797359704971313\n",
                        "linear l2 loss: 0.41346654 Linear l2 grad loss: 23.796173334121704\n",
                        "linear l2 loss: 0.41336033 Linear l2 grad loss: 23.79497265815735\n",
                        "linear l2 loss: 0.4132557 Linear l2 grad loss: 23.79381823539734\n",
                        "linear l2 loss: 0.41314936 Linear l2 grad loss: 23.79261040687561\n",
                        "linear l2 loss: 0.41304356 Linear l2 grad loss: 23.79140830039978\n",
                        "linear l2 loss: 0.41293788 Linear l2 grad loss: 23.790215015411377\n",
                        "linear l2 loss: 0.41283193 Linear l2 grad loss: 23.789007663726807\n",
                        "linear l2 loss: 0.41272554 Linear l2 grad loss: 23.787779808044434\n",
                        "linear l2 loss: 0.4126207 Linear l2 grad loss: 23.78661584854126\n",
                        "linear l2 loss: 0.41251513 Linear l2 grad loss: 23.78542160987854\n",
                        "linear l2 loss: 0.41240954 Linear l2 grad loss: 23.784230947494507\n",
                        "linear l2 loss: 0.41230413 Linear l2 grad loss: 23.783037424087524\n",
                        "linear l2 loss: 0.41219816 Linear l2 grad loss: 23.781819343566895\n",
                        "linear l2 loss: 0.41209355 Linear l2 grad loss: 23.78065800666809\n",
                        "linear l2 loss: 0.41198787 Linear l2 grad loss: 23.77945065498352\n",
                        "linear l2 loss: 0.4118824 Linear l2 grad loss: 23.77825927734375\n",
                        "linear l2 loss: 0.41177675 Linear l2 grad loss: 23.777051210403442\n",
                        "linear l2 loss: 0.41167137 Linear l2 grad loss: 23.775848150253296\n",
                        "linear l2 loss: 0.4115667 Linear l2 grad loss: 23.77468180656433\n",
                        "linear l2 loss: 0.41146073 Linear l2 grad loss: 23.773457765579224\n",
                        "linear l2 loss: 0.41135675 Linear l2 grad loss: 23.772300004959106\n",
                        "linear l2 loss: 0.41125044 Linear l2 grad loss: 23.77106475830078\n",
                        "linear l2 loss: 0.41114494 Linear l2 grad loss: 23.76986002922058\n",
                        "linear l2 loss: 0.41103974 Linear l2 grad loss: 23.768651247024536\n",
                        "linear l2 loss: 0.41093436 Linear l2 grad loss: 23.767448902130127\n",
                        "linear l2 loss: 0.41082984 Linear l2 grad loss: 23.766276597976685\n",
                        "linear l2 loss: 0.41072342 Linear l2 grad loss: 23.76502227783203\n",
                        "linear l2 loss: 0.41061994 Linear l2 grad loss: 23.763879537582397\n",
                        "linear l2 loss: 0.4105144 Linear l2 grad loss: 23.762659072875977\n",
                        "linear l2 loss: 0.41041008 Linear l2 grad loss: 23.7614905834198\n",
                        "linear l2 loss: 0.4103039 Linear l2 grad loss: 23.760243892669678\n",
                        "linear l2 loss: 0.410199 Linear l2 grad loss: 23.759032726287842\n",
                        "linear l2 loss: 0.41009465 Linear l2 grad loss: 23.757862329483032\n",
                        "linear l2 loss: 0.40998927 Linear l2 grad loss: 23.756644010543823\n",
                        "linear l2 loss: 0.409885 Linear l2 grad loss: 23.755462884902954\n",
                        "linear l2 loss: 0.40977937 Linear l2 grad loss: 23.754223585128784\n",
                        "linear l2 loss: 0.4096746 Linear l2 grad loss: 23.753031015396118\n",
                        "linear l2 loss: 0.4095698 Linear l2 grad loss: 23.751837730407715\n",
                        "linear l2 loss: 0.40946484 Linear l2 grad loss: 23.75061535835266\n",
                        "linear l2 loss: 0.4093608 Linear l2 grad loss: 23.749444484710693\n",
                        "linear l2 loss: 0.40925524 Linear l2 grad loss: 23.748212575912476\n",
                        "linear l2 loss: 0.409151 Linear l2 grad loss: 23.747035264968872\n",
                        "linear l2 loss: 0.40904623 Linear l2 grad loss: 23.745814085006714\n",
                        "linear l2 loss: 0.40894118 Linear l2 grad loss: 23.744600772857666\n",
                        "linear l2 loss: 0.4088366 Linear l2 grad loss: 23.743393421173096\n",
                        "linear l2 loss: 0.40873224 Linear l2 grad loss: 23.74220108985901\n",
                        "linear l2 loss: 0.408628 Linear l2 grad loss: 23.7410089969635\n",
                        "linear l2 loss: 0.40852317 Linear l2 grad loss: 23.739805459976196\n",
                        "linear l2 loss: 0.40841848 Linear l2 grad loss: 23.738579988479614\n",
                        "linear l2 loss: 0.40831456 Linear l2 grad loss: 23.73739528656006\n",
                        "linear l2 loss: 0.40821013 Linear l2 grad loss: 23.736207723617554\n",
                        "linear l2 loss: 0.4081048 Linear l2 grad loss: 23.734962940216064\n",
                        "linear l2 loss: 0.40800142 Linear l2 grad loss: 23.73379683494568\n",
                        "linear l2 loss: 0.40789613 Linear l2 grad loss: 23.732560873031616\n",
                        "linear l2 loss: 0.4077924 Linear l2 grad loss: 23.731365442276\n",
                        "linear l2 loss: 0.40768766 Linear l2 grad loss: 23.73015546798706\n",
                        "linear l2 loss: 0.40758356 Linear l2 grad loss: 23.728962182998657\n",
                        "linear l2 loss: 0.4074793 Linear l2 grad loss: 23.727758169174194\n",
                        "linear l2 loss: 0.40737477 Linear l2 grad loss: 23.726531744003296\n",
                        "linear l2 loss: 0.40727052 Linear l2 grad loss: 23.7253315448761\n",
                        "linear l2 loss: 0.40716648 Linear l2 grad loss: 23.72412633895874\n",
                        "linear l2 loss: 0.4070615 Linear l2 grad loss: 23.722900867462158\n",
                        "linear l2 loss: 0.40695864 Linear l2 grad loss: 23.72173285484314\n",
                        "linear l2 loss: 0.4068534 Linear l2 grad loss: 23.72050428390503\n",
                        "linear l2 loss: 0.40674964 Linear l2 grad loss: 23.719302892684937\n",
                        "linear l2 loss: 0.40664577 Linear l2 grad loss: 23.718095779418945\n",
                        "linear l2 loss: 0.40654165 Linear l2 grad loss: 23.71689224243164\n",
                        "linear l2 loss: 0.40643826 Linear l2 grad loss: 23.715704441070557\n",
                        "linear l2 loss: 0.40633374 Linear l2 grad loss: 23.714476346969604\n",
                        "linear l2 loss: 0.40623045 Linear l2 grad loss: 23.713300704956055\n",
                        "linear l2 loss: 0.4061268 Linear l2 grad loss: 23.712109804153442\n",
                        "linear l2 loss: 0.40602192 Linear l2 grad loss: 23.710866928100586\n",
                        "linear l2 loss: 0.40591908 Linear l2 grad loss: 23.709692239761353\n",
                        "linear l2 loss: 0.40581393 Linear l2 grad loss: 23.708441257476807\n",
                        "linear l2 loss: 0.4057108 Linear l2 grad loss: 23.70725917816162\n",
                        "linear l2 loss: 0.40560728 Linear l2 grad loss: 23.706076860427856\n",
                        "linear l2 loss: 0.40550327 Linear l2 grad loss: 23.704846620559692\n",
                        "linear l2 loss: 0.40539953 Linear l2 grad loss: 23.703656911849976\n",
                        "linear l2 loss: 0.40529513 Linear l2 grad loss: 23.70241641998291\n",
                        "linear l2 loss: 0.40519246 Linear l2 grad loss: 23.70125102996826\n",
                        "linear l2 loss: 0.40508816 Linear l2 grad loss: 23.700022220611572\n",
                        "linear l2 loss: 0.40498492 Linear l2 grad loss: 23.69882822036743\n",
                        "linear l2 loss: 0.40488148 Linear l2 grad loss: 23.697638511657715\n",
                        "linear l2 loss: 0.40477815 Linear l2 grad loss: 23.69643545150757\n",
                        "linear l2 loss: 0.4046741 Linear l2 grad loss: 23.69521450996399\n",
                        "linear l2 loss: 0.40457112 Linear l2 grad loss: 23.694024324417114\n",
                        "linear l2 loss: 0.40446758 Linear l2 grad loss: 23.692814826965332\n",
                        "linear l2 loss: 0.4043645 Linear l2 grad loss: 23.69163203239441\n",
                        "linear l2 loss: 0.40426102 Linear l2 grad loss: 23.690430402755737\n",
                        "linear l2 loss: 0.40415716 Linear l2 grad loss: 23.689208030700684\n",
                        "linear l2 loss: 0.40405303 Linear l2 grad loss: 23.687965631484985\n",
                        "linear l2 loss: 0.40395036 Linear l2 grad loss: 23.686796188354492\n",
                        "linear l2 loss: 0.40384775 Linear l2 grad loss: 23.685613870620728\n",
                        "linear l2 loss: 0.40374365 Linear l2 grad loss: 23.68438172340393\n",
                        "linear l2 loss: 0.40364066 Linear l2 grad loss: 23.68319034576416\n",
                        "linear l2 loss: 0.40353814 Linear l2 grad loss: 23.682016611099243\n",
                        "linear l2 loss: 0.4034341 Linear l2 grad loss: 23.68077564239502\n",
                        "linear l2 loss: 0.40333137 Linear l2 grad loss: 23.6795916557312\n",
                        "linear l2 loss: 0.403229 Linear l2 grad loss: 23.678415060043335\n",
                        "linear l2 loss: 0.40312502 Linear l2 grad loss: 23.67717742919922\n",
                        "linear l2 loss: 0.4030219 Linear l2 grad loss: 23.67598009109497\n",
                        "linear l2 loss: 0.40291926 Linear l2 grad loss: 23.674792051315308\n",
                        "linear l2 loss: 0.40281636 Linear l2 grad loss: 23.673601150512695\n",
                        "linear l2 loss: 0.40271312 Linear l2 grad loss: 23.672385931015015\n",
                        "linear l2 loss: 0.40261027 Linear l2 grad loss: 23.671194553375244\n",
                        "linear l2 loss: 0.4025076 Linear l2 grad loss: 23.669995069503784\n",
                        "linear l2 loss: 0.4024046 Linear l2 grad loss: 23.668794631958008\n",
                        "linear l2 loss: 0.4023016 Linear l2 grad loss: 23.667595863342285\n",
                        "linear l2 loss: 0.4021991 Linear l2 grad loss: 23.666399717330933\n",
                        "linear l2 loss: 0.40209594 Linear l2 grad loss: 23.665188789367676\n",
                        "linear l2 loss: 0.40199313 Linear l2 grad loss: 23.6639986038208\n",
                        "linear l2 loss: 0.40188965 Linear l2 grad loss: 23.6627779006958\n",
                        "linear l2 loss: 0.40178713 Linear l2 grad loss: 23.66157627105713\n",
                        "linear l2 loss: 0.4016861 Linear l2 grad loss: 23.660441398620605\n",
                        "linear l2 loss: 0.40158185 Linear l2 grad loss: 23.65919518470764\n",
                        "linear l2 loss: 0.4014801 Linear l2 grad loss: 23.658020973205566\n",
                        "linear l2 loss: 0.4013774 Linear l2 grad loss: 23.656820058822632\n",
                        "linear l2 loss: 0.40127435 Linear l2 grad loss: 23.655592441558838\n",
                        "linear l2 loss: 0.40117216 Linear l2 grad loss: 23.654422998428345\n",
                        "linear l2 loss: 0.4010696 Linear l2 grad loss: 23.65322518348694\n",
                        "linear l2 loss: 0.40096697 Linear l2 grad loss: 23.652019023895264\n",
                        "linear l2 loss: 0.40086418 Linear l2 grad loss: 23.6508150100708\n",
                        "linear l2 loss: 0.4007628 Linear l2 grad loss: 23.64965271949768\n",
                        "linear l2 loss: 0.40066049 Linear l2 grad loss: 23.64845609664917\n",
                        "linear l2 loss: 0.40055752 Linear l2 grad loss: 23.647239923477173\n",
                        "linear l2 loss: 0.40045467 Linear l2 grad loss: 23.646031618118286\n",
                        "linear l2 loss: 0.400352 Linear l2 grad loss: 23.644827365875244\n",
                        "linear l2 loss: 0.4002512 Linear l2 grad loss: 23.64367938041687\n",
                        "linear l2 loss: 0.40014872 Linear l2 grad loss: 23.642482042312622\n",
                        "linear l2 loss: 0.40004602 Linear l2 grad loss: 23.641271114349365\n",
                        "linear l2 loss: 0.39994413 Linear l2 grad loss: 23.640094757080078\n",
                        "linear l2 loss: 0.39984164 Linear l2 grad loss: 23.63889241218567\n",
                        "linear l2 loss: 0.39973998 Linear l2 grad loss: 23.637712478637695\n",
                        "linear l2 loss: 0.39963812 Linear l2 grad loss: 23.636531591415405\n",
                        "linear l2 loss: 0.39953646 Linear l2 grad loss: 23.635355472564697\n",
                        "linear l2 loss: 0.39943305 Linear l2 grad loss: 23.634108066558838\n",
                        "linear l2 loss: 0.3993321 Linear l2 grad loss: 23.63296389579773\n",
                        "linear l2 loss: 0.39922938 Linear l2 grad loss: 23.631744861602783\n",
                        "linear l2 loss: 0.39912683 Linear l2 grad loss: 23.630529403686523\n",
                        "linear l2 loss: 0.39902592 Linear l2 grad loss: 23.629374504089355\n",
                        "linear l2 loss: 0.39892387 Linear l2 grad loss: 23.628183841705322\n",
                        "linear l2 loss: 0.39882156 Linear l2 grad loss: 23.626975059509277\n",
                        "linear l2 loss: 0.39872047 Linear l2 grad loss: 23.625816345214844\n",
                        "linear l2 loss: 0.39861813 Linear l2 grad loss: 23.62460494041443\n",
                        "linear l2 loss: 0.39851627 Linear l2 grad loss: 23.623423099517822\n",
                        "linear l2 loss: 0.39841512 Linear l2 grad loss: 23.62223982810974\n",
                        "linear l2 loss: 0.39831352 Linear l2 grad loss: 23.62106728553772\n",
                        "linear l2 loss: 0.3982109 Linear l2 grad loss: 23.619853019714355\n",
                        "linear l2 loss: 0.39811042 Linear l2 grad loss: 23.61870503425598\n",
                        "linear l2 loss: 0.39800778 Linear l2 grad loss: 23.617464542388916\n",
                        "linear l2 loss: 0.39790633 Linear l2 grad loss: 23.616294384002686\n",
                        "linear l2 loss: 0.39780542 Linear l2 grad loss: 23.61513352394104\n",
                        "linear l2 loss: 0.397704 Linear l2 grad loss: 23.613945722579956\n",
                        "linear l2 loss: 0.39760193 Linear l2 grad loss: 23.61274814605713\n",
                        "linear l2 loss: 0.39750037 Linear l2 grad loss: 23.61155652999878\n",
                        "linear l2 loss: 0.3973992 Linear l2 grad loss: 23.610390186309814\n",
                        "linear l2 loss: 0.39729747 Linear l2 grad loss: 23.609188079833984\n",
                        "linear l2 loss: 0.3971963 Linear l2 grad loss: 23.608018398284912\n",
                        "linear l2 loss: 0.39709482 Linear l2 grad loss: 23.606825828552246\n",
                        "linear l2 loss: 0.39699346 Linear l2 grad loss: 23.605642557144165\n",
                        "linear l2 loss: 0.3968918 Linear l2 grad loss: 23.60444474220276\n",
                        "linear l2 loss: 0.39679152 Linear l2 grad loss: 23.603300094604492\n",
                        "linear l2 loss: 0.39668992 Linear l2 grad loss: 23.602118730545044\n",
                        "linear l2 loss: 0.39658886 Linear l2 grad loss: 23.600930213928223\n",
                        "linear l2 loss: 0.39648882 Linear l2 grad loss: 23.599798917770386\n",
                        "linear l2 loss: 0.3963862 Linear l2 grad loss: 23.5985586643219\n",
                        "linear l2 loss: 0.39628458 Linear l2 grad loss: 23.59736204147339\n",
                        "linear l2 loss: 0.39618412 Linear l2 grad loss: 23.59620499610901\n",
                        "linear l2 loss: 0.39608333 Linear l2 grad loss: 23.5950345993042\n",
                        "linear l2 loss: 0.3959825 Linear l2 grad loss: 23.593872547149658\n",
                        "linear l2 loss: 0.3958811 Linear l2 grad loss: 23.5926775932312\n",
                        "linear l2 loss: 0.39577985 Linear l2 grad loss: 23.591490745544434\n",
                        "linear l2 loss: 0.3956783 Linear l2 grad loss: 23.590283155441284\n",
                        "linear l2 loss: 0.39557812 Linear l2 grad loss: 23.589141845703125\n",
                        "linear l2 loss: 0.39547724 Linear l2 grad loss: 23.587968349456787\n",
                        "linear l2 loss: 0.3953773 Linear l2 grad loss: 23.586822032928467\n",
                        "linear l2 loss: 0.39527532 Linear l2 grad loss: 23.585602283477783\n",
                        "linear l2 loss: 0.3951746 Linear l2 grad loss: 23.584430932998657\n",
                        "linear l2 loss: 0.3950741 Linear l2 grad loss: 23.583277225494385\n",
                        "linear l2 loss: 0.3949729 Linear l2 grad loss: 23.582077741622925\n",
                        "linear l2 loss: 0.3948728 Linear l2 grad loss: 23.580927848815918\n",
                        "linear l2 loss: 0.39477244 Linear l2 grad loss: 23.579765796661377\n",
                        "linear l2 loss: 0.39467153 Linear l2 grad loss: 23.57858633995056\n",
                        "linear l2 loss: 0.39457107 Linear l2 grad loss: 23.577422857284546\n",
                        "linear l2 loss: 0.39447042 Linear l2 grad loss: 23.576246738433838\n",
                        "linear l2 loss: 0.39436966 Linear l2 grad loss: 23.57507038116455\n",
                        "linear l2 loss: 0.39426872 Linear l2 grad loss: 23.57387948036194\n",
                        "linear l2 loss: 0.39416838 Linear l2 grad loss: 23.572720766067505\n",
                        "linear l2 loss: 0.39406824 Linear l2 grad loss: 23.571564197540283\n",
                        "linear l2 loss: 0.39396736 Linear l2 grad loss: 23.570375680923462\n",
                        "linear l2 loss: 0.39386782 Linear l2 grad loss: 23.56923508644104\n",
                        "linear l2 loss: 0.3937671 Linear l2 grad loss: 23.56805729866028\n",
                        "linear l2 loss: 0.39366665 Linear l2 grad loss: 23.56689214706421\n",
                        "linear l2 loss: 0.39356676 Linear l2 grad loss: 23.565735578536987\n",
                        "linear l2 loss: 0.39346725 Linear l2 grad loss: 23.564595937728882\n",
                        "linear l2 loss: 0.39336598 Linear l2 grad loss: 23.56340003013611\n",
                        "linear l2 loss: 0.3932662 Linear l2 grad loss: 23.562246322631836\n",
                        "linear l2 loss: 0.39316526 Linear l2 grad loss: 23.56105136871338\n",
                        "linear l2 loss: 0.39306578 Linear l2 grad loss: 23.55991506576538\n",
                        "linear l2 loss: 0.39296612 Linear l2 grad loss: 23.558759450912476\n",
                        "linear l2 loss: 0.3928655 Linear l2 grad loss: 23.55759024620056\n",
                        "linear l2 loss: 0.3927654 Linear l2 grad loss: 23.55641770362854\n",
                        "linear l2 loss: 0.39266524 Linear l2 grad loss: 23.555255651474\n",
                        "linear l2 loss: 0.39256534 Linear l2 grad loss: 23.554097652435303\n",
                        "linear l2 loss: 0.39246544 Linear l2 grad loss: 23.55293893814087\n",
                        "linear l2 loss: 0.39236543 Linear l2 grad loss: 23.551778554916382\n",
                        "linear l2 loss: 0.39226508 Linear l2 grad loss: 23.550606966018677\n",
                        "linear l2 loss: 0.39216572 Linear l2 grad loss: 23.549450635910034\n",
                        "linear l2 loss: 0.39206553 Linear l2 grad loss: 23.548287630081177\n",
                        "linear l2 loss: 0.39196596 Linear l2 grad loss: 23.547141075134277\n",
                        "linear l2 loss: 0.39186606 Linear l2 grad loss: 23.545971155166626\n",
                        "linear l2 loss: 0.3917664 Linear l2 grad loss: 23.54481077194214\n",
                        "linear l2 loss: 0.39166665 Linear l2 grad loss: 23.54366636276245\n",
                        "linear l2 loss: 0.39156732 Linear l2 grad loss: 23.54252338409424\n",
                        "linear l2 loss: 0.39146793 Linear l2 grad loss: 23.54137349128723\n",
                        "linear l2 loss: 0.39136747 Linear l2 grad loss: 23.540191888809204\n",
                        "linear l2 loss: 0.3912688 Linear l2 grad loss: 23.53907060623169\n",
                        "linear l2 loss: 0.39116928 Linear l2 grad loss: 23.53791880607605\n",
                        "linear l2 loss: 0.39106923 Linear l2 grad loss: 23.536743879318237\n",
                        "linear l2 loss: 0.3909698 Linear l2 grad loss: 23.53559923171997\n",
                        "linear l2 loss: 0.39087018 Linear l2 grad loss: 23.534440755844116\n",
                        "linear l2 loss: 0.3907703 Linear l2 grad loss: 23.53326940536499\n",
                        "linear l2 loss: 0.39067107 Linear l2 grad loss: 23.532127380371094\n",
                        "linear l2 loss: 0.3905718 Linear l2 grad loss: 23.530981063842773\n",
                        "linear l2 loss: 0.39047277 Linear l2 grad loss: 23.529847145080566\n",
                        "linear l2 loss: 0.3903728 Linear l2 grad loss: 23.528663158416748\n",
                        "linear l2 loss: 0.39027476 Linear l2 grad loss: 23.527565956115723\n",
                        "linear l2 loss: 0.39017448 Linear l2 grad loss: 23.526376008987427\n",
                        "linear l2 loss: 0.3900764 Linear l2 grad loss: 23.525277614593506\n",
                        "linear l2 loss: 0.38997623 Linear l2 grad loss: 23.524085760116577\n",
                        "linear l2 loss: 0.38987675 Linear l2 grad loss: 23.522934436798096\n",
                        "linear l2 loss: 0.38977793 Linear l2 grad loss: 23.521795749664307\n",
                        "linear l2 loss: 0.3896788 Linear l2 grad loss: 23.52065110206604\n",
                        "linear l2 loss: 0.38957927 Linear l2 grad loss: 23.51949405670166\n",
                        "linear l2 loss: 0.3894815 Linear l2 grad loss: 23.51839852333069\n",
                        "linear l2 loss: 0.38938165 Linear l2 grad loss: 23.51722741127014\n",
                        "linear l2 loss: 0.389283 Linear l2 grad loss: 23.516090393066406\n",
                        "linear l2 loss: 0.38918367 Linear l2 grad loss: 23.51493501663208\n",
                        "linear l2 loss: 0.38908526 Linear l2 grad loss: 23.51381254196167\n",
                        "linear l2 loss: 0.38898584 Linear l2 grad loss: 23.51265048980713\n",
                        "linear l2 loss: 0.38888758 Linear l2 grad loss: 23.51153326034546\n",
                        "linear l2 loss: 0.388789 Linear l2 grad loss: 23.510401725769043\n",
                        "linear l2 loss: 0.38868952 Linear l2 grad loss: 23.509238481521606\n",
                        "linear l2 loss: 0.3885912 Linear l2 grad loss: 23.508113145828247\n",
                        "linear l2 loss: 0.38849255 Linear l2 grad loss: 23.506978034973145\n",
                        "linear l2 loss: 0.38839325 Linear l2 grad loss: 23.505823612213135\n",
                        "linear l2 loss: 0.38829544 Linear l2 grad loss: 23.504714965820312\n",
                        "linear l2 loss: 0.38819656 Linear l2 grad loss: 23.503571033477783\n",
                        "linear l2 loss: 0.38809735 Linear l2 grad loss: 23.502418518066406\n",
                        "linear l2 loss: 0.38799948 Linear l2 grad loss: 23.50130844116211\n",
                        "linear l2 loss: 0.3879009 Linear l2 grad loss: 23.500175952911377\n",
                        "linear l2 loss: 0.38780198 Linear l2 grad loss: 23.499021530151367\n",
                        "linear l2 loss: 0.3877041 Linear l2 grad loss: 23.497912883758545\n",
                        "linear l2 loss: 0.38760516 Linear l2 grad loss: 23.496755838394165\n",
                        "linear l2 loss: 0.38750625 Linear l2 grad loss: 23.49561309814453\n",
                        "linear l2 loss: 0.38740897 Linear l2 grad loss: 23.49452018737793\n",
                        "linear l2 loss: 0.3873108 Linear l2 grad loss: 23.49340581893921\n",
                        "linear l2 loss: 0.38721243 Linear l2 grad loss: 23.492269039154053\n",
                        "linear l2 loss: 0.38711333 Linear l2 grad loss: 23.49110460281372\n",
                        "linear l2 loss: 0.3870155 Linear l2 grad loss: 23.489989280700684\n",
                        "linear l2 loss: 0.3869178 Linear l2 grad loss: 23.488889932632446\n",
                        "linear l2 loss: 0.3868192 Linear l2 grad loss: 23.487749338150024\n",
                        "linear l2 loss: 0.38672113 Linear l2 grad loss: 23.486627101898193\n",
                        "linear l2 loss: 0.3866233 Linear l2 grad loss: 23.48551607131958\n",
                        "linear l2 loss: 0.38652536 Linear l2 grad loss: 23.4843966960907\n",
                        "linear l2 loss: 0.38642702 Linear l2 grad loss: 23.48324728012085\n",
                        "linear l2 loss: 0.3863288 Linear l2 grad loss: 23.482138633728027\n",
                        "linear l2 loss: 0.38623175 Linear l2 grad loss: 23.481037139892578\n",
                        "linear l2 loss: 0.38613388 Linear l2 grad loss: 23.479917526245117\n",
                        "linear l2 loss: 0.3860354 Linear l2 grad loss: 23.4787859916687\n",
                        "linear l2 loss: 0.38593718 Linear l2 grad loss: 23.477644205093384\n",
                        "linear l2 loss: 0.38583842 Linear l2 grad loss: 23.47649908065796\n",
                        "linear l2 loss: 0.38574168 Linear l2 grad loss: 23.475411891937256\n",
                        "linear l2 loss: 0.38564417 Linear l2 grad loss: 23.474305152893066\n",
                        "linear l2 loss: 0.38554588 Linear l2 grad loss: 23.473170518875122\n",
                        "linear l2 loss: 0.3854482 Linear l2 grad loss: 23.47205400466919\n",
                        "linear l2 loss: 0.3853509 Linear l2 grad loss: 23.470951557159424\n",
                        "linear l2 loss: 0.38525298 Linear l2 grad loss: 23.46983051300049\n",
                        "linear l2 loss: 0.38515505 Linear l2 grad loss: 23.46871042251587\n",
                        "linear l2 loss: 0.38505754 Linear l2 grad loss: 23.467593908309937\n",
                        "linear l2 loss: 0.38495994 Linear l2 grad loss: 23.46648335456848\n",
                        "linear l2 loss: 0.3848623 Linear l2 grad loss: 23.46536374092102\n",
                        "linear l2 loss: 0.38476452 Linear l2 grad loss: 23.464244842529297\n",
                        "linear l2 loss: 0.38466722 Linear l2 grad loss: 23.463146209716797\n",
                        "linear l2 loss: 0.38457033 Linear l2 grad loss: 23.462047338485718\n",
                        "linear l2 loss: 0.38447306 Linear l2 grad loss: 23.460941076278687\n",
                        "linear l2 loss: 0.38437554 Linear l2 grad loss: 23.45983052253723\n",
                        "linear l2 loss: 0.38427854 Linear l2 grad loss: 23.45872974395752\n",
                        "linear l2 loss: 0.3841815 Linear l2 grad loss: 23.45764470100403\n",
                        "linear l2 loss: 0.38408422 Linear l2 grad loss: 23.456539392471313\n",
                        "linear l2 loss: 0.38398656 Linear l2 grad loss: 23.455415725708008\n",
                        "linear l2 loss: 0.38388953 Linear l2 grad loss: 23.454317331314087\n",
                        "linear l2 loss: 0.38379204 Linear l2 grad loss: 23.45320749282837\n",
                        "linear l2 loss: 0.38369462 Linear l2 grad loss: 23.452094078063965\n",
                        "linear l2 loss: 0.3835981 Linear l2 grad loss: 23.451002836227417\n",
                        "linear l2 loss: 0.3835009 Linear l2 grad loss: 23.4499089717865\n",
                        "linear l2 loss: 0.38340354 Linear l2 grad loss: 23.44879126548767\n",
                        "linear l2 loss: 0.38330647 Linear l2 grad loss: 23.44768190383911\n",
                        "linear l2 loss: 0.38320962 Linear l2 grad loss: 23.446589708328247\n",
                        "linear l2 loss: 0.38311276 Linear l2 grad loss: 23.44550108909607\n",
                        "linear l2 loss: 0.3830162 Linear l2 grad loss: 23.444412231445312\n",
                        "linear l2 loss: 0.38291916 Linear l2 grad loss: 23.443306922912598\n",
                        "linear l2 loss: 0.3828228 Linear l2 grad loss: 23.442225694656372\n",
                        "linear l2 loss: 0.3827261 Linear l2 grad loss: 23.441139936447144\n",
                        "linear l2 loss: 0.38262868 Linear l2 grad loss: 23.440024614334106\n",
                        "linear l2 loss: 0.38253233 Linear l2 grad loss: 23.43894124031067\n",
                        "linear l2 loss: 0.38243547 Linear l2 grad loss: 23.43784785270691\n",
                        "linear l2 loss: 0.38233873 Linear l2 grad loss: 23.436753511428833\n",
                        "linear l2 loss: 0.3822426 Linear l2 grad loss: 23.435676097869873\n",
                        "linear l2 loss: 0.38214564 Linear l2 grad loss: 23.43456530570984\n",
                        "linear l2 loss: 0.38204834 Linear l2 grad loss: 23.433466911315918\n",
                        "linear l2 loss: 0.3819528 Linear l2 grad loss: 23.432404279708862\n",
                        "linear l2 loss: 0.38185582 Linear l2 grad loss: 23.431299448013306\n",
                        "linear l2 loss: 0.38176027 Linear l2 grad loss: 23.430248737335205\n",
                        "linear l2 loss: 0.38166267 Linear l2 grad loss: 23.42911195755005\n",
                        "linear l2 loss: 0.3815669 Linear l2 grad loss: 23.428051233291626\n",
                        "linear l2 loss: 0.3814704 Linear l2 grad loss: 23.426963090896606\n",
                        "linear l2 loss: 0.38137397 Linear l2 grad loss: 23.425885915756226\n",
                        "linear l2 loss: 0.38127825 Linear l2 grad loss: 23.42482352256775\n",
                        "linear l2 loss: 0.38118193 Linear l2 grad loss: 23.423734664916992\n"
                    ]
                }
            ],
            "source": [
                "linear_l2_grad_norm = float(\"inf\")\n",
                "iteration = 0\n",
                "\n",
                "while linear_l2_grad_norm > tolerance_linear and iteration < max_steps_linear:\n",
                "    linear_l2_loss = 0\n",
                "\n",
                "    data_loader_weights_linear.shuffle()\n",
                "    for batch_weights in data_loader_weights_linear:\n",
                "        pbo_optimal_linear.params, pbo_optimal_linear.optimizer_state, l2_loss, l2_grad_loss = pbo_optimal_linear.learn_on_batch(pbo_optimal_linear.params, pbo.optimizer_state, batch_weights)\n",
                "        linear_l2_loss += l2_loss\n",
                "\n",
                "    iteration += 1\n",
                "\n",
                "    # Visualization\n",
                "    if iteration % plot_freq == 0:\n",
                "        linear_l2_grad_norm = 0\n",
                "        for layers in l2_grad_loss.values():\n",
                "                for grad in layers.values():\n",
                "                    linear_l2_grad_norm += np.linalg.norm(grad)\n",
                "\n",
                "        print(\"linear l2 loss:\", linear_l2_loss, \"Linear l2 grad loss:\", linear_l2_grad_norm)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train PBO on several iteration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 2 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from pbo.utils.shared_axis_plot import shared_axis_plot\n",
                "\n",
                "\n",
                "# For visualization\n",
                "full_batch = {\n",
                "    \"state\": replay_buffer.states,\n",
                "    \"action\": replay_buffer.actions,\n",
                "    \"reward\": replay_buffer.rewards,\n",
                "    \"next_state\": replay_buffer.next_states,\n",
                "}\n",
                "l1_losses = []\n",
                "\n",
                "iteration = 0\n",
                "l1_loss = float(\"inf\")\n",
                "bellman_iterations = jnp.arange(max_bellman_iterations + 1)\n",
                "\n",
                "while l1_loss > tolerance and iteration < max_steps:\n",
                "    cumulative_l2_loss = 0\n",
                "    \n",
                "    data_loader_weights.shuffle()\n",
                "    for batch_weights in data_loader_weights:\n",
                "        data_loader_samples.shuffle()\n",
                "        for batch_samples in data_loader_samples:\n",
                "            pbo.params, pbo.optimizer_state, l2_loss = pbo.learn_on_batch(pbo.params, pbo.optimizer_state, batch_weights, batch_samples)            \n",
                "            cumulative_l2_loss += l2_loss\n",
                "\n",
                "    # Visualization\n",
                "    if iteration % plot_freq == 0:  \n",
                "        l1_loss = pbo.l1_loss(pbo.params, data_loader_weights.weights, full_batch)\n",
                "\n",
                "        l1_losses.append(l1_loss)\n",
                "\n",
                "    iteration += 1\n",
                "\n",
                "shared_axis_plot(l1_losses, np.array(l1_losses) * np.nan, f\"every {plot_freq} iteration\", \"l1 loss\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "iterated_weights = pbo.network.apply(pbo.params, weights)\n",
                "\n",
                "for i in range(5):\n",
                "    iterated_weights = pbo.network.apply(pbo.params, iterated_weights)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "DeviceArray(9.942391, dtype=float32)"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "jnp.linalg.norm(optimal_q - q.to_params(iterated_weights)[\"TableQNet\"][\"table\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Performances of the operators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "l1 loss 0.09958296\n",
                        "l1 loss optimal linear 0.28964972\n",
                        "l1 loss optimal 1.501849\n"
                    ]
                }
            ],
            "source": [
                "l1_loss_optimal_linear = pbo.l1_loss(pbo_optimal_linear.params, data_loader_weights.weights, full_batch)\n",
                "l1_loss_optimal = pbo_optimal.l1_loss(pbo, q, data_loader_weights.weights, full_batch, max_bellman_iterations, add_infinity)\n",
                "\n",
                "print(\"l1 loss\", l1_loss)\n",
                "print(\"l1 loss optimal linear\", l1_loss_optimal_linear)\n",
                "print(\"l1 loss optimal\", l1_loss_optimal)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'Theoretical3DQ' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=0'>1</a>\u001b[0m q_thin \u001b[39m=\u001b[39m Theoretical3DQ(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=1'>2</a>\u001b[0m     network_key\u001b[39m=\u001b[39mq_network_key,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=2'>3</a>\u001b[0m     random_weights_range\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=3'>4</a>\u001b[0m     random_weights_key\u001b[39m=\u001b[39mrandom_weights_key,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=4'>5</a>\u001b[0m     action_range_on_max\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m action_range_on_max,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=5'>6</a>\u001b[0m     n_actions_on_max\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m n_actions_on_max,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=8'>9</a>\u001b[0m pbo_thin \u001b[39m=\u001b[39m LinearPBO(pbo_network_key, q_thin, learning_rate, max_bellman_iterations, add_infinity)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vincent/Darmstadt/PBO/pbo_chain_walk.ipynb#ch0000024?line=10'>11</a>\u001b[0m l1_loss_optimal_thin \u001b[39m=\u001b[39m pbo_optimal\u001b[39m.\u001b[39ml1_loss(pbo_thin, q_thin, data_loader_weights\u001b[39m.\u001b[39mweights, full_batch, max_bellman_iterations, add_infinity)\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'Theoretical3DQ' is not defined"
                    ]
                }
            ],
            "source": [
                "q_thin = Theoretical3DQ(\n",
                "    network_key=q_network_key,\n",
                "    random_weights_range=None,\n",
                "    random_weights_key=random_weights_key,\n",
                "    action_range_on_max=10 * action_range_on_max,\n",
                "    n_actions_on_max=10 * n_actions_on_max,\n",
                ")\n",
                "\n",
                "pbo_thin = LinearPBO(pbo_network_key, q_thin, learning_rate, max_bellman_iterations, add_infinity)\n",
                "\n",
                "l1_loss_optimal_thin = pbo_optimal.l1_loss(pbo_thin, q_thin, data_loader_weights.weights, full_batch, max_bellman_iterations, add_infinity)\n",
                "\n",
                "print(\"l1 loss optimal thin scale\", l1_loss_optimal_thin)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize iterations on weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def add_points(ax, points, size, label, color):\n",
                "    xdata = points[:, 0]\n",
                "    ydata = points[:, 1]\n",
                "    zdata = points[:, 2]\n",
                "    ax.scatter3D(xdata, ydata, zdata, s=size, label=label, color=color)\n",
                "\n",
                "fig = plt.figure(figsize=(7, 7))\n",
                "ax = fig.add_subplot(111, projection='3d')\n",
                "sizes = [1, 5, 300, 1000]\n",
                "colors = [\"black\", \"b\", \"red\", \"g\"]\n",
                "iterated_weights = weights_buffer.weights\n",
                "\n",
                "for iteration in range(4):\n",
                "    add_points(ax, iterated_weights, sizes[iteration], f\"iteration {iteration}\", colors[iteration])\n",
                "    iterated_weights = pbo_optimal(iterated_weights)\n",
                "\n",
                "ax.set_xlabel('k')\n",
                "ax.set_xticklabels([])\n",
                "ax.set_xticks([])\n",
                "\n",
                "ax.set_ylabel('i')\n",
                "ax.set_yticklabels([])\n",
                "ax.set_yticks([])\n",
                "\n",
                "ax.set_zlabel('m')\n",
                "ax.set_zlim(-2, 5)\n",
                "\n",
                "ax.legend()\n",
                "ax.view_init(0, 0)\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(7, 7))\n",
                "ax = plt.axes(projection='3d')\n",
                "\n",
                "iterated_weights = weights_buffer.weights\n",
                "\n",
                "for iteration in range(4):\n",
                "    add_points(ax, iterated_weights, sizes[iteration], f\"iteration {iteration}\", colors[iteration])\n",
                "    iterated_weights = pbo_optimal_linear.network.apply(pbo_optimal_linear.params, iterated_weights)\n",
                "\n",
                "ax.set_xlabel('k')\n",
                "ax.set_xticklabels([])\n",
                "ax.set_xticks([])\n",
                "\n",
                "ax.set_ylabel('i')\n",
                "ax.set_yticklabels([])\n",
                "ax.set_yticks([])\n",
                "\n",
                "ax.set_zlabel('m')\n",
                "ax.set_zlim(-2, 5)\n",
                "\n",
                "ax.legend()\n",
                "ax.view_init(0, 0)\n",
                "fig.tight_layout()\n",
                "print(\"Contracting facteur\", jnp.linalg.norm(pbo_optimal_linear.params[\"LinearPBONet/linear\"][\"w\"], ord=1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(7, 7))\n",
                "ax = plt.axes(projection='3d')\n",
                "\n",
                "iterated_weights = weights_buffer.weights\n",
                "\n",
                "for iteration in range(4):\n",
                "    add_points(ax, iterated_weights, sizes[iteration], f\"iteration {iteration}\", colors[iteration])\n",
                "    iterated_weights = pbo.network.apply(pbo.params, iterated_weights)\n",
                "\n",
                "ax.set_xlabel('k')\n",
                "ax.set_xticklabels([])\n",
                "ax.set_xticks([])\n",
                "\n",
                "ax.set_ylabel('i')\n",
                "ax.set_yticklabels([])\n",
                "ax.set_yticks([])\n",
                "\n",
                "ax.set_zlabel('m')\n",
                "ax.set_zlim(-2, 5)\n",
                "\n",
                "ax.legend()\n",
                "ax.view_init(0, 0)\n",
                "fig.tight_layout()\n",
                "print(\"Contracting facteur\", jnp.linalg.norm(pbo.params[\"LinearPBONet/linear\"][\"w\"], ord=1))"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "1432d270abb514d077d760a4c8d2edd41cc0752b595b0513fca29951003961c1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('env': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
