{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PBO against LQR\n",
                "\n",
                "## Define paramters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import jax\n",
                "\n",
                "# keys\n",
                "seed = 1\n",
                "key = jax.random.PRNGKey(seed)\n",
                "env_key, key = jax.random.split(key)\n",
                "shuffle_key, q_network_key, random_weights_key, pbo_network_key = jax.random.split(key, 4)\n",
                "\n",
                "# Box over states and actions\n",
                "max_discrete_state = 7\n",
                "n_discrete_states = 5\n",
                "max_discrete_action = 9\n",
                "n_discrete_actions = 7\n",
                "\n",
                "gamma = 1\n",
                "\n",
                "# Q function\n",
                "layer_dimension = 50\n",
                "action_range_on_max = 2 * max_discrete_action\n",
                "n_actions_on_max = 1000\n",
                "\n",
                "# PBO trainings\n",
                "max_iterations = 10000\n",
                "batch_size_pbo = 1\n",
                "learning_rate_pbo = 0.0001\n",
                "\n",
                "# Visualisation\n",
                "plot_freq = 200\n",
                "sleeping_time = 0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "from pbo.environment.linear_quadratic import LinearQuadraticEnv\n",
                "\n",
                "\n",
                "discrete_states = np.linspace(-max_discrete_state, max_discrete_state, n_discrete_states)\n",
                "state_box_half_size = max_discrete_state / n_discrete_states\n",
                "discrete_states_boxes = np.linspace(\n",
                "    -max_discrete_state - state_box_half_size, max_discrete_state + state_box_half_size, n_discrete_states + 1\n",
                ")\n",
                "\n",
                "discrete_actions = np.linspace(-max_discrete_action, max_discrete_action, n_discrete_actions)\n",
                "action_box_half_size = max_discrete_action / n_discrete_actions\n",
                "discrete_actions_boxes = np.linspace(\n",
                "    -max_discrete_action - action_box_half_size, max_discrete_action + action_box_half_size, n_discrete_actions + 1\n",
                ")\n",
                "\n",
                "env = LinearQuadraticEnv(env_key, max_init_state=max_discrete_state)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collect samples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Epsilon greedy from optimal actions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from pbo.data_collection.replay_buffer import ReplayBuffer\n",
                "\n",
                "\n",
                "# replay_buffer = ReplayBuffer()\n",
                "\n",
                "# state = env.reset()\n",
                "# idx_sample = 0\n",
                "# epidose_length = 0\n",
                "\n",
                "# while idx_sample < n_samples:\n",
                "#     greedy_key, key = jax.random.split(greedy_key)\n",
                "#     if jax.random.uniform(key) < epsilon_greedy:\n",
                "#        action = env.optimal_action()\n",
                "#     else:\n",
                "#         greedy_key, key = jax.random.split(greedy_key)\n",
                "#         action = jax.random.uniform(key, [env.B.shape[0]], minval=discrete_actions[0], maxval=discrete_actions[-1])\n",
                "#     next_state, reward, _, _ = env.step(action)\n",
                "\n",
                "#     replay_buffer.add(state, action, reward, next_state)\n",
                "\n",
                "#     epidose_length += 1\n",
                "#     idx_sample += 1\n",
                "\n",
                "#     if epidose_length >= max_episode_length:\n",
                "#         state = env.reset()\n",
                "#         epidose_length = 0\n",
                "#     else:    \n",
                "#         state = next_state"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Samples on the crosses of the box"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "\n",
                "from pbo.data_collection.replay_buffer import ReplayBuffer\n",
                "\n",
                "\n",
                "n_samples = n_discrete_states * n_discrete_actions\n",
                "replay_buffer = ReplayBuffer()\n",
                "\n",
                "state = env.reset()\n",
                "idx_sample = 0\n",
                "epidose_length = 0\n",
                "\n",
                "for state in discrete_states:\n",
                "    for action in discrete_actions:\n",
                "        env.reset(jnp.array([state]))\n",
                "        next_state, reward, _, _ = env.step(jnp.array([action]))\n",
                "\n",
                "        replay_buffer.add(jnp.array([state]), jnp.array([action]), reward, next_state)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.data_collection.count_samples import count_samples\n",
                "from pbo.utils.state_action_mesh import StateActionMesh\n",
                "\n",
                "\n",
                "samples_count, n_outside_boxes = count_samples(replay_buffer, discrete_states_boxes, discrete_actions_boxes)\n",
                "samples_visu_mesh = StateActionMesh(discrete_states, discrete_actions, sleeping_time=0)\n",
                "\n",
                "samples_visu_mesh.set_values(samples_count, zeros_to_nan=True)\n",
                "samples_visu_mesh.show(\n",
                "    f\"Samples repartition, \\n{int(100 * n_outside_boxes / n_samples)}% are outside the box.\"\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build q network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.networks.jax.q import FullyConnectedQ\n",
                "\n",
                "\n",
                "q = FullyConnectedQ(\n",
                "    layer_dimension=layer_dimension,\n",
                "    network_key=q_network_key,\n",
                "    random_weights_range=None,\n",
                "    random_weights_key=random_weights_key,\n",
                "    action_range_on_max=action_range_on_max,\n",
                "    n_actions_on_max=n_actions_on_max,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build PBO network and its dataloader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.networks.jax.pbo import LinearPBO\n",
                "from pbo.data_collection.dataloader import DataLoader\n",
                "\n",
                "\n",
                "pbo = LinearPBO(pbo_network_key, gamma, q)\n",
                "\n",
                "data_loader = DataLoader(replay_buffer, batch_size_pbo, shuffle_key)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train PBO on one iteration over one random init weight"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Starting from random init weiths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_init_q_weights = q.random_init_weights()\n",
                "random_init_q_params = q.to_params(random_init_q_weights)\n",
                "\n",
                "discrete_q_from_random = q.discretize(random_init_q_params, discrete_states, discrete_actions)\n",
                "q_visu_mesh.set_values(discrete_q_from_random)\n",
                "q_visu_mesh.show(f\"Discrete Q with random init weights\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Iterating over the Q function with the random init weiths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_batch = {\n",
                "    \"reward\": replay_buffer.rewards,\n",
                "    \"next_state\": replay_buffer.next_states,\n",
                "}\n",
                "discrete_iterated_q_from_random_init = pbo.compute_target(full_batch, random_init_q_weights.reshape(1, -1)).reshape((n_discrete_states, n_discrete_actions))\n",
                "discrete_iterated_q_from_random_init_pd = pd.DataFrame(discrete_iterated_q_from_random_init, index=discrete_states, columns=discrete_actions)\n",
                "\n",
                "q_visu_mesh.set_values(discrete_iterated_q_from_random_init)\n",
                "q_visu_mesh.show(\n",
                "    f\"Discrete iterated Q with random init weights. \\nDifference with optimal Q: {np.linalg.norm(discrete_iterated_q_from_random_init - optimal_q, ord=1)}\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import optax\n",
                "\n",
                "optimizer = optax.sgd(learning_rate=learning_rate_pbo)\n",
                "optimizer_state = optimizer.init(pbo.params)\n",
                "\n",
                "# For visualization\n",
                "former_discrete_iterated_q = q.discretize(random_init_q_params, discrete_states, discrete_actions)\n",
                "cumulative_losses = []\n",
                "shifts = []\n",
                "\n",
                "iteration = 0\n",
                "cumulative_loss = float(\"inf\")\n",
                "\n",
                "while cumulative_loss > n_discrete_states * n_discrete_actions and iteration < max_iterations:\n",
                "    cumulative_loss = 0\n",
                "    \n",
                "    data_loader_pbo.shuffle()\n",
                "    for batch in data_loader_pbo:\n",
                "        target = pbo.compute_target(batch, random_init_q_weights.reshape(1, -1))  # reshape since only one weights\n",
                "\n",
                "        loss, grad_loss = pbo.loss_and_grad(pbo.params, batch, random_init_q_weights.reshape(1, -1), target)\n",
                "        updates, optimizer_state = optimizer.update(\n",
                "            grad_loss, optimizer_state\n",
                "        )\n",
                "        pbo.params = optax.apply_updates(pbo.params, updates)\n",
                "\n",
                "        cumulative_loss += loss\n",
                "\n",
                "    iteration += 1\n",
                "\n",
                "    # Visualization\n",
                "    if iteration % plot_freq == 0:\n",
                "        iterated_q_params = q.to_params(pbo.network.apply(pbo.params, random_init_q_weights))\n",
                "        discrete_iterated_q = q.discretize(iterated_q_params, discrete_states, discrete_actions)\n",
                "        shift = np.linalg.norm(discrete_iterated_q - former_discrete_iterated_q)\n",
                "\n",
                "        q_visu_mesh.set_values(discrete_iterated_q_from_random_init - discrete_iterated_q)\n",
                "        q_visu_mesh.show(f\"Discrete iterated Q with Bellmann - Discrete Q at iteration {iteration}, \\nloss: {int(cumulative_loss)}, shift: {np.around(shift, 2)}\")\n",
                "        \n",
                "        cumulative_losses.append(cumulative_loss)\n",
                "        shifts.append(shift)\n",
                "        former_discrete_iterated_q = discrete_iterated_q\n",
                "\n",
                "iterated_q_params = q.to_params(pbo.network.apply(pbo.params, random_init_q_weights))\n",
                "final_discrete_iterated_q = q.discretize(iterated_q_params, discrete_states, discrete_actions)\n",
                "q_visu_mesh.set_values(discrete_iterated_q_from_random_init - final_discrete_iterated_q)\n",
                "q_visu_mesh.show(f\"Discrete iterated Q with Bellmann - Discrete Q at iteration {iteration}, \\n final loss: {int(cumulative_loss)}\")\n",
                "\n",
                "# Plot the shifts and the loss\n",
                "fig, ax1 = plt.subplots()\n",
                "\n",
                "ax1.set_xlabel(f\"every {plot_freq} iteration\")\n",
                "ax1.set_ylabel(\"cumulative loss\", color=\"blue\")\n",
                "ax1.plot(np.arange(len(cumulative_losses)), cumulative_losses, color=\"blue\")\n",
                "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
                "\n",
                "ax2 = ax1.twinx()\n",
                "ax2.set_ylabel(\"shift\", color=\"red\")\n",
                "ax2.plot(np.arange(len(shifts)), shifts, color=\"red\")\n",
                "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
                "\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train PBO on one iteration over two random init weight"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Define the two random init weights and its iterated versions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_batch = {\n",
                "    \"reward\": replay_buffer.rewards,\n",
                "    \"next_state\": replay_buffer.next_states,\n",
                "}\n",
                "\n",
                "random_init_q_weights_1 = q_function.get_random_init_weights()\n",
                "random_init_q_params_1 = q_function.convert_to_params(random_init_q_weights_1)\n",
                "discrete_iterated_q_1 = pbo_function.compute_target(full_batch, random_init_q_weights_1.reshape(1, -1)).reshape((n_discrete_states, n_discrete_actions))\n",
                "\n",
                "random_init_q_weights_2 = q_function.get_random_init_weights()\n",
                "random_init_q_params_2 = q_function.convert_to_params(random_init_q_weights_2)\n",
                "discrete_iterated_q_2 = pbo_function.compute_target(full_batch, random_init_q_weights_2.reshape(1, -1)).reshape((n_discrete_states, n_discrete_actions))\n",
                "\n",
                "random_init_q_weights = jnp.vstack((random_init_q_weights_1, random_init_q_weights_2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import optax\n",
                "\n",
                "optimizer = optax.sgd(learning_rate=learning_rate_pbo)\n",
                "optimizer_state = optimizer.init(pbo_function.params)\n",
                "\n",
                "# For visualization\n",
                "previous_discrete_q_network_1 = q_function.get_discrete_q(random_init_q_params_1, discrete_states, discrete_actions)\n",
                "previous_discrete_q_network_2 = q_function.get_discrete_q(random_init_q_params_2, discrete_states, discrete_actions)\n",
                "cumulative_losses = []\n",
                "shifts = []\n",
                "\n",
                "iteration = 0\n",
                "cumulative_loss = float(\"inf\")\n",
                "\n",
                "while cumulative_loss > n_discrete_states * n_discrete_actions and iteration < max_iterations:\n",
                "    cumulative_loss = 0\n",
                "\n",
                "    data_loader_pbo.shuffle()\n",
                "    for batch in data_loader_pbo:\n",
                "        target = pbo_function.compute_target(batch, random_init_q_weights)\n",
                "\n",
                "        loss, grad_loss = pbo_function.loss_and_grad_loss(pbo_function.params, batch, random_init_q_weights, target)\n",
                "        updates, optimizer_state = optimizer.update(\n",
                "            grad_loss, optimizer_state\n",
                "        )\n",
                "        pbo_function.params = optax.apply_updates(pbo_function.params, updates)\n",
                "\n",
                "        cumulative_loss += loss\n",
                "\n",
                "    iteration += 1\n",
                "\n",
                "    # Plot Q network\n",
                "    if iteration % plot_freq == 0:\n",
                "        iterated_q_weights = pbo_function.network.apply(pbo_function.params, random_init_q_weights)\n",
                "        \n",
                "        iterated_q_params_1 = q_function.convert_to_params(iterated_q_weights[0])\n",
                "        discrete_q_network_1 = q_function.get_discrete_q(iterated_q_params_1, discrete_states, discrete_actions)        \n",
                "        iterated_q_params_2 = q_function.convert_to_params(iterated_q_weights[1])\n",
                "        discrete_q_network_2 = q_function.get_discrete_q(iterated_q_params_2, discrete_states, discrete_actions)\n",
                "\n",
                "        shift = (np.linalg.norm(discrete_q_network_1 - previous_discrete_q_network_1) + np.linalg.norm(discrete_q_network_1 - previous_discrete_q_network_1)) / 2\n",
                "\n",
                "        q_funcions_visualization_mesh.set_values(discrete_q_network_1)\n",
                "        q_funcions_visualization_mesh.show(f\"Discrete Q function 1 at iteration {iteration}, \\nloss: {int(cumulative_loss)}, shift: {np.around(shift, 2)}\")\n",
                "        \n",
                "        cumulative_losses.append(cumulative_loss)\n",
                "        shifts.append(shift)\n",
                "        previous_discrete_q_network = discrete_q_network\n",
                "\n",
                "iterated_q_weights = pbo_function.network.apply(pbo_function.params, random_init_q_weights)\n",
                "iterated_q_params_1 = q_function.convert_to_params(iterated_q_weights[0])\n",
                "discrete_q_network_1 = q_function.get_discrete_q(iterated_q_params_1, discrete_states, discrete_actions)        \n",
                "iterated_q_params_2 = q_function.convert_to_params(iterated_q_weights[1])\n",
                "discrete_q_network_2 = q_function.get_discrete_q(iterated_q_params_2, discrete_states, discrete_actions)\n",
                "\n",
                "q_funcions_visualization_mesh.set_values((np.abs(discrete_q_network_1 - discrete_iterated_q_1) + np.abs(discrete_q_network_2 - discrete_iterated_q_2)) / 2)\n",
                "q_funcions_visualization_mesh.show(f\"Difference between optimal Q function and the neural network Q at iteration {iteration}, \\n final loss: {int(cumulative_loss)}\")\n",
                "\n",
                "# Plot the shifts and the loss\n",
                "fig, ax1 = plt.subplots()\n",
                "\n",
                "ax1.set_xlabel(f\"every {plot_freq} iteration\")\n",
                "ax1.set_ylabel(\"cumulative loss\", color=\"blue\")\n",
                "ax1.plot(np.arange(len(cumulative_losses)), cumulative_losses, color=\"blue\")\n",
                "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
                "\n",
                "ax2 = ax1.twinx()\n",
                "ax2.set_ylabel(\"shift\", color=\"red\")\n",
                "ax2.plot(np.arange(len(shifts)), shifts, color=\"red\")\n",
                "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
                "\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train PBO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import optax\n",
                "\n",
                "# optimizer = optax.sgd(learning_rate=learning_rate_pbo)\n",
                "# optimizer_state = optimizer.init(pbo_function.params)\n",
                "\n",
                "# for iteration in range(n_iterations):\n",
                "#     random_weights = q_function.get_random_init_weights()\n",
                "#     data_loader_pbo.shuffle()\n",
                "\n",
                "#     for batch in data_loader_pbo:\n",
                "#         target = pbo_function.compute_target(batch, random_weights)\n",
                "#         loss, grad_loss = pbo_function.loss_and_grad_loss(pbo_function.params, batch, random_weights, target)\n",
                "#         updates, optimizer_state = optimizer.update(\n",
                "#             grad_loss, optimizer_state\n",
                "#         )\n",
                "#         pbo_function.params = optax.apply_updates(pbo_function.params, updates)\n",
                "\n",
                "#     # Plot the fixed point Q network\n",
                "#     fixed_point_params = q_function.convert_to_params(pbo_function.get_fixed_point())\n",
                "#     discrete_q_network = q_function.get_discrete_q(fixed_point_params, discrete_states, discrete_actions)\n",
                "    \n",
                "#     # q_funcions_visualization_mesh.set_values(discrete_q_network)\n",
                "#     # q_funcions_visualization_mesh.show(f\"Discrete Q function at iteration {iteration}\")"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "1432d270abb514d077d760a4c8d2edd41cc0752b595b0513fca29951003961c1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('env': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}