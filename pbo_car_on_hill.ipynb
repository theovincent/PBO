{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PBO learnt on several iterations and one weigth one the car on hill environment\n",
                "\n",
                "## Define parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import numpy as np\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "\n",
                "# keys\n",
                "seed = 0\n",
                "key = jax.random.PRNGKey(seed)\n",
                "shuffle_key, q_network_key, random_weights_key, pbo_network_key = jax.random.split(key, 4)\n",
                "\n",
                "# Sample collection\n",
                "n_states_x = 10\n",
                "n_states_v = 10\n",
                "gamma = 0.9\n",
                "\n",
                "# # Weights collection\n",
                "# n_weights = 500\n",
                "\n",
                "# # Trainings\n",
                "# add_infinity = True\n",
                "# max_bellman_iterations = 20\n",
                "\n",
                "# ## Linear PBO\n",
                "# fitting_steps = 2\n",
                "# training_steps = 200\n",
                "# batch_size_samples = n_states\n",
                "# batch_size_weights = n_weights\n",
                "# learning_rate = {\"first\": 0.01, \"last\": 0.001, \"duration\": training_steps * fitting_steps * n_actions * n_repetitions}\n",
                "\n",
                "# ## Q-learning\n",
                "# fitting_steps_q = fitting_steps * training_steps\n",
                "# learning_rate_q = {\"first\": 0.01, \"last\": 0.001, \"duration\": fitting_steps_q * max_bellman_iterations * n_actions * n_repetitions}\n",
                "\n",
                "# # Visualisation of errors and performances\n",
                "# max_bellman_iterations_validation = max_bellman_iterations + 20\n",
                "# horizon = 10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.environment.car_on_hill import CarOnHillEnv\n",
                "\n",
                "max_pos = 1.0\n",
                "max_velocity = 3.0\n",
                "n_actions = 2\n",
                "\n",
                "\n",
                "env = CarOnHillEnv(max_pos, max_velocity, gamma)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collect samples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Samples on the mesh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.sample_collection.replay_buffer import ReplayBuffer\n",
                "\n",
                "\n",
                "replay_buffer = ReplayBuffer()\n",
                "\n",
                "n_samples = n_states_x * n_states_v * n_actions\n",
                "\n",
                "for state_x in jnp.linspace(-max_pos, max_pos, n_states_x):\n",
                "    for state_v in jnp.linspace(-max_velocity, max_velocity, n_states_v):\n",
                "        state = jnp.array([state_x, state_v])\n",
                "        \n",
                "        for action in jnp.arange(n_actions):\n",
                "            env.reset(state)\n",
                "            next_state, reward, _, _ = env.step(jnp.array([action]))\n",
                "\n",
                "            replay_buffer.add(state, jnp.array([action]), reward, next_state)\n",
                "\n",
                "replay_buffer.cast_to_jax_array()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.sample_collection.count_samples import count_samples\n",
                "from pbo.utils.state_action_mesh import StateActionMesh\n",
                "\n",
                "\n",
                "samples_count, n_outside_boxes = count_samples(replay_buffer, states_boxes, actions_boxes)\n",
                "samples_visu_mesh = StateActionMesh(states, actions, sleeping_time=0)\n",
                "\n",
                "samples_visu_mesh.set_values(samples_count, zeros_to_nan=True)\n",
                "samples_visu_mesh.show(\n",
                "    f\"Samples repartition, \\n{int(100 * n_outside_boxes / n_samples)}% are outside the box.\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimal Q function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "optimal_q = env.optimal_Q_mesh()\n",
                "\n",
                "q_visu_mesh = StateActionMesh(states, actions, 0)\n",
                "\n",
                "q_visu_mesh.set_values(optimal_q)\n",
                "q_visu_mesh.show(\"Optimal q function\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train Q with Fitted-Q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "from pbo.sample_collection.dataloader import SampleDataLoader\n",
                "from pbo.networks.learnable_q import TableQ\n",
                "\n",
                "\n",
                "data_loader_samples = SampleDataLoader(replay_buffer, batch_size_samples, shuffle_key)\n",
                "q = TableQ(\n",
                "    gamma=gamma,\n",
                "    network_key=q_network_key,\n",
                "    random_weights_range=None,\n",
                "    random_weights_key=random_weights_key,\n",
                "    n_states=n_states,\n",
                "    n_actions=n_actions,\n",
                "    learning_rate=learning_rate_q,\n",
                "    zero_initializer=True\n",
                ")\n",
                "validation_initial_weight = q.to_weights(q.params)\n",
                "\n",
                "training_losses_q = np.zeros(max_bellman_iterations)\n",
                "validation_losses_q = np.zeros(max_bellman_iterations)\n",
                "absording_probabilities_q = np.zeros((max_bellman_iterations, horizon))\n",
                "\n",
                "for bellman_iteration in tqdm(range(max_bellman_iterations)):\n",
                "    params_target = q.params\n",
                "\n",
                "    for step in range(fitting_steps_q):\n",
                "        data_loader_samples.shuffle()\n",
                "        for batch_samples in data_loader_samples:\n",
                "            q.params, q.optimizer_state, _ = q.learn_on_batch(q.params, params_target, q.optimizer_state, batch_samples)        \n",
                "\n",
                "    q_i = q.discretize(q.to_weights(params_target).reshape((-1, q.weights_dimension)), states, actions)[0]\n",
                "    q_i_plus_1 = q.discretize(q.to_weights(q.params).reshape((-1, q.weights_dimension)), states, actions)[0]\n",
                "    policy_q = q_i_plus_1.argmax(axis=1)\n",
                "\n",
                "    training_losses_q[bellman_iteration] = jnp.abs(env.apply_bellman_operator(q_i) - q_i_plus_1).mean()\n",
                "    validation_losses_q[bellman_iteration] = jnp.abs(optimal_q - q_i_plus_1).mean()\n",
                "    absording_probabilities_q[bellman_iteration] = env.absorbing_probability(policy_q, horizon)\n",
                "\n",
                "    print(policy_q)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimal PBO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "from pbo.sample_collection.dataloader import SampleDataLoader\n",
                "from pbo.networks.learnable_pbo import TabularPBO\n",
                "\n",
                "\n",
                "data_loader_samples = SampleDataLoader(replay_buffer, batch_size_samples, shuffle_key)\n",
                "\n",
                "pbo_optimal = TabularPBO(q, max_bellman_iterations, add_infinity, pbo_network_key, learning_rate, n_actions)\n",
                "pbo_optimal.params[\"TabularPBONet/linear\"][\"w\"] = gamma * env.transition_proba.T\n",
                "pbo_optimal.params[\"TabularPBONet/linear\"][\"b\"] = env.PR.T\n",
                "\n",
                "training_losses_optimal = np.zeros(max_bellman_iterations_validation)\n",
                "validation_losses_optimal = np.zeros(max_bellman_iterations_validation)\n",
                "absording_probabilities_optimal = np.zeros((max_bellman_iterations_validation, horizon))\n",
                "\n",
                "batch_iterated_weights = validation_initial_weight.reshape((1, -1))\n",
                "for bellman_iteration in range(max_bellman_iterations_validation):\n",
                "    q_i = q.discretize(batch_iterated_weights, states, actions)[0]\n",
                "    batch_iterated_weights = pbo_optimal(pbo_optimal.params, batch_iterated_weights)\n",
                "    q_i_plus_1 = q.discretize(batch_iterated_weights, states, actions)[0]\n",
                "    policy_q = q_i_plus_1.argmax(axis=1)\n",
                "\n",
                "    training_losses_optimal[bellman_iteration] = jnp.abs(env.apply_bellman_operator(q_i) - q_i_plus_1).mean()\n",
                "    validation_losses_optimal[bellman_iteration] = jnp.abs(optimal_q - q_i_plus_1).mean()\n",
                "    absording_probabilities_optimal[bellman_iteration] = env.absorbing_probability(policy_q, horizon)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collect weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.weights_collection.weights_buffer import WeightsBuffer\n",
                "\n",
                "\n",
                "weights_buffer = WeightsBuffer()\n",
                "\n",
                "# Add initial validation weights\n",
                "weights_buffer.add(validation_initial_weight)\n",
                "\n",
                "# Add randow weights\n",
                "q = TableQ(\n",
                "    gamma=gamma,\n",
                "    network_key=q_network_key,\n",
                "    random_weights_range=None,\n",
                "    random_weights_key=random_weights_key,\n",
                "    n_states=n_states,\n",
                "    n_actions=n_actions,\n",
                "    learning_rate=learning_rate_q\n",
                ")\n",
                "\n",
                "while len(weights_buffer) < n_weights:\n",
                "    weights = q.random_init_weights()\n",
                "    weights_buffer.add(weights)\n",
                "\n",
                "weights_buffer.cast_to_jax_array()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train non linear PBO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "from pbo.sample_collection.dataloader import SampleDataLoader\n",
                "from pbo.weights_collection.dataloader import WeightsDataLoader\n",
                "from pbo.networks.learnable_pbo import TabularPBO\n",
                "\n",
                "\n",
                "data_loader_samples = SampleDataLoader(replay_buffer, batch_size_samples, shuffle_key)\n",
                "data_loader_weights = WeightsDataLoader(weights_buffer, batch_size_weights, shuffle_key)\n",
                "pbo_tabular = TabularPBO(q, max_bellman_iterations, add_infinity, pbo_network_key, learning_rate, n_actions)\n",
                "importance_iteration = jnp.ones(max_bellman_iterations + 1)\n",
                "importance_iteration = importance_iteration.at[-1].set(0)\n",
                "\n",
                "for _ in tqdm(range(training_steps)):\n",
                "    params_target = pbo_tabular.params\n",
                "\n",
                "    for _ in range(fitting_steps):\n",
                "        data_loader_weights.shuffle()\n",
                "        for batch_weights in data_loader_weights:\n",
                "            data_loader_samples.shuffle()\n",
                "            for batch_samples in data_loader_samples:\n",
                "                pbo_tabular.params, pbo_tabular.optimizer_state, _ = pbo_tabular.learn_on_batch(\n",
                "                    pbo_tabular.params, params_target, pbo_tabular.optimizer_state, batch_weights, batch_samples, importance_iteration\n",
                "                )\n",
                "\n",
                "training_losses_tabular = np.zeros(max_bellman_iterations_validation)\n",
                "validation_losses_tabular = np.zeros(max_bellman_iterations_validation)\n",
                "absording_probabilities_tabular = np.zeros((max_bellman_iterations_validation, horizon))\n",
                "\n",
                "batch_iterated_weights = validation_initial_weight.reshape((1, -1))\n",
                "for bellman_iteration in range(max_bellman_iterations_validation):\n",
                "    q_i = q.discretize(batch_iterated_weights, states, actions)[0]\n",
                "    batch_iterated_weights = pbo_tabular(pbo_tabular.params, batch_iterated_weights)\n",
                "    q_i_plus_1 = q.discretize(batch_iterated_weights, states, actions)[0]\n",
                "    policy_q = q_i_plus_1.argmax(axis=1)\n",
                "\n",
                "    training_losses_tabular[bellman_iteration] = jnp.abs(env.apply_bellman_operator(q_i) - q_i_plus_1).mean()\n",
                "    validation_losses_tabular[bellman_iteration] = jnp.abs(optimal_q - q_i_plus_1).mean()\n",
                "    absording_probabilities_tabular[bellman_iteration] = env.absorbing_probability(policy_q, horizon)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train linear PBO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "from pbo.sample_collection.dataloader import SampleDataLoader\n",
                "from pbo.weights_collection.dataloader import WeightsDataLoader\n",
                "from pbo.networks.learnable_pbo import LinearPBO\n",
                "\n",
                "\n",
                "data_loader_samples = SampleDataLoader(replay_buffer, batch_size_samples, shuffle_key)\n",
                "data_loader_weights = WeightsDataLoader(weights_buffer, batch_size_weights, shuffle_key)\n",
                "pbo = LinearPBO(q, max_bellman_iterations, True, pbo_network_key, learning_rate)  # infinity added\n",
                "importance_iteration = jnp.ones(max_bellman_iterations + 1)\n",
                "\n",
                "\n",
                "for _ in tqdm(range(training_steps)):\n",
                "    params_target = pbo.params\n",
                "    for _ in range(fitting_steps):\n",
                "        data_loader_weights.shuffle()\n",
                "        for batch_weights in data_loader_weights:\n",
                "            data_loader_samples.shuffle()\n",
                "            for batch_samples in data_loader_samples:\n",
                "                pbo.params, pbo.optimizer_state, _ = pbo.learn_on_batch(\n",
                "                    pbo.params, params_target, pbo.optimizer_state, batch_weights, batch_samples, importance_iteration\n",
                "                )\n",
                "\n",
                "training_losses = np.zeros(max_bellman_iterations_validation)\n",
                "validation_losses = np.zeros(max_bellman_iterations_validation)\n",
                "absording_probabilities = np.zeros((max_bellman_iterations_validation, horizon))\n",
                "\n",
                "batch_iterated_weights = validation_initial_weight.reshape((1, -1))\n",
                "for bellman_iteration in range(max_bellman_iterations_validation):\n",
                "    q_i = q.discretize(batch_iterated_weights, states, actions)[0]\n",
                "    batch_iterated_weights = pbo(pbo.params, batch_iterated_weights)\n",
                "    q_i_plus_1 = q.discretize(batch_iterated_weights, states, actions)[0]\n",
                "    policy_q = q_i_plus_1.argmax(axis=1)\n",
                "\n",
                "    training_losses[bellman_iteration] = jnp.abs(env.apply_bellman_operator(q_i) - q_i_plus_1).mean()\n",
                "    validation_losses[bellman_iteration] = jnp.abs(optimal_q - q_i_plus_1).mean()\n",
                "    absording_probabilities[bellman_iteration] = env.absorbing_probability(policy_q, horizon)\n",
                "\n",
                "    print(policy_q)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize errors in Q functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt \n",
                "\n",
                "iterations = range(1, max_bellman_iterations_validation + 1)\n",
                "\n",
                "plt.plot(iterations, validation_losses, label=\"pbo linear\", color=\"green\")\n",
                "plt.plot(iterations, validation_losses_tabular, label=\"pbo max-linear\", color=\"grey\", linestyle=\"--\")\n",
                "plt.plot(iterations, validation_losses_optimal, label=\"pbo optimal\", color=\"black\")\n",
                "plt.plot(range(1, max_bellman_iterations + 1), validation_losses_q, label=\"FQI\", color=\"red\", linewidth=4)\n",
                "plt.vlines(max_bellman_iterations, 0, np.max(validation_losses_q), color=\"black\", linestyle=\"--\")\n",
                "\n",
                "plt.xticks(iterations)\n",
                "plt.xlabel(\"iterations\")\n",
                "\n",
                "plt.title(r\"$|| Q^* - Q_i ||_1$\")\n",
                "_ = plt.legend()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(iterations, training_losses, label=\"pbo linear\", color=\"green\")\n",
                "plt.plot(iterations, training_losses_tabular, label=\"pbo max-linear\", color=\"grey\")\n",
                "plt.plot(iterations, training_losses_optimal, label=\"pbo optimal\", color=\"black\")\n",
                "plt.plot(range(1, max_bellman_iterations + 1), training_losses_q, label=\"FQI\", color=\"red\")\n",
                "plt.vlines(max_bellman_iterations, 0, np.maximum(np.max(training_losses_q), np.max(training_losses)), color=\"black\", linestyle=\"--\")\n",
                "\n",
                "plt.xticks(iterations)\n",
                "plt.xlabel(\"iterations\")\n",
                "\n",
                "plt.title(r\"$|| \\Gamma^*Q_{i-1} - Q_i ||_1$\")\n",
                "_ = plt.legend()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Understanding the learning process"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "empirical_transition_proba = np.zeros((n_states, n_states * n_actions))\n",
                "\n",
                "for idx_sample in range(len(replay_buffer.states)):\n",
                "    state = replay_buffer.states[idx_sample, 0]\n",
                "    action = replay_buffer.actions[idx_sample, 0]\n",
                "    next_state = replay_buffer.next_states[idx_sample, 0]\n",
                "\n",
                "    empirical_transition_proba[next_state, state * n_actions + action] += 1 \n",
                "\n",
                "empirical_transition_proba /= n_repetitions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.linalg.norm(empirical_transition_proba - pbo_tabular.params[\"TabularPBONet/linear\"][\"w\"] / gamma)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize errors in preformances"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "iteration = 5\n",
                "time = range(1, horizon + 1)\n",
                "\n",
                "\n",
                "plt.plot(time, absording_probabilities[iteration], label=\"pbo linear\", color=\"green\")\n",
                "plt.plot(time, absording_probabilities_tabular[iteration], label=\"pbo max-linear\", color=\"grey\", linestyle=\"--\")\n",
                "plt.plot(time, absording_probabilities_optimal[iteration], label=\"pbo optimal\", color=\"black\")\n",
                "plt.plot(time, absording_probabilities_q[min([iteration, max_bellman_iterations - 1])], label=\"FQI\", color=\"red\")\n",
                "\n",
                "plt.xticks(time)\n",
                "plt.xlabel(\"time\")\n",
                "\n",
                "plt.title(r\"$P^{\\pi}(s_i =$ absorbing state)\")\n",
                "_ = plt.legend()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "iteration = 1\n",
                "\n",
                "\n",
                "plt.plot(iterations, absording_probabilities[:, -1], label=\"pbo linear\", color=\"green\")\n",
                "plt.plot(iterations, absording_probabilities_tabular[:, -1], label=\"pbo max-linear\", color=\"grey\", linestyle=\"--\")\n",
                "plt.plot(iterations, absording_probabilities_optimal[:, -1], label=\"pbo optimal\", color=\"black\")\n",
                "plt.plot(range(1, max_bellman_iterations + 1), absording_probabilities_q[:, -1], label=\"FQI\", color=\"red\")\n",
                "plt.vlines(max_bellman_iterations, 0, 1, color=\"black\", linestyle=\"--\")\n",
                "\n",
                "plt.xticks(iterations)\n",
                "plt.xlabel(\"iterations\")\n",
                "\n",
                "plt.title(r\"$P^{\\pi}(s_i =$ absorbing state)\" + f\" after {horizon} steps\")\n",
                "_ = plt.legend()"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "1432d270abb514d077d760a4c8d2edd41cc0752b595b0513fca29951003961c1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('env': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
