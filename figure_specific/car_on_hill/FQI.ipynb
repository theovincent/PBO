{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# FQI on Car On Hill\n",
                "\n",
                "## Define parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import os\n",
                "import numpy as np\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "\n",
                "\n",
                "gamma = 0.95\n",
                "n_actions = 2\n",
                "\n",
                "# Sample collection\n",
                "oriented_state = jnp.array([0.5, 0.38])\n",
                "n_random_samples = 3500\n",
                "n_oriented_samples = 500\n",
                "n_samples = n_random_samples + n_oriented_samples\n",
                "\n",
                "# Trainings\n",
                "layers_dimension = (7, 7)\n",
                "max_bellman_iterations = 15\n",
                "batch_size_samples = n_samples\n",
                "\n",
                "## FQI\n",
                "fitting_steps = 400 * 20\n",
                "batch_size_weights = 1\n",
                "learning_rate = {\"first\": 0.001, \"last\": 0.00005, \"duration\": fitting_steps * n_samples // batch_size_samples}\n",
                "\n",
                "# Visualisation of errors and performances\n",
                "n_states_x = 17\n",
                "n_states_v = 17\n",
                "horizon = 100\n",
                "plot_freq = 10\n",
                "\n",
                "# Search for an unused seed\n",
                "max_used_seed = 0\n",
                "for file in os.listdir(\"figures/data/FQI/\"):\n",
                "    if int(file.split(\"_\")[0]) == max_bellman_iterations and int(file.split(\"_\")[2][:-4]) > max_used_seed:\n",
                "        max_used_seed = int(file.split(\"_\")[2][:-4])\n",
                "max_used_seed\n",
                "\n",
                "# keys\n",
                "seed = max_used_seed + 1\n",
                "seed_sample = 0\n",
                "sample_key = jax.random.PRNGKey(seed_sample)\n",
                "key = jax.random.PRNGKey(seed)\n",
                "shuffle_key, q_network_key, pbo_network_key = jax.random.split(key, 3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.environment.car_on_hill import CarOnHillEnv\n",
                "\n",
                "\n",
                "max_pos = 1.0\n",
                "max_velocity = 3.0\n",
                "\n",
                "states_x = jnp.linspace(-max_pos, max_pos, n_states_x)\n",
                "boxes_x_size = (2 * max_pos) / (n_states_x - 1)\n",
                "states_x_boxes = (np.linspace(-max_pos, max_pos + boxes_x_size, n_states_x + 1) - boxes_x_size / 2)\n",
                "states_v = jnp.linspace(-max_velocity, max_velocity, n_states_v)\n",
                "boxes_v_size = (2 * max_velocity) / (n_states_v - 1)\n",
                "states_v_boxes = (np.linspace(-max_velocity, max_velocity + boxes_v_size, n_states_v + 1) - boxes_v_size / 2)\n",
                "\n",
                "\n",
                "env = CarOnHillEnv(max_pos, max_velocity, gamma)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Collect samples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Samples on the mesh and with a uniform policy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.sample_collection.replay_buffer import ReplayBuffer\n",
                "\n",
                "\n",
                "replay_buffer = ReplayBuffer()\n",
                "\n",
                "env.reset()\n",
                "n_episodes = 0\n",
                "n_steps = 0\n",
                "for idx_sample in range(n_random_samples):\n",
                "    state = env.state\n",
                "    \n",
                "    sample_key, key = jax.random.split(sample_key)\n",
                "    if jax.random.uniform(key) > 0.5:\n",
                "        action = jnp.array([0])\n",
                "    else:\n",
                "        action = jnp.array([1])\n",
                "    next_state, reward, absorbing, _ = env.step(action)\n",
                "    n_steps += 1\n",
                "\n",
                "    replay_buffer.add(state, action, reward, next_state, absorbing)\n",
                "\n",
                "    if absorbing or n_steps > horizon:\n",
                "        env.reset()\n",
                "        n_episodes += 1\n",
                "        n_steps = 0\n",
                "\n",
                "\n",
                "env.reset(oriented_state)\n",
                "n_episodes += 1\n",
                "n_steps = 0\n",
                "for idx_sample in range(n_oriented_samples):\n",
                "    state = env.state\n",
                "    \n",
                "    sample_key, key = jax.random.split(sample_key)\n",
                "    if jax.random.uniform(key) > 0.5:\n",
                "        action = jnp.array([0])\n",
                "    else:\n",
                "        action = jnp.array([1])\n",
                "    next_state, reward, absorbing, _ = env.step(action)\n",
                "    n_steps += 1\n",
                "\n",
                "    replay_buffer.add(state, action, reward, next_state, absorbing)\n",
                "\n",
                "    if absorbing or n_steps > horizon:\n",
                "        sample_key, key = jax.random.split(sample_key)\n",
                "        env.reset(jax.random.normal(key, (2,)) / 2 + oriented_state)\n",
                "\n",
                "        n_episodes += 1\n",
                "        n_steps = 0\n",
                "\n",
                "replay_buffer.cast_to_jax_array()\n",
                "assert sum(replay_buffer.rewards == 1) > 0, \"No positive reward has been sampled, please do something!\"\n",
                "print(f\"Number of episodes: {n_episodes}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "replay_buffer.actions[(replay_buffer.rewards == 1).flatten()]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pbo.sample_collection.count_samples import count_samples\n",
                "from pbo.utils.two_dimesions_mesh import TwoDimesionsMesh\n",
                "\n",
                "\n",
                "samples_count, n_outside_boxes, rewards_count = count_samples(replay_buffer.states[:, 0], replay_buffer.states[:, 1], states_x_boxes, states_v_boxes, replay_buffer.rewards)\n",
                "samples_visu_mesh = TwoDimesionsMesh(states_x, states_v, sleeping_time=0, axis_equal=False)\n",
                "\n",
                "samples_visu_mesh.set_values(samples_count, zeros_to_nan=True)\n",
                "samples_visu_mesh.show(\n",
                "    f\"Samples repartition, \\n{int(100 * n_outside_boxes / n_samples)}% are outside the box.\", xlabel=\"x\", ylabel=\"v\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "samples_visu_mesh.set_values(rewards_count, zeros_to_nan=True)\n",
                "samples_visu_mesh.show(\n",
                "    f\"Rewards repartition, \\n{int(100 * n_outside_boxes / n_samples)}% are outside the box.\", xlabel=\"x\", ylabel=\"v\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train FQI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from pbo.sample_collection.dataloader import SampleDataLoader\n",
                "from pbo.networks.learnable_q import FullyConnectedQ\n",
                "\n",
                "\n",
                "data_loader_samples = SampleDataLoader(replay_buffer, batch_size_samples, shuffle_key)\n",
                "q = FullyConnectedQ(\n",
                "    state_dim=2,\n",
                "    action_dim=1,\n",
                "    continuous_actions=False,\n",
                "    n_actions_on_max=n_actions,\n",
                "    action_range_on_max=None,\n",
                "    gamma=gamma,\n",
                "    network_key=q_network_key,\n",
                "    random_weights_range=None,\n",
                "    random_weights_key=None,\n",
                "    learning_rate=learning_rate,\n",
                "    layers_dimension=layers_dimension,\n",
                "    zero_initializer=True,\n",
                ")\n",
                "iterated_q = np.zeros((max_bellman_iterations, n_states_x, n_states_v, n_actions))\n",
                "iterated_v = np.zeros((max_bellman_iterations, n_states_x, n_states_v))\n",
                "\n",
                "q_visu_mesh = TwoDimesionsMesh(states_x, states_v, sleeping_time=0, axis_equal=False, zero_centered=True)\n",
                "l2_losses = np.ones((max_bellman_iterations, fitting_steps)) * np.nan\n",
                "\n",
                "for bellman_iteration in range(max_bellman_iterations):\n",
                "    params_target = q.params\n",
                "    best_loss = float('inf')\n",
                "    patience = 0\n",
                "\n",
                "    for step in range(fitting_steps):\n",
                "        cumulative_l2_loss = 0\n",
                "        \n",
                "        data_loader_samples.shuffle()\n",
                "        for batch_samples in data_loader_samples:\n",
                "            q.params, q.optimizer_state, l2_loss = q.learn_on_batch(q.params, params_target, q.optimizer_state, batch_samples)\n",
                "            cumulative_l2_loss += l2_loss\n",
                "\n",
                "        l2_losses[bellman_iteration, step] = cumulative_l2_loss\n",
                "        if cumulative_l2_loss < best_loss:\n",
                "            patience = 0\n",
                "            best_loss = cumulative_l2_loss\n",
                "        else:\n",
                "            patience += 1\n",
                "        \n",
                "        if patience > 5:\n",
                "            break\n",
                "\n",
                "    diff_q = env.diff_q_mesh(q, states_x, states_v)\n",
                "\n",
                "    q_visu_mesh.set_values(diff_q)\n",
                "    q_visu_mesh.show(f\"Q differences, \\nl2 loss: {str(jnp.round(cumulative_l2_loss, 3))}, iteration {bellman_iteration + 1}\", xlabel=\"x\", ylabel=\"v\")\n",
                "\n",
                "    iterated_q[bellman_iteration] = env.q_mesh(q, states_x, states_v)\n",
                "    iterated_v[bellman_iteration] = env.v_mesh(q, horizon, states_x, states_x)\n",
                "\n",
                "for bellman_iteration in range(0, max_bellman_iterations, max(max_bellman_iterations // 5, 1)):\n",
                "    plt.plot(l2_losses[bellman_iteration], label=f\"Iteration {bellman_iteration + 1}\")\n",
                "\n",
                "plt.legend()\n",
                "plt.xlabel(\"fitting step\")\n",
                "plt.ylabel(r\"$(\\Gamma^*Q_i - Q_{i +  1})^2$\")\n",
                "plt.title(\"Training losses\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "q_visu_mesh.set_values((diff_q > 0).astype(float))\n",
                "q_visu_mesh.show(f\"FQI Q policy, {max_bellman_iterations} iterations\", xlabel=\"x\", ylabel=\"v\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "v_mesh = env.v_mesh(q, horizon, states_x, states_v)\n",
                "\n",
                "q_visu_mesh.set_values(v_mesh)\n",
                "q_visu_mesh.show(r\"$V^{\\pi_{fqi}}, \\mathbb{E}\\left[ V^{\\pi_{fqi}} \\right] =$\" + str(np.round(np.mean(v_mesh), 2)), xlabel=\"x\", ylabel=\"v\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save the data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.save(f\"figures/data/FQI/{max_bellman_iterations}_Q_{seed}.npy\", iterated_q)\n",
                "np.save(f\"figures/data/FQI/{max_bellman_iterations}_V_{seed}.npy\", iterated_v)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.10 ('env_gpu': venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "ab25a1b93454a4c459c4cc7389b07f26610faebbcb955b8654e03002939afa4a"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
